{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: DeepMIMO data: BS16, row3500_3516, 3.4 GHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "\n",
    "# Add the Torch_code directory to the Python path\n",
    "# sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "sys.path.append(os.path.abspath('../helper'))\n",
    "import config\n",
    "import utils\n",
    "import loader\n",
    "import plotfig\n",
    "# import skfuzzy as fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_PATH = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "# print(FILE_PATH)\n",
    "# print(config.temp_path)\n",
    "# print(config.FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 32 #64  # Batch size\n",
    "NUM_EPOCHS = 20 # 20\n",
    "\n",
    "# rows from DeepMIMO dataset settings\n",
    "# change rows according to the .mat dataset file \n",
    "rows = [['1500', '1509'], ['4000', '4004']] \n",
    "fc = '3p4' #Hz can change to '60'\n",
    "rowss = \"1500_1509_4000_4004\"\n",
    "learning_rate = 0.00001 # 1e-5\n",
    "SNR = np.arange(0, 31, 5) # 0:5:30 dB\n",
    "outer_file_path = os.path.abspath(os.path.join(config.FILE_PATH, \n",
    "                                                '..', 'DeepMIMOv2', 'DeepMIMO_Data', 'Static_BS16', 'freq_symb_1ant_612sub_ver5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../model/static/CNN/BS16/1500_1509_4000_4004/ver3_/readme.txt' and ' ../figure/static/CNN/BS16/1500_1509_4000_4004/ver3_/readme.txt ' created and content written.\n"
     ]
    }
   ],
   "source": [
    "# create readme.txt file\n",
    "content = \"\"\"Generated by file 'train/static_CNN_lr1e-5_v5_r1500_1509_4000_4004_3p4_instance_min_max.ipynb'.\n",
    "Correspond with BS16, 3.4 GHz fc, rows 1500_1509 4000_4004,\n",
    "DeepMIMOv2/DeepMIMO_Dta_Static_BS16/freq_sym_1ant_612sub_ver5,\n",
    "Using min-max scaler for each sample\"\"\"\n",
    "\n",
    "norm_approach = 'minmax' # can be set to 'std'\n",
    "\n",
    "# Paths to save\n",
    "idx_save_path = loader.find_incremental_filename('../model/static/CNN/BS16/'+ rowss,'ver', '_', '')\n",
    "model_path = '../model/static/CNN/BS16/' + rowss + '/ver' + str(idx_save_path) + '_/readme.txt'\n",
    "figure_path = '../figure/static/CNN/BS16/' + rowss + '/ver' + str(idx_save_path) + '_/readme.txt'\n",
    "\n",
    "if not os.path.exists(os.path.dirname(model_path)):\n",
    "    os.makedirs(os.path.dirname(model_path))\n",
    "if not os.path.exists(os.path.dirname(figure_path)):\n",
    "    os.makedirs(os.path.dirname(figure_path))\n",
    "\n",
    "# Open the file in write mode ('w'). If the file does not exist, it will be created.\n",
    "with open(model_path, 'w') as file:\n",
    "    # Write the content to the file\n",
    "    file.write(content)\n",
    "\n",
    "with open(figure_path, 'w') as file:\n",
    "    # Write the content to the file\n",
    "    file.write(content)\n",
    "\n",
    "print(f\"File '{model_path}' and ' {figure_path} ' created and content written.\")\n",
    "\n",
    "save_folder_model = os.path.join(config.FILE_PATH, 'model/static/CNN', 'BS16', rowss, 'ver' + str(idx_save_path) + '_')\n",
    "save_folder_fig = os.path.join(config.FILE_PATH, 'figure', 'static', 'CNN', 'BS16' ,  rowss, 'ver' + str(idx_save_path) +'_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmse_LS_LI_val   = []\n",
    "nmse_LS_NN_val   = []\n",
    "nmse_LI_NN_val   = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snr = 0\n",
    "# [trainLabels, valLabels], [H_equal_train, H_linear_train, H_practical_train], [H_equal_val, H_linear_val, H_practical_val] = loader.load_data(outer_file_path, rows, fc, device, snr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SNR: 0/30\n",
      " Training for LS+LI\n",
      "SNR: 0/30, LS+LI, Epoch 1/20, Loss: 0.16336524870135996 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.0521672866307199\n",
      "SNR: 0/30, LS+LI, Epoch 2/20, Loss: 0.047264187227897714 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.035475545252362885\n",
      "SNR: 0/30, LS+LI, Epoch 3/20, Loss: 0.033501244694142056 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.03055703950424989\n",
      "SNR: 0/30, LS+LI, Epoch 4/20, Loss: 0.032241091132164 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.03014695450353126\n",
      "SNR: 0/30, LS+LI, Epoch 5/20, Loss: 0.03174224399989194 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.029832657271375258\n",
      "SNR: 0/30, LS+LI, Epoch 6/20, Loss: 0.03147024043494522 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.030014399516706664\n",
      "SNR: 0/30, LS+LI, Epoch 7/20, Loss: 0.031188737845398707 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.029335121701781947\n",
      "SNR: 0/30, LS+LI, Epoch 8/20, Loss: 0.031044338516300857 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.029328659332046907\n",
      "SNR: 0/30, LS+LI, Epoch 9/20, Loss: 0.030737685804983768 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02911466662772\n",
      "SNR: 0/30, LS+LI, Epoch 10/20, Loss: 0.030738467728663788 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.028943562026446063\n",
      "SNR: 0/30, LS+LI, Epoch 11/20, Loss: 0.030522720307333044 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.028848969377577305\n",
      "SNR: 0/30, LS+LI, Epoch 12/20, Loss: 0.03024046905892025 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02901147787148754\n",
      "SNR: 0/30, LS+LI, Epoch 13/20, Loss: 0.030223919604305583 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.028481042323013146\n",
      "SNR: 0/30, LS+LI, Epoch 14/20, Loss: 0.030036375198587036 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.028289019285390776\n",
      "SNR: 0/30, LS+LI, Epoch 15/20, Loss: 0.029860879815561642 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.028350452659651637\n",
      "SNR: 0/30, LS+LI, Epoch 16/20, Loss: 0.029867505101431713 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.028672231128439307\n",
      "SNR: 0/30, LS+LI, Epoch 17/20, Loss: 0.029822080359231718 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02892921251865725\n",
      "SNR: 0/30, LS+LI, Epoch 18/20, Loss: 0.02957549694427612 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.027961978611225884\n",
      "SNR: 0/30, LS+LI, Epoch 19/20, Loss: 0.02952525228962745 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.0281137569497029\n",
      "SNR: 0/30, LS+LI, Epoch 20/20, Loss: 0.02949081988322853 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.028052897813419502\n",
      "LI+NN NMSE: 0.09946275502443314\n",
      "LS+LI NMSE: 0.08247403800487518\n",
      " Training for LS\n",
      "SNR: 0/30, LS, Epoch 1/20, Loss: 0.29158002042239256 \n",
      "SNR: 0/30, LS, Val Loss: 0.22521580941975117\n",
      "SNR: 0/30, LS, Epoch 2/20, Loss: 0.17798964863661493 \n",
      "SNR: 0/30, LS, Val Loss: 0.07627826339254777\n",
      "SNR: 0/30, LS, Epoch 3/20, Loss: 0.03092632975687485 \n",
      "SNR: 0/30, LS, Val Loss: 0.0178954410366714\n",
      "SNR: 0/30, LS, Epoch 4/20, Loss: 0.01647518721344595 \n",
      "SNR: 0/30, LS, Val Loss: 0.014360888240238031\n",
      "SNR: 0/30, LS, Epoch 5/20, Loss: 0.013360404635383056 \n",
      "SNR: 0/30, LS, Val Loss: 0.011761384938533107\n",
      "SNR: 0/30, LS, Epoch 6/20, Loss: 0.011032535740644625 \n",
      "SNR: 0/30, LS, Val Loss: 0.009900856840734681\n",
      "SNR: 0/30, LS, Epoch 7/20, Loss: 0.009337804642146324 \n",
      "SNR: 0/30, LS, Val Loss: 0.00842657006190469\n",
      "SNR: 0/30, LS, Epoch 8/20, Loss: 0.008061971271060186 \n",
      "SNR: 0/30, LS, Val Loss: 0.00735832810945188\n",
      "SNR: 0/30, LS, Epoch 9/20, Loss: 0.007199030659982179 \n",
      "SNR: 0/30, LS, Val Loss: 0.0066783123183995485\n",
      "SNR: 0/30, LS, Epoch 10/20, Loss: 0.00667595075406503 \n",
      "SNR: 0/30, LS, Val Loss: 0.006275671670058121\n",
      "SNR: 0/30, LS, Epoch 11/20, Loss: 0.006330278045082889 \n",
      "SNR: 0/30, LS, Val Loss: 0.0062808078558494644\n",
      "SNR: 0/30, LS, Epoch 12/20, Loss: 0.006066548534309362 \n",
      "SNR: 0/30, LS, Val Loss: 0.005892782045217852\n",
      "SNR: 0/30, LS, Epoch 13/20, Loss: 0.00592595620292912 \n",
      "SNR: 0/30, LS, Val Loss: 0.005771068642692019\n",
      "SNR: 0/30, LS, Epoch 14/20, Loss: 0.005837958483578693 \n",
      "SNR: 0/30, LS, Val Loss: 0.005705391871742904\n",
      "SNR: 0/30, LS, Epoch 15/20, Loss: 0.005705518903911556 \n",
      "SNR: 0/30, LS, Val Loss: 0.005750745787130048\n",
      "SNR: 0/30, LS, Epoch 16/20, Loss: 0.005625461615474507 \n",
      "SNR: 0/30, LS, Val Loss: 0.005486718864024927\n",
      "SNR: 0/30, LS, Epoch 17/20, Loss: 0.005565457555493063 \n",
      "SNR: 0/30, LS, Val Loss: 0.005454276222735643\n",
      "SNR: 0/30, LS, Epoch 18/20, Loss: 0.005475441063062685 \n",
      "SNR: 0/30, LS, Val Loss: 0.006016005490285655\n",
      "SNR: 0/30, LS, Epoch 19/20, Loss: 0.005417448938778132 \n",
      "SNR: 0/30, LS, Val Loss: 0.005275828356388956\n",
      "SNR: 0/30, LS, Epoch 20/20, Loss: 0.005340906148223151 \n",
      "SNR: 0/30, LS, Val Loss: 0.005217214153769116\n",
      "LS+LI NMSE: 0.01792595349252224\n",
      " SNR: 5/30\n",
      " Training for LS+LI\n",
      "SNR: 5/30, LS+LI, Epoch 1/20, Loss: 0.15161878451483674 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.038654634806637965\n",
      "SNR: 5/30, LS+LI, Epoch 2/20, Loss: 0.02819857343404305 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.017256718633385997\n",
      "SNR: 5/30, LS+LI, Epoch 3/20, Loss: 0.014383247818355219 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.013052616268396378\n",
      "SNR: 5/30, LS+LI, Epoch 4/20, Loss: 0.01324565181886722 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.013017788315967968\n",
      "SNR: 5/30, LS+LI, Epoch 5/20, Loss: 0.013122128894423494 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.012593140980849663\n",
      "SNR: 5/30, LS+LI, Epoch 6/20, Loss: 0.012919234312462186 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.012462074810173362\n",
      "SNR: 5/30, LS+LI, Epoch 7/20, Loss: 0.012830963231494078 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.012417001707945019\n",
      "SNR: 5/30, LS+LI, Epoch 8/20, Loss: 0.01277981380387993 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.012323713337536901\n",
      "SNR: 5/30, LS+LI, Epoch 9/20, Loss: 0.01272757467352739 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.01303184567950666\n",
      "SNR: 5/30, LS+LI, Epoch 10/20, Loss: 0.012661915150295832 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.012053814386793723\n",
      "SNR: 5/30, LS+LI, Epoch 11/20, Loss: 0.012481554118584436 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.012317988943929473\n",
      "SNR: 5/30, LS+LI, Epoch 12/20, Loss: 0.012356798325914263 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.011889466414383302\n",
      "SNR: 5/30, LS+LI, Epoch 13/20, Loss: 0.012286942525960432 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.011836734600365162\n",
      "SNR: 5/30, LS+LI, Epoch 14/20, Loss: 0.012225133938585767 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.01176353379075105\n",
      "SNR: 5/30, LS+LI, Epoch 15/20, Loss: 0.012173447585065323 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.01165195288679873\n",
      "SNR: 5/30, LS+LI, Epoch 16/20, Loss: 0.012082845226188403 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.011740733267894635\n",
      "SNR: 5/30, LS+LI, Epoch 17/20, Loss: 0.012039432060065688 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.011919682418617109\n",
      "SNR: 5/30, LS+LI, Epoch 18/20, Loss: 0.011991257516501269 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.012147589935921133\n",
      "SNR: 5/30, LS+LI, Epoch 19/20, Loss: 0.011878414457608568 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.011642870784271508\n",
      "SNR: 5/30, LS+LI, Epoch 20/20, Loss: 0.011876019347219331 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.011489746277220547\n",
      "LI+NN NMSE: 0.0368926115334034\n",
      "LS+LI NMSE: 0.02603435516357422\n",
      " Training for LS\n",
      "SNR: 5/30, LS, Epoch 1/20, Loss: 0.2672054057959283 \n",
      "SNR: 5/30, LS, Val Loss: 0.17309745214879513\n",
      "SNR: 5/30, LS, Epoch 2/20, Loss: 0.12609259775000634 \n",
      "SNR: 5/30, LS, Val Loss: 0.07539289087677996\n",
      "SNR: 5/30, LS, Epoch 3/20, Loss: 0.034752485250113624 \n",
      "SNR: 5/30, LS, Val Loss: 0.01834994259600838\n",
      "SNR: 5/30, LS, Epoch 4/20, Loss: 0.015012361339782135 \n",
      "SNR: 5/30, LS, Val Loss: 0.012994918273761868\n",
      "SNR: 5/30, LS, Epoch 5/20, Loss: 0.01164427767553837 \n",
      "SNR: 5/30, LS, Val Loss: 0.010580364886360863\n",
      "SNR: 5/30, LS, Epoch 6/20, Loss: 0.009558206760684158 \n",
      "SNR: 5/30, LS, Val Loss: 0.008787982786695162\n",
      "SNR: 5/30, LS, Epoch 7/20, Loss: 0.007885642958307031 \n",
      "SNR: 5/30, LS, Val Loss: 0.0072302120000434416\n",
      "SNR: 5/30, LS, Epoch 8/20, Loss: 0.006524854053807731 \n",
      "SNR: 5/30, LS, Val Loss: 0.006021487468387932\n",
      "SNR: 5/30, LS, Epoch 9/20, Loss: 0.005389864219924306 \n",
      "SNR: 5/30, LS, Val Loss: 0.004968116836001475\n",
      "SNR: 5/30, LS, Epoch 10/20, Loss: 0.004531424031546801 \n",
      "SNR: 5/30, LS, Val Loss: 0.004259346208224694\n",
      "SNR: 5/30, LS, Epoch 11/20, Loss: 0.003981714561065235 \n",
      "SNR: 5/30, LS, Val Loss: 0.003808539671202501\n",
      "SNR: 5/30, LS, Epoch 12/20, Loss: 0.003683743290695371 \n",
      "SNR: 5/30, LS, Val Loss: 0.004030070063890889\n",
      "SNR: 5/30, LS, Epoch 13/20, Loss: 0.0035247292823739128 \n",
      "SNR: 5/30, LS, Val Loss: 0.003469368336178983\n",
      "SNR: 5/30, LS, Epoch 14/20, Loss: 0.0033467189146039805 \n",
      "SNR: 5/30, LS, Val Loss: 0.003326346205237011\n",
      "SNR: 5/30, LS, Epoch 15/20, Loss: 0.0032564308356014217 \n",
      "SNR: 5/30, LS, Val Loss: 0.0032193602819461375\n",
      "SNR: 5/30, LS, Epoch 16/20, Loss: 0.003168368982934937 \n",
      "SNR: 5/30, LS, Val Loss: 0.0031285676814150065\n",
      "SNR: 5/30, LS, Epoch 17/20, Loss: 0.0031166983627723436 \n",
      "SNR: 5/30, LS, Val Loss: 0.0032490541440589973\n",
      "SNR: 5/30, LS, Epoch 18/20, Loss: 0.003040597951592933 \n",
      "SNR: 5/30, LS, Val Loss: 0.003057525143958628\n",
      "SNR: 5/30, LS, Epoch 19/20, Loss: 0.003017151864282418 \n",
      "SNR: 5/30, LS, Val Loss: 0.0029473435618759445\n",
      "SNR: 5/30, LS, Epoch 20/20, Loss: 0.002918276750242872 \n",
      "SNR: 5/30, LS, Val Loss: 0.0028820815011082837\n",
      "LS+LI NMSE: 0.009327171370387077\n",
      " SNR: 10/30\n",
      " Training for LS+LI\n",
      "SNR: 10/30, LS+LI, Epoch 1/20, Loss: 0.13278332121190753 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.024944000722219545\n",
      "SNR: 10/30, LS+LI, Epoch 2/20, Loss: 0.013935551946374154 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.005651124559032421\n",
      "SNR: 10/30, LS+LI, Epoch 3/20, Loss: 0.006113171076060891 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.005172403228546803\n",
      "SNR: 10/30, LS+LI, Epoch 4/20, Loss: 0.00590690306853503 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.005225307744694874\n",
      "SNR: 10/30, LS+LI, Epoch 5/20, Loss: 0.00579153704756668 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004909190960461274\n",
      "SNR: 10/30, LS+LI, Epoch 6/20, Loss: 0.005668680848655208 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004899409483186901\n",
      "SNR: 10/30, LS+LI, Epoch 7/20, Loss: 0.005617067413773424 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004825217756054674\n",
      "SNR: 10/30, LS+LI, Epoch 8/20, Loss: 0.005514171788247653 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004666467245745783\n",
      "SNR: 10/30, LS+LI, Epoch 9/20, Loss: 0.005462202870535968 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.0045695798471570015\n",
      "SNR: 10/30, LS+LI, Epoch 10/20, Loss: 0.005366886469148247 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.0045142750507996725\n",
      "SNR: 10/30, LS+LI, Epoch 11/20, Loss: 0.005318036717869858 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004551006917608902\n",
      "SNR: 10/30, LS+LI, Epoch 12/20, Loss: 0.005272781848676945 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004428041681724911\n",
      "SNR: 10/30, LS+LI, Epoch 13/20, Loss: 0.005206984709856091 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004476352264949431\n",
      "SNR: 10/30, LS+LI, Epoch 14/20, Loss: 0.005142918591950052 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004292765603167936\n",
      "SNR: 10/30, LS+LI, Epoch 15/20, Loss: 0.005105467228504764 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004285903899775197\n",
      "SNR: 10/30, LS+LI, Epoch 16/20, Loss: 0.005102913007186264 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004362428153399378\n",
      "SNR: 10/30, LS+LI, Epoch 17/20, Loss: 0.005037600447718829 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004709924076450989\n",
      "SNR: 10/30, LS+LI, Epoch 18/20, Loss: 0.005021838042516225 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004159779763237263\n",
      "SNR: 10/30, LS+LI, Epoch 19/20, Loss: 0.0049641138371449 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004218745181181778\n",
      "SNR: 10/30, LS+LI, Epoch 20/20, Loss: 0.00497660325208476 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004152674489887431\n",
      "LI+NN NMSE: 0.013915733434259892\n",
      "LS+LI NMSE: 0.008258768357336521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thien/Hprediction/one_shot_Hest_cleanver/Torch_code/helper/plotfig.py:30: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(10, 5))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training for LS\n",
      "SNR: 10/30, LS, Epoch 1/20, Loss: 0.2514993558720787 \n",
      "SNR: 10/30, LS, Val Loss: 0.1291678585112095\n",
      "SNR: 10/30, LS, Epoch 2/20, Loss: 0.10807055485720682 \n",
      "SNR: 10/30, LS, Val Loss: 0.0843912847340107\n",
      "SNR: 10/30, LS, Epoch 3/20, Loss: 0.058641436279262646 \n",
      "SNR: 10/30, LS, Val Loss: 0.03401209398483237\n",
      "SNR: 10/30, LS, Epoch 4/20, Loss: 0.022021580220080248 \n",
      "SNR: 10/30, LS, Val Loss: 0.014824645477347076\n",
      "SNR: 10/30, LS, Epoch 5/20, Loss: 0.011658728851171413 \n",
      "SNR: 10/30, LS, Val Loss: 0.009119359504741928\n",
      "SNR: 10/30, LS, Epoch 6/20, Loss: 0.007466999454294691 \n",
      "SNR: 10/30, LS, Val Loss: 0.005865418371589233\n",
      "SNR: 10/30, LS, Epoch 7/20, Loss: 0.004795691256623457 \n",
      "SNR: 10/30, LS, Val Loss: 0.003972399126117428\n",
      "SNR: 10/30, LS, Epoch 8/20, Loss: 0.0034712087651161423 \n",
      "SNR: 10/30, LS, Val Loss: 0.0031738564236244806\n",
      "SNR: 10/30, LS, Epoch 9/20, Loss: 0.003031887747992826 \n",
      "SNR: 10/30, LS, Val Loss: 0.0030244251247495413\n",
      "SNR: 10/30, LS, Epoch 10/20, Loss: 0.002806633826123901 \n",
      "SNR: 10/30, LS, Val Loss: 0.0026842537239038697\n",
      "SNR: 10/30, LS, Epoch 11/20, Loss: 0.0026335172680215816 \n",
      "SNR: 10/30, LS, Val Loss: 0.0025724418907581517\n",
      "SNR: 10/30, LS, Epoch 12/20, Loss: 0.0025001818228900284 \n",
      "SNR: 10/30, LS, Val Loss: 0.002389052552947154\n",
      "SNR: 10/30, LS, Epoch 13/20, Loss: 0.002376725706433195 \n",
      "SNR: 10/30, LS, Val Loss: 0.0022746475151507184\n",
      "SNR: 10/30, LS, Epoch 14/20, Loss: 0.0022766815871922393 \n",
      "SNR: 10/30, LS, Val Loss: 0.002195154176054833\n",
      "SNR: 10/30, LS, Epoch 15/20, Loss: 0.002185984274330964 \n",
      "SNR: 10/30, LS, Val Loss: 0.002091838478615197\n",
      "SNR: 10/30, LS, Epoch 16/20, Loss: 0.0021063539258315053 \n",
      "SNR: 10/30, LS, Val Loss: 0.002033254136525405\n",
      "SNR: 10/30, LS, Epoch 17/20, Loss: 0.002032409691237983 \n",
      "SNR: 10/30, LS, Val Loss: 0.001965052079564581\n",
      "SNR: 10/30, LS, Epoch 18/20, Loss: 0.001975402654388385 \n",
      "SNR: 10/30, LS, Val Loss: 0.0019656696191911274\n",
      "SNR: 10/30, LS, Epoch 19/20, Loss: 0.0019144946905221827 \n",
      "SNR: 10/30, LS, Val Loss: 0.0018354821222601458\n",
      "SNR: 10/30, LS, Epoch 20/20, Loss: 0.0018601163721523515 \n",
      "SNR: 10/30, LS, Val Loss: 0.001803107516025193\n",
      "LS+LI NMSE: 0.0062881954945623875\n",
      " SNR: 15/30\n",
      " Training for LS+LI\n",
      "SNR: 15/30, LS+LI, Epoch 1/20, Loss: 0.12769480627767815 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.024961296080922086\n",
      "SNR: 15/30, LS+LI, Epoch 2/20, Loss: 0.012194187616662003 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0040856521906486405\n",
      "SNR: 15/30, LS+LI, Epoch 3/20, Loss: 0.0031927983797533383 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.003141709176513056\n",
      "SNR: 15/30, LS+LI, Epoch 4/20, Loss: 0.0028612392379367482 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.002966320423486953\n",
      "SNR: 15/30, LS+LI, Epoch 5/20, Loss: 0.0027315186602893367 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.00286249215908659\n",
      "SNR: 15/30, LS+LI, Epoch 6/20, Loss: 0.002656806381373196 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.003169209997092063\n",
      "SNR: 15/30, LS+LI, Epoch 7/20, Loss: 0.002587708999001707 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.002971186195888246\n",
      "SNR: 15/30, LS+LI, Epoch 8/20, Loss: 0.002488846464156367 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.002608208693951989\n",
      "SNR: 15/30, LS+LI, Epoch 9/20, Loss: 0.0024036817577676754 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0025652536714915186\n",
      "SNR: 15/30, LS+LI, Epoch 10/20, Loss: 0.0023488680541626002 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0024453697308975584\n",
      "SNR: 15/30, LS+LI, Epoch 11/20, Loss: 0.002263901787487292 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.002440458284885002\n",
      "SNR: 15/30, LS+LI, Epoch 12/20, Loss: 0.00219436647718551 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.002406995074125007\n",
      "SNR: 15/30, LS+LI, Epoch 13/20, Loss: 0.0021822055931996075 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0023133021265190714\n",
      "SNR: 15/30, LS+LI, Epoch 14/20, Loss: 0.002121181031320207 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0022620843859234205\n",
      "SNR: 15/30, LS+LI, Epoch 15/20, Loss: 0.002091903321325115 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0022292987850960344\n",
      "SNR: 15/30, LS+LI, Epoch 16/20, Loss: 0.002056152890044458 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0022237920541859544\n",
      "SNR: 15/30, LS+LI, Epoch 17/20, Loss: 0.0019969305416596777 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0021750894520664588\n",
      "SNR: 15/30, LS+LI, Epoch 18/20, Loss: 0.001993206533658696 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.002149093799137821\n",
      "SNR: 15/30, LS+LI, Epoch 19/20, Loss: 0.0019816002643685605 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0021453090060579902\n",
      "SNR: 15/30, LS+LI, Epoch 20/20, Loss: 0.0019409355861918613 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0021459204581333324\n",
      "LI+NN NMSE: 0.006962048355489969\n",
      "LS+LI NMSE: 0.0026962857227772474\n",
      " Training for LS\n",
      "SNR: 15/30, LS, Epoch 1/20, Loss: 0.23928936390150893 \n",
      "SNR: 15/30, LS, Val Loss: 0.11396358193208773\n",
      "SNR: 15/30, LS, Epoch 2/20, Loss: 0.09751992381297715 \n",
      "SNR: 15/30, LS, Val Loss: 0.08160596402982871\n",
      "SNR: 15/30, LS, Epoch 3/20, Loss: 0.06385613806398198 \n",
      "SNR: 15/30, LS, Val Loss: 0.04713388439267874\n",
      "SNR: 15/30, LS, Epoch 4/20, Loss: 0.03233605903564113 \n",
      "SNR: 15/30, LS, Val Loss: 0.020814333809539676\n",
      "SNR: 15/30, LS, Epoch 5/20, Loss: 0.014446752454790443 \n",
      "SNR: 15/30, LS, Val Loss: 0.009906365109297136\n",
      "SNR: 15/30, LS, Epoch 6/20, Loss: 0.007438186065081765 \n",
      "SNR: 15/30, LS, Val Loss: 0.005569041609608878\n",
      "SNR: 15/30, LS, Epoch 7/20, Loss: 0.004657895763008164 \n",
      "SNR: 15/30, LS, Val Loss: 0.004000499514707674\n",
      "SNR: 15/30, LS, Epoch 8/20, Loss: 0.003627278294534155 \n",
      "SNR: 15/30, LS, Val Loss: 0.0033387958246748894\n",
      "SNR: 15/30, LS, Epoch 9/20, Loss: 0.003131766035913093 \n",
      "SNR: 15/30, LS, Val Loss: 0.002984131705792\n",
      "SNR: 15/30, LS, Epoch 10/20, Loss: 0.0027974320259775117 \n",
      "SNR: 15/30, LS, Val Loss: 0.002682955275910596\n",
      "SNR: 15/30, LS, Epoch 11/20, Loss: 0.002549666295860811 \n",
      "SNR: 15/30, LS, Val Loss: 0.0024499316156531372\n",
      "SNR: 15/30, LS, Epoch 12/20, Loss: 0.0023442107410686515 \n",
      "SNR: 15/30, LS, Val Loss: 0.0022684084348535785\n",
      "SNR: 15/30, LS, Epoch 13/20, Loss: 0.0021813970807211306 \n",
      "SNR: 15/30, LS, Val Loss: 0.002121973103688409\n",
      "SNR: 15/30, LS, Epoch 14/20, Loss: 0.0020427442447393026 \n",
      "SNR: 15/30, LS, Val Loss: 0.0020191378231781223\n",
      "SNR: 15/30, LS, Epoch 15/20, Loss: 0.0019307178226161269 \n",
      "SNR: 15/30, LS, Val Loss: 0.0018856340223768104\n",
      "SNR: 15/30, LS, Epoch 16/20, Loss: 0.0018306184850522492 \n",
      "SNR: 15/30, LS, Val Loss: 0.001790117933220851\n",
      "SNR: 15/30, LS, Epoch 17/20, Loss: 0.0017400068043048797 \n",
      "SNR: 15/30, LS, Val Loss: 0.0017324277772180114\n",
      "SNR: 15/30, LS, Epoch 18/20, Loss: 0.001658953966038062 \n",
      "SNR: 15/30, LS, Val Loss: 0.0016246091884871323\n",
      "SNR: 15/30, LS, Epoch 19/20, Loss: 0.0015872270566190012 \n",
      "SNR: 15/30, LS, Val Loss: 0.0015491033603514854\n",
      "SNR: 15/30, LS, Epoch 20/20, Loss: 0.0015117743624245177 \n",
      "SNR: 15/30, LS, Val Loss: 0.001494510497044151\n",
      "LS+LI NMSE: 0.00515271769836545\n",
      " SNR: 20/30\n",
      " Training for LS+LI\n",
      "SNR: 20/30, LS+LI, Epoch 1/20, Loss: 0.12666748186554944 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.024042558545867603\n",
      "SNR: 20/30, LS+LI, Epoch 2/20, Loss: 0.010958927684684865 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.003001942444825545\n",
      "SNR: 20/30, LS+LI, Epoch 3/20, Loss: 0.002187889446804097 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.001809020866251861\n",
      "SNR: 20/30, LS+LI, Epoch 4/20, Loss: 0.0017295809708176052 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0015796518952508147\n",
      "SNR: 20/30, LS+LI, Epoch 5/20, Loss: 0.00156781853391042 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0014415402789988245\n",
      "SNR: 20/30, LS+LI, Epoch 6/20, Loss: 0.0014472881045035724 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0013338760715366031\n",
      "SNR: 20/30, LS+LI, Epoch 7/20, Loss: 0.0013215162429386878 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0011840610507836875\n",
      "SNR: 20/30, LS+LI, Epoch 8/20, Loss: 0.001193359454155503 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0011576327257595647\n",
      "SNR: 20/30, LS+LI, Epoch 9/20, Loss: 0.0011116502412345888 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0010288447835288632\n",
      "SNR: 20/30, LS+LI, Epoch 10/20, Loss: 0.0010351286549812717 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.000965882155772609\n",
      "SNR: 20/30, LS+LI, Epoch 11/20, Loss: 0.001002376065485751 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0009177012907457538\n",
      "SNR: 20/30, LS+LI, Epoch 12/20, Loss: 0.0009592019204778762 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0009997390807257034\n",
      "SNR: 20/30, LS+LI, Epoch 13/20, Loss: 0.0009254641421162292 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0008470801597771546\n",
      "SNR: 20/30, LS+LI, Epoch 14/20, Loss: 0.0009083551459019027 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0008768983243498951\n",
      "SNR: 20/30, LS+LI, Epoch 15/20, Loss: 0.0008827294205721136 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0008428960184877118\n",
      "SNR: 20/30, LS+LI, Epoch 16/20, Loss: 0.0008784027620031969 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0008701226955357318\n",
      "SNR: 20/30, LS+LI, Epoch 17/20, Loss: 0.0008551967122804115 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0007798132161648633\n",
      "SNR: 20/30, LS+LI, Epoch 18/20, Loss: 0.0008539670546963295 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.000797248758317437\n",
      "SNR: 20/30, LS+LI, Epoch 19/20, Loss: 0.0008465160268801495 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.000775102133047767\n",
      "SNR: 20/30, LS+LI, Epoch 20/20, Loss: 0.0008447518306242597 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0007757726950027669\n",
      "LI+NN NMSE: 0.002390786074101925\n",
      "LS+LI NMSE: 0.0009251029696315527\n",
      " Training for LS\n",
      "SNR: 20/30, LS, Epoch 1/20, Loss: 0.2358908771288277 \n",
      "SNR: 20/30, LS, Val Loss: 0.10476261842995882\n",
      "SNR: 20/30, LS, Epoch 2/20, Loss: 0.08473501004057356 \n",
      "SNR: 20/30, LS, Val Loss: 0.06859381295119722\n",
      "SNR: 20/30, LS, Epoch 3/20, Loss: 0.05154434606285378 \n",
      "SNR: 20/30, LS, Val Loss: 0.03690609491119782\n",
      "SNR: 20/30, LS, Epoch 4/20, Loss: 0.024121925087258366 \n",
      "SNR: 20/30, LS, Val Loss: 0.016267617465928197\n",
      "SNR: 20/30, LS, Epoch 5/20, Loss: 0.01227177449013337 \n",
      "SNR: 20/30, LS, Val Loss: 0.009244219788039723\n",
      "SNR: 20/30, LS, Epoch 6/20, Loss: 0.007203477031394544 \n",
      "SNR: 20/30, LS, Val Loss: 0.00559936403684939\n",
      "SNR: 20/30, LS, Epoch 7/20, Loss: 0.004646794646886168 \n",
      "SNR: 20/30, LS, Val Loss: 0.0038243209710344672\n",
      "SNR: 20/30, LS, Epoch 8/20, Loss: 0.0033815483998341283 \n",
      "SNR: 20/30, LS, Val Loss: 0.0029646225448232144\n",
      "SNR: 20/30, LS, Epoch 9/20, Loss: 0.0027416934833244077 \n",
      "SNR: 20/30, LS, Val Loss: 0.002504298376152292\n",
      "SNR: 20/30, LS, Epoch 10/20, Loss: 0.0023878139263885743 \n",
      "SNR: 20/30, LS, Val Loss: 0.0022355541586875916\n",
      "SNR: 20/30, LS, Epoch 11/20, Loss: 0.0021385635781830344 \n",
      "SNR: 20/30, LS, Val Loss: 0.002004150087789943\n",
      "SNR: 20/30, LS, Epoch 12/20, Loss: 0.001946846189180223 \n",
      "SNR: 20/30, LS, Val Loss: 0.0018375307893923794\n",
      "SNR: 20/30, LS, Epoch 13/20, Loss: 0.00178777414378664 \n",
      "SNR: 20/30, LS, Val Loss: 0.0017179967981064692\n",
      "SNR: 20/30, LS, Epoch 14/20, Loss: 0.0016568493816107143 \n",
      "SNR: 20/30, LS, Val Loss: 0.0015951673995004967\n",
      "SNR: 20/30, LS, Epoch 15/20, Loss: 0.0015493222595293922 \n",
      "SNR: 20/30, LS, Val Loss: 0.001479498198023066\n",
      "SNR: 20/30, LS, Epoch 16/20, Loss: 0.0014608388151143594 \n",
      "SNR: 20/30, LS, Val Loss: 0.001388470006835026\n",
      "SNR: 20/30, LS, Epoch 17/20, Loss: 0.0013762239982272582 \n",
      "SNR: 20/30, LS, Val Loss: 0.0013328814723839362\n",
      "SNR: 20/30, LS, Epoch 18/20, Loss: 0.001298352255887066 \n",
      "SNR: 20/30, LS, Val Loss: 0.0012582718894312468\n",
      "SNR: 20/30, LS, Epoch 19/20, Loss: 0.0012345784075793321 \n",
      "SNR: 20/30, LS, Val Loss: 0.0011935569321697888\n",
      "SNR: 20/30, LS, Epoch 20/20, Loss: 0.001181115842387449 \n",
      "SNR: 20/30, LS, Val Loss: 0.00113123563399616\n",
      "LS+LI NMSE: 0.003766775829717517\n",
      " SNR: 25/30\n",
      " Training for LS+LI\n",
      "SNR: 25/30, LS+LI, Epoch 1/20, Loss: 0.13216731838402476 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.02159167523495853\n",
      "SNR: 25/30, LS+LI, Epoch 2/20, Loss: 0.010872584659292703 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0032481405845222375\n",
      "SNR: 25/30, LS+LI, Epoch 3/20, Loss: 0.0017469288207277065 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.001328655508890127\n",
      "SNR: 25/30, LS+LI, Epoch 4/20, Loss: 0.0012163438040090006 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0011477235651303392\n",
      "SNR: 25/30, LS+LI, Epoch 5/20, Loss: 0.0010661059335833139 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.001012438098162723\n",
      "SNR: 25/30, LS+LI, Epoch 6/20, Loss: 0.000942567369720834 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0008985526898565391\n",
      "SNR: 25/30, LS+LI, Epoch 7/20, Loss: 0.0008534002267654137 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0008223519398598\n",
      "SNR: 25/30, LS+LI, Epoch 8/20, Loss: 0.0007688208410544044 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.000736684822186362\n",
      "SNR: 25/30, LS+LI, Epoch 9/20, Loss: 0.0007008036876922603 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0006712751589172209\n",
      "SNR: 25/30, LS+LI, Epoch 10/20, Loss: 0.0006558318657853576 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0006263782731063353\n",
      "SNR: 25/30, LS+LI, Epoch 11/20, Loss: 0.0006064078463690983 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0005844764770396674\n",
      "SNR: 25/30, LS+LI, Epoch 12/20, Loss: 0.0005724374344630056 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0005441365974547807\n",
      "SNR: 25/30, LS+LI, Epoch 13/20, Loss: 0.000545093172320884 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0006160015970332703\n",
      "SNR: 25/30, LS+LI, Epoch 14/20, Loss: 0.000532932137629008 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0006432736675681857\n",
      "SNR: 25/30, LS+LI, Epoch 15/20, Loss: 0.0005026177202332949 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0005309533274460895\n",
      "SNR: 25/30, LS+LI, Epoch 16/20, Loss: 0.0004801330000155209 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0004670838400973783\n",
      "SNR: 25/30, LS+LI, Epoch 17/20, Loss: 0.00047532404530309326 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0004704974599007983\n",
      "SNR: 25/30, LS+LI, Epoch 18/20, Loss: 0.0004722961385407033 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0004699584690873356\n",
      "SNR: 25/30, LS+LI, Epoch 19/20, Loss: 0.0004482336189899833 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.00045672151948868606\n",
      "SNR: 25/30, LS+LI, Epoch 20/20, Loss: 0.00043926553369619617 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0005579738014300043\n",
      "LI+NN NMSE: 0.0018206271342933178\n",
      "LS+LI NMSE: 0.0003691763849928975\n",
      " Training for LS\n",
      "SNR: 25/30, LS, Epoch 1/20, Loss: 0.24170867512279218 \n",
      "SNR: 25/30, LS, Val Loss: 0.10301368869841099\n",
      "SNR: 25/30, LS, Epoch 2/20, Loss: 0.07846317029014083 \n",
      "SNR: 25/30, LS, Val Loss: 0.06884242206191023\n",
      "SNR: 25/30, LS, Epoch 3/20, Loss: 0.04723372197903619 \n",
      "SNR: 25/30, LS, Val Loss: 0.033335550067325435\n",
      "SNR: 25/30, LS, Epoch 4/20, Loss: 0.02205543860442715 \n",
      "SNR: 25/30, LS, Val Loss: 0.015373148334523043\n",
      "SNR: 25/30, LS, Epoch 5/20, Loss: 0.011516270960801014 \n",
      "SNR: 25/30, LS, Val Loss: 0.008638184750452638\n",
      "SNR: 25/30, LS, Epoch 6/20, Loss: 0.006625599857179983 \n",
      "SNR: 25/30, LS, Val Loss: 0.005109435316020002\n",
      "SNR: 25/30, LS, Epoch 7/20, Loss: 0.004108211827824022 \n",
      "SNR: 25/30, LS, Val Loss: 0.0035035248147323728\n",
      "SNR: 25/30, LS, Epoch 8/20, Loss: 0.003081825636742212 \n",
      "SNR: 25/30, LS, Val Loss: 0.0028676391229964793\n",
      "SNR: 25/30, LS, Epoch 9/20, Loss: 0.002595081749616541 \n",
      "SNR: 25/30, LS, Val Loss: 0.0024787952521971115\n",
      "SNR: 25/30, LS, Epoch 10/20, Loss: 0.002264917172297909 \n",
      "SNR: 25/30, LS, Val Loss: 0.0021887616506622485\n",
      "SNR: 25/30, LS, Epoch 11/20, Loss: 0.0020122510786767644 \n",
      "SNR: 25/30, LS, Val Loss: 0.0019517458761887003\n",
      "SNR: 25/30, LS, Epoch 12/20, Loss: 0.0017942749782197988 \n",
      "SNR: 25/30, LS, Val Loss: 0.0017441054612087707\n",
      "SNR: 25/30, LS, Epoch 13/20, Loss: 0.0016081118241579521 \n",
      "SNR: 25/30, LS, Val Loss: 0.0015928235322159405\n",
      "SNR: 25/30, LS, Epoch 14/20, Loss: 0.0014488488074973003 \n",
      "SNR: 25/30, LS, Val Loss: 0.001423180602917758\n",
      "SNR: 25/30, LS, Epoch 15/20, Loss: 0.0013072722698654058 \n",
      "SNR: 25/30, LS, Val Loss: 0.0012866392110784848\n",
      "SNR: 25/30, LS, Epoch 16/20, Loss: 0.001183669928848596 \n",
      "SNR: 25/30, LS, Val Loss: 0.0011601091779690857\n",
      "SNR: 25/30, LS, Epoch 17/20, Loss: 0.0010766976254780105 \n",
      "SNR: 25/30, LS, Val Loss: 0.0010951912481687032\n",
      "SNR: 25/30, LS, Epoch 18/20, Loss: 0.0009885902022643217 \n",
      "SNR: 25/30, LS, Val Loss: 0.0009665231797650146\n",
      "SNR: 25/30, LS, Epoch 19/20, Loss: 0.0009013009571705456 \n",
      "SNR: 25/30, LS, Val Loss: 0.0008843347920143666\n",
      "SNR: 25/30, LS, Epoch 20/20, Loss: 0.0008349812334278912 \n",
      "SNR: 25/30, LS, Val Loss: 0.0008683356960924963\n",
      "LS+LI NMSE: 0.003245751140639186\n",
      " SNR: 30/30\n",
      " Training for LS+LI\n",
      "SNR: 30/30, LS+LI, Epoch 1/20, Loss: 0.11723988239496651 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.021413013028601807\n",
      "SNR: 30/30, LS+LI, Epoch 2/20, Loss: 0.008721504775383094 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0018530122615629807\n",
      "SNR: 30/30, LS+LI, Epoch 3/20, Loss: 0.001468337503478558 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0013445620279526338\n",
      "SNR: 30/30, LS+LI, Epoch 4/20, Loss: 0.001224977639157989 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0011715034561348148\n",
      "SNR: 30/30, LS+LI, Epoch 5/20, Loss: 0.0010512215831653576 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0010007523524109274\n",
      "SNR: 30/30, LS+LI, Epoch 6/20, Loss: 0.0008807101749151134 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.000827689558112373\n",
      "SNR: 30/30, LS+LI, Epoch 7/20, Loss: 0.0007309952415387598 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0007166388822952285\n",
      "SNR: 30/30, LS+LI, Epoch 8/20, Loss: 0.0006317533637652874 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.000614897086052224\n",
      "SNR: 30/30, LS+LI, Epoch 9/20, Loss: 0.0005395049430973975 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0005225227529687496\n",
      "SNR: 30/30, LS+LI, Epoch 10/20, Loss: 0.0004557192181225516 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0004331334275775589\n",
      "SNR: 30/30, LS+LI, Epoch 11/20, Loss: 0.00040014807133714086 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00039030375774018466\n",
      "SNR: 30/30, LS+LI, Epoch 12/20, Loss: 0.00035999330754235093 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0003830411721234365\n",
      "SNR: 30/30, LS+LI, Epoch 13/20, Loss: 0.00034195435540877413 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0003468697735418876\n",
      "SNR: 30/30, LS+LI, Epoch 14/20, Loss: 0.00032137413013560496 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0003598989557455449\n",
      "SNR: 30/30, LS+LI, Epoch 15/20, Loss: 0.00031717900085345945 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00032815617669257335\n",
      "SNR: 30/30, LS+LI, Epoch 16/20, Loss: 0.00028778021726676606 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0003211226988544998\n",
      "SNR: 30/30, LS+LI, Epoch 17/20, Loss: 0.00028534032544755094 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.000282465298975391\n",
      "SNR: 30/30, LS+LI, Epoch 18/20, Loss: 0.0002737627971985396 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00027966588519727037\n",
      "SNR: 30/30, LS+LI, Epoch 19/20, Loss: 0.00026225963305442196 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0002690861668573537\n",
      "SNR: 30/30, LS+LI, Epoch 20/20, Loss: 0.00025924658725536107 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00027591055671412806\n",
      "LI+NN NMSE: 0.0008675318094901741\n",
      "LS+LI NMSE: 0.00018632912542670965\n",
      " Training for LS\n",
      "SNR: 30/30, LS, Epoch 1/20, Loss: 0.231607237185287 \n",
      "SNR: 30/30, LS, Val Loss: 0.09805313373605411\n",
      "SNR: 30/30, LS, Epoch 2/20, Loss: 0.08501385183691389 \n",
      "SNR: 30/30, LS, Val Loss: 0.07206554446990292\n",
      "SNR: 30/30, LS, Epoch 3/20, Loss: 0.059343060423241986 \n",
      "SNR: 30/30, LS, Val Loss: 0.04677583587666353\n",
      "SNR: 30/30, LS, Epoch 4/20, Loss: 0.03591364515152308 \n",
      "SNR: 30/30, LS, Val Loss: 0.02589639904908836\n",
      "SNR: 30/30, LS, Epoch 5/20, Loss: 0.01793629933369927 \n",
      "SNR: 30/30, LS, Val Loss: 0.012119040669252476\n",
      "SNR: 30/30, LS, Epoch 6/20, Loss: 0.008917870470276564 \n",
      "SNR: 30/30, LS, Val Loss: 0.006416599537866811\n",
      "SNR: 30/30, LS, Epoch 7/20, Loss: 0.004890158027871558 \n",
      "SNR: 30/30, LS, Val Loss: 0.0036956236484305314\n",
      "SNR: 30/30, LS, Epoch 8/20, Loss: 0.0030462438676340303 \n",
      "SNR: 30/30, LS, Val Loss: 0.0025539240644623837\n",
      "SNR: 30/30, LS, Epoch 9/20, Loss: 0.0023229551042826604 \n",
      "SNR: 30/30, LS, Val Loss: 0.0021233127772575244\n",
      "SNR: 30/30, LS, Epoch 10/20, Loss: 0.0020247809473646454 \n",
      "SNR: 30/30, LS, Val Loss: 0.0019055503071285784\n",
      "SNR: 30/30, LS, Epoch 11/20, Loss: 0.001842275524101598 \n",
      "SNR: 30/30, LS, Val Loss: 0.001746098811660583\n",
      "SNR: 30/30, LS, Epoch 12/20, Loss: 0.0016942915689928623 \n",
      "SNR: 30/30, LS, Val Loss: 0.001613434932854337\n",
      "SNR: 30/30, LS, Epoch 13/20, Loss: 0.0015759960094715772 \n",
      "SNR: 30/30, LS, Val Loss: 0.0015104982109429936\n",
      "SNR: 30/30, LS, Epoch 14/20, Loss: 0.0014677708968520164 \n",
      "SNR: 30/30, LS, Val Loss: 0.0014064521237742156\n",
      "SNR: 30/30, LS, Epoch 15/20, Loss: 0.0013774140279205276 \n",
      "SNR: 30/30, LS, Val Loss: 0.0013208983048874263\n",
      "SNR: 30/30, LS, Epoch 16/20, Loss: 0.0012951464280440003 \n",
      "SNR: 30/30, LS, Val Loss: 0.0012415215896908194\n",
      "SNR: 30/30, LS, Epoch 17/20, Loss: 0.001221282352603474 \n",
      "SNR: 30/30, LS, Val Loss: 0.001170533262969305\n",
      "SNR: 30/30, LS, Epoch 18/20, Loss: 0.0011536537448870996 \n",
      "SNR: 30/30, LS, Val Loss: 0.0011133111596185092\n",
      "SNR: 30/30, LS, Epoch 19/20, Loss: 0.0010894901850514373 \n",
      "SNR: 30/30, LS, Val Loss: 0.001047246842063032\n",
      "SNR: 30/30, LS, Epoch 20/20, Loss: 0.0010277864216056902 \n",
      "SNR: 30/30, LS, Val Loss: 0.0009793371452057424\n",
      "LS+LI NMSE: 0.003341428004205227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for snr in SNR:\n",
    "    print(f\" SNR: {snr}/{SNR[-1]}\")\n",
    "\n",
    "    [trainLabels, valLabels], [H_equal_train, H_linear_train, H_practical_train], [H_equal_val, H_linear_val, H_practical_val] = loader.load_data(outer_file_path, rows, fc, device, snr)\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 1. When input is H_linear (after LS+LI)\n",
    "    print(f\" Training for LS+LI\")\n",
    "    # [samples, 2, 612, 14]\n",
    "    train_loader, trainLabel_min, trainLabel_max = loader.genLoader(H_linear_train, trainLabels, BATCH_SIZE, device, 'train', True, norm_approach)\n",
    "    val_loader,     valLabel_min,   valLabel_max = loader.genLoader(H_linear_val,     valLabels, BATCH_SIZE, device, 'valid', False, norm_approach)\n",
    "        # no shuffle in validation so valLabel_min and valLable_max remain the min and max arrays\n",
    "        # of valLabels\n",
    "        \n",
    "    # model\n",
    "    model = utils.CNN_Est().to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 1.5 Training loop\n",
    "    train_loss =[]\n",
    "    val_loss = []\n",
    "    H_NN_val = torch.empty_like(valLabels) # [nVal, 2, 612, 14]\n",
    "    min_H_true = []\n",
    "    max_H_true = []\n",
    "    num_epochs = NUM_EPOCHS\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        if (epoch == num_epochs-1):\n",
    "            i = 0\n",
    "        for inputs, targets, targets_min, targets_max in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_loss.append(avg_train_loss)\n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS+LI, Epoch {epoch+1}/{num_epochs}, Loss: {avg_train_loss} \")\n",
    "        \n",
    "        # Validation \n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets, val_targetsMin, val_targetsMax in val_loader:\n",
    "                val_inputs_real = val_inputs[:,0,:,:].unsqueeze(1)\n",
    "                val_inputs_imag = val_inputs[:,1,:,:].unsqueeze(1)\n",
    "                val_targets_real = val_targets[:,0,:,:].unsqueeze(1)\n",
    "                val_targets_imag = val_targets[:,1,:,:].unsqueeze(1)\n",
    "                \n",
    "                val_outputs_real = model(val_inputs_real)\n",
    "                val_loss_real = criterion(val_outputs_real, val_targets_real)\n",
    "                running_val_loss += val_loss_real.item()\n",
    "                \n",
    "                val_outputs_imag = model(val_inputs_imag)\n",
    "                val_loss_imag = criterion(val_outputs_imag, val_targets_imag)\n",
    "                running_val_loss += val_loss_imag.item()\n",
    "                \n",
    "                if (epoch == num_epochs-1): # the results after the last training \n",
    "                    H_NN_val[i:i+val_outputs_real.size(0),0,:,:].unsqueeze(1).copy_(val_outputs_real)\n",
    "                    H_NN_val[i:i+val_outputs_imag.size(0),1,:,:].unsqueeze(1).copy_(val_outputs_imag)\n",
    "                    \n",
    "                    i = i+val_outputs_imag.size(0)       \n",
    "                    \n",
    "                \n",
    "        avg_val_loss = running_val_loss / (len(val_loader)*2)\n",
    "        val_loss.append(avg_val_loss)    \n",
    "                \n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS+LI, Val Loss: {avg_val_loss}\")\n",
    "\n",
    "\n",
    "    save_folder = os.path.join(save_folder_model, str(snr)+'dB')\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    index_save = loader.find_incremental_filename(save_folder, 'CNN_', '_variable')\n",
    "\n",
    "    model_save_path = os.path.join(save_folder,  'CNN_' +str(index_save)+'_LS_LI_CNN_model.pth')\n",
    "    variable_save_path = os.path.join(save_folder, 'CNN_' +str(index_save)+'_variable.pth')\n",
    "    params_save_path = os.path.join(save_folder, 'CNN_' +str(index_save)+'_params.mat')\n",
    "    \n",
    "    params = {   \n",
    "                'SNR': snr,\n",
    "                'epoc': NUM_EPOCHS,\n",
    "                'rows': rowss,\n",
    "                'learning_rate': learning_rate,\n",
    "                'train_track_LI': train_loss,\n",
    "                'val_track_LI': val_loss,\n",
    "    }\n",
    "    variables = {             \n",
    "                'train_track_LI': train_loss,\n",
    "                'val_track_LI': val_loss,\n",
    "                # 'train_min_LI': trainData_min.cpu(),\n",
    "                # 'train_max_LI': trainData_max.cpu(),\n",
    "                # 'train_label_min': trainLabels_min.cpu(),\n",
    "                # 'train_label_max': trainLabels_max.cpu(),\n",
    "    }\n",
    "    # variable to save \n",
    "    # Save the models' state dictionaries \n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, model_save_path)\n",
    "\n",
    "    figure_save_path = os.path.join(save_folder_fig, str(snr) + 'dB') \n",
    "    \n",
    "    os.makedirs(figure_save_path, exist_ok=True)\n",
    "\n",
    "    plotfig.figLoss(train_loss, val_loss, index_save, figure_save_path, '_LS_LI_Loss.png')\n",
    "\n",
    "\n",
    "    # True channel\n",
    "    H_val_true = valLabels.cpu()\n",
    "    # convert to complex matrices\n",
    "    H_val_true_complex = torch.complex(H_val_true[:,0,:,:], H_val_true[:,1,:,:])\n",
    "    # variables['H_val_true'] = H_val_true # (nVal, 2, 612, 14)\n",
    "\n",
    "    plotfig.figTrueChan(H_val_true[-1,0,:,:], 'True Channel', index_save, figure_save_path, '_trueChannel.png')\n",
    "\n",
    "    # Estimated Channel \n",
    "    H_val_NN = H_NN_val.cpu()    \n",
    "    plotfig.figTrueChan(H_val_NN[-1,0,:,:], 'LI+CNN Estimated Channel (before de-normlized)', \n",
    "                            index_save, figure_save_path, '_LS_LI_CNN_estimatedChan_before_denorm.png')\n",
    "\n",
    "    # De-normalized                                                               \n",
    "    H_val_NN_denormd = utils.deMinMax(H_NN_val, valLabel_min, valLabel_max)\n",
    "                        #     H_NN_val == [nVal, 2, 612, 14] \n",
    "                        # valLabel_min == [nVal,1]\n",
    "                        \n",
    "    H_val_NN_denormd = H_val_NN_denormd.cpu()\n",
    "    # variables['H_val_LI_NN'] = H_val_NN_denormd # (nVal, 2, 612, 14)\n",
    "\n",
    "    # convert to complex matrices\n",
    "    H_val_NN_denormd_complex = torch.complex(H_val_NN_denormd[:,0,:,:], H_val_NN_denormd[:,1,:,:])\n",
    "    \n",
    "    nmse_LI_NN = utils.calNMSE(H_val_NN_denormd_complex, H_val_true_complex)\n",
    "    \n",
    "    variables['NMSE_LI_NN'] = nmse_LI_NN.cpu().mean()\n",
    "    nmse_LI_NN_val.append(variables['NMSE_LI_NN'].item())\n",
    "    print(f\"LI+NN NMSE: {variables['NMSE_LI_NN'].item()}\")\n",
    "    \n",
    "    plotfig.figPredChan(H_val_NN_denormd[-1,0,:,:], 'LI+CNN Estimated Channel (after de-normlized)',\n",
    "                            nmse_LI_NN[-1], index_save, figure_save_path, '_LS_LI_CNN_estimatedChan.png')\n",
    "#####\n",
    "##### above is LS+LI+NN \n",
    "\n",
    "##### following is Linear interpolated channel (only LS+LI)\n",
    "    H_val_linInterp = H_linear_val.cpu()\n",
    "    # convert to complex matrices\n",
    "    H_val_linInterp_complex = torch.complex(H_val_linInterp[:,0,:,:], H_val_linInterp[:,1,:,:]) # [?, 612, 14]\n",
    "\n",
    "    # NMSE of Linear Interpolation\n",
    "    # Calculate the NMSE\n",
    "    nmse_LI = utils.calNMSE(H_val_linInterp_complex, H_val_true_complex)\n",
    "    \n",
    "    variables['NMSE_LI'] = nmse_LI.cpu().mean()\n",
    "    nmse_LS_LI_val.append(variables['NMSE_LI'].item())\n",
    "    print(f\"LS+LI NMSE: {variables['NMSE_LI'].item()}\")\n",
    "    \n",
    "    plotfig.figPredChan(H_val_linInterp[-1,0,:,:], 'LS + Interpolate Estimated Channel',\n",
    "                            nmse_LI[-1], index_save, figure_save_path, '_LS_LI_estimatedChan.png')\n",
    "\n",
    "\n",
    "##########################################\n",
    "    # ------------------------------------------------------\n",
    "    # When Input of the NN is just H_equalized\n",
    "    print(f\" Training for LS\")\n",
    "    # [samples, 2, 612, 14]\n",
    "    H_LS_train = H_equal_train.cpu()\n",
    "    plotfig.figTrueChan(H_LS_train[0,0,:,:], 'LS Channel', index_save, figure_save_path, '_LS_Chan.png')\n",
    "    \n",
    "    # Split into training and validation sets for H_NN training\n",
    "    train_loader, trainLabel_min, trainLabel_max = loader.genLoader(H_equal_train, trainLabels, BATCH_SIZE, device, 'train',  True, norm_approach)\n",
    "    val_loader,     valLabel_min,   vallabel_max = loader.genLoader(H_equal_val,     valLabels, BATCH_SIZE, device, 'valid', False, norm_approach)\n",
    "\n",
    "\n",
    "    model2 = utils.CNN_Est().to(device)\n",
    "    optimizer2 = torch.optim.Adam(model2.parameters(), lr=learning_rate) \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Training loop\n",
    "    train_loss =[]\n",
    "    val_loss = []\n",
    "    H_NN_val = torch.empty_like(valLabels) # [nVal, 2, 612, 14]\n",
    "    num_epochs = NUM_EPOCHS\n",
    "    for epoch in range(num_epochs):\n",
    "        model2.train()\n",
    "        running_loss = 0.0\n",
    "        if (epoch == num_epochs-1):\n",
    "            i = 0\n",
    "        for inputs, targets, targets_min, targets_max in train_loader:\n",
    "            optimizer2.zero_grad()\n",
    "            outputs = model2(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_loss.append(avg_train_loss)\n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS, Epoch {epoch+1}/{num_epochs}, Loss: {avg_train_loss} \")\n",
    "        \n",
    "        # Validation \n",
    "        model2.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets, val_targetsMin, val_targetsMax in val_loader:\n",
    "                val_inputs_real = val_inputs[:,0,:,:].unsqueeze(1)\n",
    "                val_inputs_imag = val_inputs[:,1,:,:].unsqueeze(1)\n",
    "                val_targets_real = val_targets[:,0,:,:].unsqueeze(1)\n",
    "                val_targets_imag = val_targets[:,1,:,:].unsqueeze(1)\n",
    "                \n",
    "                val_outputs_real = model2(val_inputs_real)\n",
    "                val_loss_real = criterion(val_outputs_real, val_targets_real)\n",
    "                running_val_loss += val_loss_real.item()\n",
    "                \n",
    "                val_outputs_imag = model2(val_inputs_imag)\n",
    "                val_loss_imag = criterion(val_outputs_imag, val_targets_imag)\n",
    "                running_val_loss += val_loss_imag.item()\n",
    "                \n",
    "                if (epoch == num_epochs-1):\n",
    "                    H_NN_val[i:i+val_outputs_real.size(0),0,:,:].unsqueeze(1).copy_(val_outputs_real)\n",
    "                    H_NN_val[i:i+val_outputs_imag.size(0),1,:,:].unsqueeze(1).copy_(val_outputs_imag)\n",
    "                    i = i+val_outputs_imag.size(0)\n",
    "                \n",
    "        avg_val_loss = running_val_loss / (len(val_loader)*2)\n",
    "        val_loss.append(avg_val_loss)    \n",
    "                \n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS, Val Loss: {avg_val_loss}\")\n",
    "\n",
    "    plotfig.figLoss(train_loss, val_loss, index_save, figure_save_path, '_LS_Loss.png')\n",
    "\n",
    "    # De-normalized                                                                \n",
    "    H_val_NN_denormd = utils.deMinMax(H_NN_val, valLabel_min, valLabel_max)\n",
    "                        #     H_NN_val == [nVal, 2, 612, 14] \n",
    "                        # valLabel_min == [nVal,1]\n",
    "    H_val_NN_denormd = H_val_NN_denormd.cpu()\n",
    "\n",
    "    model_save_path = os.path.join(save_folder,  'CNN_' +str(index_save)+'_LS_CNN_model.pth')\n",
    "\n",
    "    # variables['H_val_LS_NN']= H_val_NN_denormd.cpu() # (nVal, 2, 612, 14)\n",
    "    variables['train_track_LS']= train_loss\n",
    "    variables['val_track_LS']= val_loss\n",
    "\n",
    "    # Save parameters\n",
    "    params['train_track_LS']= train_loss\n",
    "    params['val_track_LS']= val_loss\n",
    "    savemat(params_save_path, params)\n",
    "    # Save the models' state dictionaries \n",
    "    torch.save({'model_state_dict': model2.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "                }, model_save_path)\n",
    "\n",
    "\n",
    "    # NMSE of LS + NN\n",
    "    H_val_LS_NN_complex = torch.complex(H_val_NN_denormd[:,0,:,:], H_val_NN_denormd[:,1,:,:])\n",
    "    # Calculate the NMSE\n",
    "    nmse_LS_NN = utils.calNMSE(H_val_LS_NN_complex, H_val_true_complex)\n",
    "    \n",
    "    variables['NMSE_LS_NN'] = nmse_LS_NN.cpu().mean()\n",
    "    nmse_LS_NN_val.append(variables['NMSE_LS_NN'].item())\n",
    "    print(f\"LS+LI NMSE: {variables['NMSE_LS_NN'].item()}\")\n",
    "    \n",
    "    plotfig.figPredChan(H_val_NN_denormd[-1,0,:,:], 'LS+CNN Estimated Channel (after de-normlized)',\n",
    "                            nmse_LS_NN[-1], index_save, figure_save_path, '_LS_CNN_estimatedChan.png')\n",
    "    \n",
    "\n",
    "    torch.save( variables,variable_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(SNR, nmse_LS_LI_val, label='LS+LI')\n",
    "plt.plot(SNR, nmse_LS_NN_val, label='LS+NN')\n",
    "plt.plot(SNR, nmse_LI_NN_val, label='LS+LI+NN')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('NMSE')\n",
    "plt.title('Average NMSE over SNR')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
