{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: DeepMIMO data: BS16, row3500_3516, 3.4 GHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "\n",
    "# Add the Torch_code directory to the Python path\n",
    "# sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "sys.path.append(os.path.abspath('../helper'))\n",
    "import config\n",
    "import utils\n",
    "import loader\n",
    "import plotfig\n",
    "# import skfuzzy as fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_PATH = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "# print(FILE_PATH)\n",
    "# print(config.temp_path)\n",
    "# print(config.FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 32 #64  # Batch size\n",
    "NUM_EPOCHS = 20 # 20\n",
    "\n",
    "# rows from DeepMIMO dataset settings\n",
    "# change rows according to the .mat dataset file \n",
    "rows = [['3500', '3516']] \n",
    "fc = '3p4' #Hz can change to '60'\n",
    "rowss = \"3500_3516\"\n",
    "learning_rate = 0.00001 # 1e-5\n",
    "SNR = np.arange(0, 31, 5) # 0:5:30 dB\n",
    "outer_file_path = os.path.abspath(os.path.join(config.FILE_PATH, \n",
    "                                                '..', 'DeepMIMOv2', 'DeepMIMO_Data', 'Static_BS16', 'freq_symb_1ant_612sub_ver4'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../model/static/CNN/BS16/3500_3516/ver6_/readme.txt' and ' ../figure/static/CNN/BS16/3500_3516/ver6_/readme.txt ' created and content written.\n"
     ]
    }
   ],
   "source": [
    "# create readme.txt file\n",
    "content = \"\"\"Generated by file 'train/static_CNN_lr1e-5_v6_(...).ipynb'.\n",
    "Correspond with BS16, 3.4 GHz fc, rows 3500_3516,\n",
    "DeepMIMOv2/DeepMIMO_Dta_Static_BS16/freq_sym_1ant_612sub_ver4,\n",
    "Using min-max [-1 1] scaler for each sample\n",
    "Using Tanh as activation function of CNN\"\"\"\n",
    "\n",
    "norm_approach = 'minmax' # can be set to 'std'\n",
    "\n",
    "# Paths to save\n",
    "idx_save_path = loader.find_incremental_filename('../model/static/CNN/BS16/'+ rowss,'ver', '_', '')\n",
    "model_path = '../model/static/CNN/BS16/' + rowss + '/ver' + str(idx_save_path) + '_/readme.txt'\n",
    "figure_path = '../figure/static/CNN/BS16/' + rowss + '/ver' + str(idx_save_path) + '_/readme.txt'\n",
    "\n",
    "if not os.path.exists(os.path.dirname(model_path)):\n",
    "    os.makedirs(os.path.dirname(model_path))\n",
    "if not os.path.exists(os.path.dirname(figure_path)):\n",
    "    os.makedirs(os.path.dirname(figure_path))\n",
    "\n",
    "# Open the file in write mode ('w'). If the file does not exist, it will be created.\n",
    "with open(model_path, 'w') as file:\n",
    "    # Write the content to the file\n",
    "    file.write(content)\n",
    "\n",
    "with open(figure_path, 'w') as file:\n",
    "    # Write the content to the file\n",
    "    file.write(content)\n",
    "\n",
    "print(f\"File '{model_path}' and ' {figure_path} ' created and content written.\")\n",
    "\n",
    "save_folder_model = os.path.join(config.FILE_PATH, 'model/static/CNN', 'BS16', rowss, 'ver' + str(idx_save_path) + '_')\n",
    "save_folder_fig = os.path.join(config.FILE_PATH, 'figure', 'static', 'CNN', 'BS16' ,  rowss, 'ver' + str(idx_save_path) +'_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmse_LS_LI_val   = []\n",
    "nmse_LS_NN_val   = []\n",
    "nmse_LI_NN_val   = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snr = 0\n",
    "# [trainLabels, valLabels], [H_equal_train, H_linear_train, H_practical_train], [H_equal_val, H_linear_val, H_practical_val] = loader.load_data(outer_file_path, rows, fc, device, snr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SNR: 0/30\n",
      " Training for LS+LI\n",
      "SNR: 0/30, LS+LI, Epoch 1/20, Loss: 0.0968410637619537 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.04255653274330226\n",
      "SNR: 0/30, LS+LI, Epoch 2/20, Loss: 0.036354943828354046 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.026868176646530628\n",
      "SNR: 0/30, LS+LI, Epoch 3/20, Loss: 0.02718578178305612 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.024583540475842627\n",
      "SNR: 0/30, LS+LI, Epoch 4/20, Loss: 0.026540200805378167 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02425756881182844\n",
      "SNR: 0/30, LS+LI, Epoch 5/20, Loss: 0.026391579525891776 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02412009641358798\n",
      "SNR: 0/30, LS+LI, Epoch 6/20, Loss: 0.026332372869906383 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.024045768617229027\n",
      "SNR: 0/30, LS+LI, Epoch 7/20, Loss: 0.02624219445360088 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02412429760972207\n",
      "SNR: 0/30, LS+LI, Epoch 8/20, Loss: 0.02619334274556401 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.024001789787276226\n",
      "SNR: 0/30, LS+LI, Epoch 9/20, Loss: 0.026156334807489846 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.023987855719910425\n",
      "SNR: 0/30, LS+LI, Epoch 10/20, Loss: 0.026114814422028357 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.023933783935552292\n",
      "SNR: 0/30, LS+LI, Epoch 11/20, Loss: 0.026071819070683316 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02404350182041526\n",
      "SNR: 0/30, LS+LI, Epoch 12/20, Loss: 0.026007756460891214 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02388992439955473\n",
      "SNR: 0/30, LS+LI, Epoch 13/20, Loss: 0.02599741317071887 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02375461495566097\n",
      "SNR: 0/30, LS+LI, Epoch 14/20, Loss: 0.025952678493245743 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02366646904159676\n",
      "SNR: 0/30, LS+LI, Epoch 15/20, Loss: 0.025921885075784006 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.023669491860676895\n",
      "SNR: 0/30, LS+LI, Epoch 16/20, Loss: 0.025869229976304395 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.023777258074419064\n",
      "SNR: 0/30, LS+LI, Epoch 17/20, Loss: 0.025823742421993683 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.023533390030603518\n",
      "SNR: 0/30, LS+LI, Epoch 18/20, Loss: 0.02575571314691631 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.023547273340888998\n",
      "SNR: 0/30, LS+LI, Epoch 19/20, Loss: 0.025726276839715102 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02347007156773047\n",
      "SNR: 0/30, LS+LI, Epoch 20/20, Loss: 0.02569692102694061 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.023434466978704386\n",
      "LI+NN NMSE: 0.07012461125850677\n",
      "LS+LI NMSE: 0.08225691318511963\n",
      " Training for LS\n",
      "SNR: 0/30, LS, Epoch 1/20, Loss: 0.30424920885368834 \n",
      "SNR: 0/30, LS, Val Loss: 0.2834411629221656\n",
      "SNR: 0/30, LS, Epoch 2/20, Loss: 0.25521382438235507 \n",
      "SNR: 0/30, LS, Val Loss: 0.19793463904749264\n",
      "SNR: 0/30, LS, Epoch 3/20, Loss: 0.09561395805415719 \n",
      "SNR: 0/30, LS, Val Loss: 0.0329590393230319\n",
      "SNR: 0/30, LS, Epoch 4/20, Loss: 0.022446077555244744 \n",
      "SNR: 0/30, LS, Val Loss: 0.013874724422666159\n",
      "SNR: 0/30, LS, Epoch 5/20, Loss: 0.01073683900921064 \n",
      "SNR: 0/30, LS, Val Loss: 0.008280538340014491\n",
      "SNR: 0/30, LS, Epoch 6/20, Loss: 0.007045342002150624 \n",
      "SNR: 0/30, LS, Val Loss: 0.006475900871340524\n",
      "SNR: 0/30, LS, Epoch 7/20, Loss: 0.0060960391704209665 \n",
      "SNR: 0/30, LS, Val Loss: 0.005576020868664438\n",
      "SNR: 0/30, LS, Epoch 8/20, Loss: 0.005789432087155102 \n",
      "SNR: 0/30, LS, Val Loss: 0.0057236050594259395\n",
      "SNR: 0/30, LS, Epoch 9/20, Loss: 0.005507323739313802 \n",
      "SNR: 0/30, LS, Val Loss: 0.0053932183760810985\n",
      "SNR: 0/30, LS, Epoch 10/20, Loss: 0.0055231156347449436 \n",
      "SNR: 0/30, LS, Val Loss: 0.005408018086613579\n",
      "SNR: 0/30, LS, Epoch 11/20, Loss: 0.005417501293989115 \n",
      "SNR: 0/30, LS, Val Loss: 0.005236502948471091\n",
      "SNR: 0/30, LS, Epoch 12/20, Loss: 0.005489807153597127 \n",
      "SNR: 0/30, LS, Val Loss: 0.005389824424954978\n",
      "SNR: 0/30, LS, Epoch 13/20, Loss: 0.005359562333175089 \n",
      "SNR: 0/30, LS, Val Loss: 0.005395249971611934\n",
      "SNR: 0/30, LS, Epoch 14/20, Loss: 0.005309537145157539 \n",
      "SNR: 0/30, LS, Val Loss: 0.005138628332960335\n",
      "SNR: 0/30, LS, Epoch 15/20, Loss: 0.005308499738444076 \n",
      "SNR: 0/30, LS, Val Loss: 0.005196834766221317\n",
      "SNR: 0/30, LS, Epoch 16/20, Loss: 0.00528780919456378 \n",
      "SNR: 0/30, LS, Val Loss: 0.005071704635735263\n",
      "SNR: 0/30, LS, Epoch 17/20, Loss: 0.005302311549353045 \n",
      "SNR: 0/30, LS, Val Loss: 0.005427936515347524\n",
      "SNR: 0/30, LS, Epoch 18/20, Loss: 0.005186716698348349 \n",
      "SNR: 0/30, LS, Val Loss: 0.0051525902976705265\n",
      "SNR: 0/30, LS, Epoch 19/20, Loss: 0.005188205560972524 \n",
      "SNR: 0/30, LS, Val Loss: 0.005033633735200221\n",
      "SNR: 0/30, LS, Epoch 20/20, Loss: 0.005296836517816193 \n",
      "SNR: 0/30, LS, Val Loss: 0.004954454541968351\n",
      "LS+LI NMSE: 0.01515190675854683\n",
      " SNR: 5/30\n",
      " Training for LS+LI\n",
      "SNR: 5/30, LS+LI, Epoch 1/20, Loss: 0.08129213134173391 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.028220886195247822\n",
      "SNR: 5/30, LS+LI, Epoch 2/20, Loss: 0.017818848055592456 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.01303527948700569\n",
      "SNR: 5/30, LS+LI, Epoch 3/20, Loss: 0.01144963729758422 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.011974834837019444\n",
      "SNR: 5/30, LS+LI, Epoch 4/20, Loss: 0.010873028339796462 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.01162482209672982\n",
      "SNR: 5/30, LS+LI, Epoch 5/20, Loss: 0.010660588646545833 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.011382031381468882\n",
      "SNR: 5/30, LS+LI, Epoch 6/20, Loss: 0.010504548389161395 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.011373486412181095\n",
      "SNR: 5/30, LS+LI, Epoch 7/20, Loss: 0.010407371606222938 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.011204492364248092\n",
      "SNR: 5/30, LS+LI, Epoch 8/20, Loss: 0.010343425980292607 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.011103881819343025\n",
      "SNR: 5/30, LS+LI, Epoch 9/20, Loss: 0.010283062551278881 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.01123917746272954\n",
      "SNR: 5/30, LS+LI, Epoch 10/20, Loss: 0.010216897924777207 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.011354589474980126\n",
      "SNR: 5/30, LS+LI, Epoch 11/20, Loss: 0.01015243971135554 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010975992285900495\n",
      "SNR: 5/30, LS+LI, Epoch 12/20, Loss: 0.010112014526046466 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010867183562368155\n",
      "SNR: 5/30, LS+LI, Epoch 13/20, Loss: 0.010028313999714027 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010926051848483357\n",
      "SNR: 5/30, LS+LI, Epoch 14/20, Loss: 0.00998539650585329 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010881397724998269\n",
      "SNR: 5/30, LS+LI, Epoch 15/20, Loss: 0.009897944842313612 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010777135654775933\n",
      "SNR: 5/30, LS+LI, Epoch 16/20, Loss: 0.009860471229415474 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010705116141418164\n",
      "SNR: 5/30, LS+LI, Epoch 17/20, Loss: 0.00979843434368715 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010743400314822793\n",
      "SNR: 5/30, LS+LI, Epoch 18/20, Loss: 0.009726207603180652 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010519597975706512\n",
      "SNR: 5/30, LS+LI, Epoch 19/20, Loss: 0.009652354175138265 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010483862705190073\n",
      "SNR: 5/30, LS+LI, Epoch 20/20, Loss: 0.009591610568878783 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010494394227862358\n",
      "LI+NN NMSE: 0.032965291291475296\n",
      "LS+LI NMSE: 0.025900740176439285\n",
      " Training for LS\n",
      "SNR: 5/30, LS, Epoch 1/20, Loss: 0.29290859357908716 \n",
      "SNR: 5/30, LS, Val Loss: 0.26425149224021216\n",
      "SNR: 5/30, LS, Epoch 2/20, Loss: 0.23081928517582806 \n",
      "SNR: 5/30, LS, Val Loss: 0.15783052281899887\n",
      "SNR: 5/30, LS, Epoch 3/20, Loss: 0.0632578628570881 \n",
      "SNR: 5/30, LS, Val Loss: 0.018779637419026007\n",
      "SNR: 5/30, LS, Epoch 4/20, Loss: 0.013086884610116655 \n",
      "SNR: 5/30, LS, Val Loss: 0.008312326411462644\n",
      "SNR: 5/30, LS, Epoch 5/20, Loss: 0.006462334855567924 \n",
      "SNR: 5/30, LS, Val Loss: 0.004953220232643865\n",
      "SNR: 5/30, LS, Epoch 6/20, Loss: 0.0041935870296668344 \n",
      "SNR: 5/30, LS, Val Loss: 0.003579033104787496\n",
      "SNR: 5/30, LS, Epoch 7/20, Loss: 0.003434841873037607 \n",
      "SNR: 5/30, LS, Val Loss: 0.0032338985686444425\n",
      "SNR: 5/30, LS, Epoch 8/20, Loss: 0.003101428958511543 \n",
      "SNR: 5/30, LS, Val Loss: 0.0029983356468040834\n",
      "SNR: 5/30, LS, Epoch 9/20, Loss: 0.0029690505064980583 \n",
      "SNR: 5/30, LS, Val Loss: 0.0030022604094648905\n",
      "SNR: 5/30, LS, Epoch 10/20, Loss: 0.0028217559745318667 \n",
      "SNR: 5/30, LS, Val Loss: 0.0029075857865708795\n",
      "SNR: 5/30, LS, Epoch 11/20, Loss: 0.00283987297362453 \n",
      "SNR: 5/30, LS, Val Loss: 0.0029780232792042875\n",
      "SNR: 5/30, LS, Epoch 12/20, Loss: 0.002762692568953647 \n",
      "SNR: 5/30, LS, Val Loss: 0.0028718004786324773\n",
      "SNR: 5/30, LS, Epoch 13/20, Loss: 0.0026886093194683105 \n",
      "SNR: 5/30, LS, Val Loss: 0.0026227130808613515\n",
      "SNR: 5/30, LS, Epoch 14/20, Loss: 0.002656644891113649 \n",
      "SNR: 5/30, LS, Val Loss: 0.0028255485442721033\n",
      "SNR: 5/30, LS, Epoch 15/20, Loss: 0.0025695907940200074 \n",
      "SNR: 5/30, LS, Val Loss: 0.002560229188847271\n",
      "SNR: 5/30, LS, Epoch 16/20, Loss: 0.002635484278407814 \n",
      "SNR: 5/30, LS, Val Loss: 0.002691081862642684\n",
      "SNR: 5/30, LS, Epoch 17/20, Loss: 0.0025984658764913503 \n",
      "SNR: 5/30, LS, Val Loss: 0.0028032865057784047\n",
      "SNR: 5/30, LS, Epoch 18/20, Loss: 0.002615170167531645 \n",
      "SNR: 5/30, LS, Val Loss: 0.0025592701805924826\n",
      "SNR: 5/30, LS, Epoch 19/20, Loss: 0.002619463993712913 \n",
      "SNR: 5/30, LS, Val Loss: 0.0024921745696867056\n",
      "SNR: 5/30, LS, Epoch 20/20, Loss: 0.0025179561330438698 \n",
      "SNR: 5/30, LS, Val Loss: 0.0032560781215910206\n",
      "LS+LI NMSE: 0.009706367738544941\n",
      " SNR: 10/30\n",
      " Training for LS+LI\n",
      "SNR: 10/30, LS+LI, Epoch 1/20, Loss: 0.063499121082037 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.017063818871974945\n",
      "SNR: 10/30, LS+LI, Epoch 2/20, Loss: 0.01008621830666481 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.007651683874428272\n",
      "SNR: 10/30, LS+LI, Epoch 3/20, Loss: 0.006840534938694259 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.00650977770882574\n",
      "SNR: 10/30, LS+LI, Epoch 4/20, Loss: 0.005944747775576489 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.0057736599013548\n",
      "SNR: 10/30, LS+LI, Epoch 5/20, Loss: 0.005263794985123325 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.005188860761171038\n",
      "SNR: 10/30, LS+LI, Epoch 6/20, Loss: 0.004832300000152615 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004876943813128905\n",
      "SNR: 10/30, LS+LI, Epoch 7/20, Loss: 0.0046114039019854785 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004761271771382202\n",
      "SNR: 10/30, LS+LI, Epoch 8/20, Loss: 0.0045262554736244815 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004684322608888827\n",
      "SNR: 10/30, LS+LI, Epoch 9/20, Loss: 0.0044496179636849395 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.00476565032096749\n",
      "SNR: 10/30, LS+LI, Epoch 10/20, Loss: 0.004376197343443109 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004509772692637687\n",
      "SNR: 10/30, LS+LI, Epoch 11/20, Loss: 0.004322914422901217 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004500297120433639\n",
      "SNR: 10/30, LS+LI, Epoch 12/20, Loss: 0.0042439440473762535 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.0044127957874231715\n",
      "SNR: 10/30, LS+LI, Epoch 13/20, Loss: 0.004196679218249872 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004360646633854644\n",
      "SNR: 10/30, LS+LI, Epoch 14/20, Loss: 0.004105522857740695 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004249941684644331\n",
      "SNR: 10/30, LS+LI, Epoch 15/20, Loss: 0.004031129460240346 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004192810918373818\n",
      "SNR: 10/30, LS+LI, Epoch 16/20, Loss: 0.003961972036767144 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004102059692906385\n",
      "SNR: 10/30, LS+LI, Epoch 17/20, Loss: 0.003938727608075105 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004128878586925566\n",
      "SNR: 10/30, LS+LI, Epoch 18/20, Loss: 0.003876972132698135 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.003981207179921595\n",
      "SNR: 10/30, LS+LI, Epoch 19/20, Loss: 0.003784098541959687 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.003954201396978037\n",
      "SNR: 10/30, LS+LI, Epoch 20/20, Loss: 0.003762870920053142 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.0039012836588715964\n",
      "LI+NN NMSE: 0.01209929957985878\n",
      "LS+LI NMSE: 0.0081618158146739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thien/Hprediction/one_shot_Hest_cleanver/Torch_code/helper/plotfig.py:30: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(10, 5))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training for LS\n",
      "SNR: 10/30, LS, Epoch 1/20, Loss: 0.29403030534469804 \n",
      "SNR: 10/30, LS, Val Loss: 0.2604386745528741\n",
      "SNR: 10/30, LS, Epoch 2/20, Loss: 0.2392239369625269 \n",
      "SNR: 10/30, LS, Val Loss: 0.19424580647186798\n",
      "SNR: 10/30, LS, Epoch 3/20, Loss: 0.10952452144607208 \n",
      "SNR: 10/30, LS, Val Loss: 0.021243397866121748\n",
      "SNR: 10/30, LS, Epoch 4/20, Loss: 0.011123472765131398 \n",
      "SNR: 10/30, LS, Val Loss: 0.00561277316459878\n",
      "SNR: 10/30, LS, Epoch 5/20, Loss: 0.004256509919921586 \n",
      "SNR: 10/30, LS, Val Loss: 0.003060716479508714\n",
      "SNR: 10/30, LS, Epoch 6/20, Loss: 0.0026049238228953854 \n",
      "SNR: 10/30, LS, Val Loss: 0.0023712244583293796\n",
      "SNR: 10/30, LS, Epoch 7/20, Loss: 0.0021472952889040288 \n",
      "SNR: 10/30, LS, Val Loss: 0.00198810855561698\n",
      "SNR: 10/30, LS, Epoch 8/20, Loss: 0.002012390401051921 \n",
      "SNR: 10/30, LS, Val Loss: 0.0018887881940992718\n",
      "SNR: 10/30, LS, Epoch 9/20, Loss: 0.0019418338806433386 \n",
      "SNR: 10/30, LS, Val Loss: 0.0019278831631791863\n",
      "SNR: 10/30, LS, Epoch 10/20, Loss: 0.0018856195126508558 \n",
      "SNR: 10/30, LS, Val Loss: 0.0019524399646219206\n",
      "SNR: 10/30, LS, Epoch 11/20, Loss: 0.0018354708431721773 \n",
      "SNR: 10/30, LS, Val Loss: 0.0017561621180820193\n",
      "SNR: 10/30, LS, Epoch 12/20, Loss: 0.0017730955617924675 \n",
      "SNR: 10/30, LS, Val Loss: 0.0016784929532811723\n",
      "SNR: 10/30, LS, Epoch 13/20, Loss: 0.00176124598933817 \n",
      "SNR: 10/30, LS, Val Loss: 0.0017093843198381364\n",
      "SNR: 10/30, LS, Epoch 14/20, Loss: 0.0017299789679507444 \n",
      "SNR: 10/30, LS, Val Loss: 0.0017150513806634328\n",
      "SNR: 10/30, LS, Epoch 15/20, Loss: 0.001700931942756396 \n",
      "SNR: 10/30, LS, Val Loss: 0.001598896363496103\n",
      "SNR: 10/30, LS, Epoch 16/20, Loss: 0.0016609285897196291 \n",
      "SNR: 10/30, LS, Val Loss: 0.0015806200734170322\n",
      "SNR: 10/30, LS, Epoch 17/20, Loss: 0.0016423346052422774 \n",
      "SNR: 10/30, LS, Val Loss: 0.001862030518664555\n",
      "SNR: 10/30, LS, Epoch 18/20, Loss: 0.001604079561264709 \n",
      "SNR: 10/30, LS, Val Loss: 0.0015483890469609337\n",
      "SNR: 10/30, LS, Epoch 19/20, Loss: 0.0016118576702780934 \n",
      "SNR: 10/30, LS, Val Loss: 0.001636646534527906\n",
      "SNR: 10/30, LS, Epoch 20/20, Loss: 0.0015236757496685917 \n",
      "SNR: 10/30, LS, Val Loss: 0.0016530129095454786\n",
      "LS+LI NMSE: 0.005109016317874193\n",
      " SNR: 15/30\n",
      " Training for LS+LI\n",
      "SNR: 15/30, LS+LI, Epoch 1/20, Loss: 0.06291938477823901 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.015988590619103474\n",
      "SNR: 15/30, LS+LI, Epoch 2/20, Loss: 0.007966999857968022 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.004914446077732878\n",
      "SNR: 15/30, LS+LI, Epoch 3/20, Loss: 0.004457747173863788 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.003955394939773462\n",
      "SNR: 15/30, LS+LI, Epoch 4/20, Loss: 0.0037215868306216286 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0033519248562780294\n",
      "SNR: 15/30, LS+LI, Epoch 5/20, Loss: 0.0031896280354906828 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.002846058976667171\n",
      "SNR: 15/30, LS+LI, Epoch 6/20, Loss: 0.002742539859799192 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.002438737936741249\n",
      "SNR: 15/30, LS+LI, Epoch 7/20, Loss: 0.0023832275778553342 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.002144876089666716\n",
      "SNR: 15/30, LS+LI, Epoch 8/20, Loss: 0.002182378249546123 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0019938444496471097\n",
      "SNR: 15/30, LS+LI, Epoch 9/20, Loss: 0.00207536373772052 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0019067979830486531\n",
      "SNR: 15/30, LS+LI, Epoch 10/20, Loss: 0.0020080239100511684 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0018905553416433659\n",
      "SNR: 15/30, LS+LI, Epoch 11/20, Loss: 0.0019328627128933752 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0018011578940786421\n",
      "SNR: 15/30, LS+LI, Epoch 12/20, Loss: 0.0018795717109622823 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0017186992494812744\n",
      "SNR: 15/30, LS+LI, Epoch 13/20, Loss: 0.0018020109463006604 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.001651718027212403\n",
      "SNR: 15/30, LS+LI, Epoch 14/20, Loss: 0.00173932311144051 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.001642134109384973\n",
      "SNR: 15/30, LS+LI, Epoch 15/20, Loss: 0.001685197556494237 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0015626336892389438\n",
      "SNR: 15/30, LS+LI, Epoch 16/20, Loss: 0.0016344292028428076 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0014917384833097458\n",
      "SNR: 15/30, LS+LI, Epoch 17/20, Loss: 0.001584714035257199 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0014561368205415254\n",
      "SNR: 15/30, LS+LI, Epoch 18/20, Loss: 0.0015477603629670041 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.001513334742577916\n",
      "SNR: 15/30, LS+LI, Epoch 19/20, Loss: 0.0015213068325592334 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0013732686003839428\n",
      "SNR: 15/30, LS+LI, Epoch 20/20, Loss: 0.001516700087745977 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.001381111447699368\n",
      "LI+NN NMSE: 0.004211470019072294\n",
      "LS+LI NMSE: 0.002595741767436266\n",
      " Training for LS\n",
      "SNR: 15/30, LS, Epoch 1/20, Loss: 0.283237891748201 \n",
      "SNR: 15/30, LS, Val Loss: 0.2517373270609162\n",
      "SNR: 15/30, LS, Epoch 2/20, Loss: 0.22572490070448364 \n",
      "SNR: 15/30, LS, Val Loss: 0.17719217999414963\n",
      "SNR: 15/30, LS, Epoch 3/20, Loss: 0.08725921588213464 \n",
      "SNR: 15/30, LS, Val Loss: 0.018381399864500218\n",
      "SNR: 15/30, LS, Epoch 4/20, Loss: 0.012342048132099038 \n",
      "SNR: 15/30, LS, Val Loss: 0.007581055608832024\n",
      "SNR: 15/30, LS, Epoch 5/20, Loss: 0.005668875651683138 \n",
      "SNR: 15/30, LS, Val Loss: 0.003803907523185692\n",
      "SNR: 15/30, LS, Epoch 6/20, Loss: 0.0030478566254185904 \n",
      "SNR: 15/30, LS, Val Loss: 0.0023284362713721666\n",
      "SNR: 15/30, LS, Epoch 7/20, Loss: 0.002032083568845464 \n",
      "SNR: 15/30, LS, Val Loss: 0.0018584847037511115\n",
      "SNR: 15/30, LS, Epoch 8/20, Loss: 0.001679740051554819 \n",
      "SNR: 15/30, LS, Val Loss: 0.0015274596569890325\n",
      "SNR: 15/30, LS, Epoch 9/20, Loss: 0.001536815771075009 \n",
      "SNR: 15/30, LS, Val Loss: 0.0014428426458669658\n",
      "SNR: 15/30, LS, Epoch 10/20, Loss: 0.0014437760895139775 \n",
      "SNR: 15/30, LS, Val Loss: 0.0013598797906359489\n",
      "SNR: 15/30, LS, Epoch 11/20, Loss: 0.0014022717016835718 \n",
      "SNR: 15/30, LS, Val Loss: 0.001405536731578071\n",
      "SNR: 15/30, LS, Epoch 12/20, Loss: 0.0013829356351824 \n",
      "SNR: 15/30, LS, Val Loss: 0.0013233528879936785\n",
      "SNR: 15/30, LS, Epoch 13/20, Loss: 0.0012887675635960647 \n",
      "SNR: 15/30, LS, Val Loss: 0.0012164342756891115\n",
      "SNR: 15/30, LS, Epoch 14/20, Loss: 0.0012273968337509825 \n",
      "SNR: 15/30, LS, Val Loss: 0.0013011246760883791\n",
      "SNR: 15/30, LS, Epoch 15/20, Loss: 0.0012157226108599367 \n",
      "SNR: 15/30, LS, Val Loss: 0.001139638781420548\n",
      "SNR: 15/30, LS, Epoch 16/20, Loss: 0.0011306656429782337 \n",
      "SNR: 15/30, LS, Val Loss: 0.001248682997274128\n",
      "SNR: 15/30, LS, Epoch 17/20, Loss: 0.0011266405023444921 \n",
      "SNR: 15/30, LS, Val Loss: 0.0011579472285864706\n",
      "SNR: 15/30, LS, Epoch 18/20, Loss: 0.001092899111194298 \n",
      "SNR: 15/30, LS, Val Loss: 0.0010510166810656135\n",
      "SNR: 15/30, LS, Epoch 19/20, Loss: 0.0010550533039603643 \n",
      "SNR: 15/30, LS, Val Loss: 0.001207601972220635\n",
      "SNR: 15/30, LS, Epoch 20/20, Loss: 0.0010175248296649822 \n",
      "SNR: 15/30, LS, Val Loss: 0.0009518261464439671\n",
      "LS+LI NMSE: 0.0029228373896330595\n",
      " SNR: 20/30\n",
      " Training for LS+LI\n",
      "SNR: 20/30, LS+LI, Epoch 1/20, Loss: 0.057242710868892974 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0185229101810943\n",
      "SNR: 20/30, LS+LI, Epoch 2/20, Loss: 0.009304799659307613 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.004974025809629397\n",
      "SNR: 20/30, LS+LI, Epoch 3/20, Loss: 0.003882394387484203 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0029221142176538706\n",
      "SNR: 20/30, LS+LI, Epoch 4/20, Loss: 0.0022689590148010485 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.001770322724372487\n",
      "SNR: 20/30, LS+LI, Epoch 5/20, Loss: 0.0015843899888094775 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0014584954743358221\n",
      "SNR: 20/30, LS+LI, Epoch 6/20, Loss: 0.0014000746765347241 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.001348176785871725\n",
      "SNR: 20/30, LS+LI, Epoch 7/20, Loss: 0.0012815639838240607 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0012258625753350896\n",
      "SNR: 20/30, LS+LI, Epoch 8/20, Loss: 0.0011839479117296897 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0011224333429709077\n",
      "SNR: 20/30, LS+LI, Epoch 9/20, Loss: 0.0010935126182482426 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0010470281386832621\n",
      "SNR: 20/30, LS+LI, Epoch 10/20, Loss: 0.0010140538882317974 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0009808665040922774\n",
      "SNR: 20/30, LS+LI, Epoch 11/20, Loss: 0.0009487189980613631 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.000895563782912425\n",
      "SNR: 20/30, LS+LI, Epoch 12/20, Loss: 0.0008837804587444204 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0008500699239613658\n",
      "SNR: 20/30, LS+LI, Epoch 13/20, Loss: 0.0008258854403149683 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.000769717520928349\n",
      "SNR: 20/30, LS+LI, Epoch 14/20, Loss: 0.0007698624509895696 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0007303547595669938\n",
      "SNR: 20/30, LS+LI, Epoch 15/20, Loss: 0.0007205221031759974 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0006993453743317249\n",
      "SNR: 20/30, LS+LI, Epoch 16/20, Loss: 0.0007059990245981553 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0006563275555064055\n",
      "SNR: 20/30, LS+LI, Epoch 17/20, Loss: 0.0006751679552402239 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0006465867839076302\n",
      "SNR: 20/30, LS+LI, Epoch 18/20, Loss: 0.0006523827086798413 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0006203335400310938\n",
      "SNR: 20/30, LS+LI, Epoch 19/20, Loss: 0.0006355875558192777 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0006111574118470096\n",
      "SNR: 20/30, LS+LI, Epoch 20/20, Loss: 0.0006397586780516322 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0006096844603730874\n",
      "LI+NN NMSE: 0.0019208985613659024\n",
      "LS+LI NMSE: 0.0008209433290176094\n",
      " Training for LS\n",
      "SNR: 20/30, LS, Epoch 1/20, Loss: 0.2798951174630675 \n",
      "SNR: 20/30, LS, Val Loss: 0.2707660421729088\n",
      "SNR: 20/30, LS, Epoch 2/20, Loss: 0.22074636903613112 \n",
      "SNR: 20/30, LS, Val Loss: 0.18743280524557288\n",
      "SNR: 20/30, LS, Epoch 3/20, Loss: 0.08519215001399781 \n",
      "SNR: 20/30, LS, Val Loss: 0.0326298482038758\n",
      "SNR: 20/30, LS, Epoch 4/20, Loss: 0.014407457515792272 \n",
      "SNR: 20/30, LS, Val Loss: 0.010466617891903628\n",
      "SNR: 20/30, LS, Epoch 5/20, Loss: 0.005295761051414491 \n",
      "SNR: 20/30, LS, Val Loss: 0.0042205716970122676\n",
      "SNR: 20/30, LS, Epoch 6/20, Loss: 0.00260473397319465 \n",
      "SNR: 20/30, LS, Val Loss: 0.0023334271443838425\n",
      "SNR: 20/30, LS, Epoch 7/20, Loss: 0.0018130358793166314 \n",
      "SNR: 20/30, LS, Val Loss: 0.0018082599324936216\n",
      "SNR: 20/30, LS, Epoch 8/20, Loss: 0.001587565904374906 \n",
      "SNR: 20/30, LS, Val Loss: 0.0016561996361071413\n",
      "SNR: 20/30, LS, Epoch 9/20, Loss: 0.001497718209108381 \n",
      "SNR: 20/30, LS, Val Loss: 0.001571100136392157\n",
      "SNR: 20/30, LS, Epoch 10/20, Loss: 0.0014236284241210236 \n",
      "SNR: 20/30, LS, Val Loss: 0.0014762024776163426\n",
      "SNR: 20/30, LS, Epoch 11/20, Loss: 0.001358863400826014 \n",
      "SNR: 20/30, LS, Val Loss: 0.0014209887171587484\n",
      "SNR: 20/30, LS, Epoch 12/20, Loss: 0.0012969819319212463 \n",
      "SNR: 20/30, LS, Val Loss: 0.0013774246644144032\n",
      "SNR: 20/30, LS, Epoch 13/20, Loss: 0.0012572112795761461 \n",
      "SNR: 20/30, LS, Val Loss: 0.0013149822268381038\n",
      "SNR: 20/30, LS, Epoch 14/20, Loss: 0.0012055740896125087 \n",
      "SNR: 20/30, LS, Val Loss: 0.0012399031150959092\n",
      "SNR: 20/30, LS, Epoch 15/20, Loss: 0.0011526636774570485 \n",
      "SNR: 20/30, LS, Val Loss: 0.0011900676710700448\n",
      "SNR: 20/30, LS, Epoch 16/20, Loss: 0.0011166043662152057 \n",
      "SNR: 20/30, LS, Val Loss: 0.0011422963527200575\n",
      "SNR: 20/30, LS, Epoch 17/20, Loss: 0.0010847487709577043 \n",
      "SNR: 20/30, LS, Val Loss: 0.0011340664985420351\n",
      "SNR: 20/30, LS, Epoch 18/20, Loss: 0.0010440635669510812 \n",
      "SNR: 20/30, LS, Val Loss: 0.0011640159143346616\n",
      "SNR: 20/30, LS, Epoch 19/20, Loss: 0.0010063302977996076 \n",
      "SNR: 20/30, LS, Val Loss: 0.001065025584962727\n",
      "SNR: 20/30, LS, Epoch 20/20, Loss: 0.0009646753013691713 \n",
      "SNR: 20/30, LS, Val Loss: 0.0010309452138079162\n",
      "LS+LI NMSE: 0.0031829713843762875\n",
      " SNR: 25/30\n",
      " Training for LS+LI\n",
      "SNR: 25/30, LS+LI, Epoch 1/20, Loss: 0.06614671141818859 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.019631686705079945\n",
      "SNR: 25/30, LS+LI, Epoch 2/20, Loss: 0.010655260642145782 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.00532185138118538\n",
      "SNR: 25/30, LS+LI, Epoch 3/20, Loss: 0.004307290216972835 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.003585702400993217\n",
      "SNR: 25/30, LS+LI, Epoch 4/20, Loss: 0.00312715214758375 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.002669817461124198\n",
      "SNR: 25/30, LS+LI, Epoch 5/20, Loss: 0.0022888028869665292 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0018970221175219524\n",
      "SNR: 25/30, LS+LI, Epoch 6/20, Loss: 0.0015792096707818294 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0012995113211218268\n",
      "SNR: 25/30, LS+LI, Epoch 7/20, Loss: 0.0011642495265669125 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0010484880952968854\n",
      "SNR: 25/30, LS+LI, Epoch 8/20, Loss: 0.0009815310890147419 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0009011105496690354\n",
      "SNR: 25/30, LS+LI, Epoch 9/20, Loss: 0.0008604206841271154 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0008048919209448451\n",
      "SNR: 25/30, LS+LI, Epoch 10/20, Loss: 0.0007659149715584855 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0007136595430148935\n",
      "SNR: 25/30, LS+LI, Epoch 11/20, Loss: 0.0006921410182934947 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0006497590643861754\n",
      "SNR: 25/30, LS+LI, Epoch 12/20, Loss: 0.0006307859811661115 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.000580720317719335\n",
      "SNR: 25/30, LS+LI, Epoch 13/20, Loss: 0.0005709399273930399 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0005330968269845471\n",
      "SNR: 25/30, LS+LI, Epoch 14/20, Loss: 0.0005146361810295995 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0004774147303330458\n",
      "SNR: 25/30, LS+LI, Epoch 15/20, Loss: 0.0004687978769990962 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.00043070665692952883\n",
      "SNR: 25/30, LS+LI, Epoch 16/20, Loss: 0.00042571958701321204 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0004109149568566036\n",
      "SNR: 25/30, LS+LI, Epoch 17/20, Loss: 0.00039166628199001385 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0003876108332240785\n",
      "SNR: 25/30, LS+LI, Epoch 18/20, Loss: 0.00036146134381090443 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0003294835438613187\n",
      "SNR: 25/30, LS+LI, Epoch 19/20, Loss: 0.00034697979614531165 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.00032262297728183597\n",
      "SNR: 25/30, LS+LI, Epoch 20/20, Loss: 0.00032368172164857497 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.00032492362052752554\n",
      "LI+NN NMSE: 0.0009483376052230597\n",
      "LS+LI NMSE: 0.0002607222704682499\n",
      " Training for LS\n",
      "SNR: 25/30, LS, Epoch 1/20, Loss: 0.2853995382265989 \n",
      "SNR: 25/30, LS, Val Loss: 0.2526229105212472\n",
      "SNR: 25/30, LS, Epoch 2/20, Loss: 0.23035254112856332 \n",
      "SNR: 25/30, LS, Val Loss: 0.1877854825420813\n",
      "SNR: 25/30, LS, Epoch 3/20, Loss: 0.09623988365299653 \n",
      "SNR: 25/30, LS, Val Loss: 0.018148980442095886\n",
      "SNR: 25/30, LS, Epoch 4/20, Loss: 0.012212391552883525 \n",
      "SNR: 25/30, LS, Val Loss: 0.006589959947053681\n",
      "SNR: 25/30, LS, Epoch 5/20, Loss: 0.005068668064141516 \n",
      "SNR: 25/30, LS, Val Loss: 0.003362998197024519\n",
      "SNR: 25/30, LS, Epoch 6/20, Loss: 0.002817841115418475 \n",
      "SNR: 25/30, LS, Val Loss: 0.0022672240050848236\n",
      "SNR: 25/30, LS, Epoch 7/20, Loss: 0.0020633792629205557 \n",
      "SNR: 25/30, LS, Val Loss: 0.001850688025694002\n",
      "SNR: 25/30, LS, Epoch 8/20, Loss: 0.0017783467693059424 \n",
      "SNR: 25/30, LS, Val Loss: 0.001686482155822556\n",
      "SNR: 25/30, LS, Epoch 9/20, Loss: 0.0016237210481005267 \n",
      "SNR: 25/30, LS, Val Loss: 0.0015507006292781707\n",
      "SNR: 25/30, LS, Epoch 10/20, Loss: 0.0015138853029997715 \n",
      "SNR: 25/30, LS, Val Loss: 0.0014409258226144382\n",
      "SNR: 25/30, LS, Epoch 11/20, Loss: 0.0014134656900518336 \n",
      "SNR: 25/30, LS, Val Loss: 0.0013586592315484515\n",
      "SNR: 25/30, LS, Epoch 12/20, Loss: 0.001340399130090508 \n",
      "SNR: 25/30, LS, Val Loss: 0.0012888947676401585\n",
      "SNR: 25/30, LS, Epoch 13/20, Loss: 0.0012636830086744022 \n",
      "SNR: 25/30, LS, Val Loss: 0.001224165054736659\n",
      "SNR: 25/30, LS, Epoch 14/20, Loss: 0.0011927873007195114 \n",
      "SNR: 25/30, LS, Val Loss: 0.001173797560940412\n",
      "SNR: 25/30, LS, Epoch 15/20, Loss: 0.0011403178061808047 \n",
      "SNR: 25/30, LS, Val Loss: 0.0010916139131454243\n",
      "SNR: 25/30, LS, Epoch 16/20, Loss: 0.001086324874005757 \n",
      "SNR: 25/30, LS, Val Loss: 0.0010944238959134302\n",
      "SNR: 25/30, LS, Epoch 17/20, Loss: 0.0010497665758986503 \n",
      "SNR: 25/30, LS, Val Loss: 0.0011042961215769703\n",
      "SNR: 25/30, LS, Epoch 18/20, Loss: 0.0010089931057273346 \n",
      "SNR: 25/30, LS, Val Loss: 0.0009616054329936477\n",
      "SNR: 25/30, LS, Epoch 19/20, Loss: 0.000986174501608624 \n",
      "SNR: 25/30, LS, Val Loss: 0.0008962587744463235\n",
      "SNR: 25/30, LS, Epoch 20/20, Loss: 0.0009050455809365091 \n",
      "SNR: 25/30, LS, Val Loss: 0.0008928299622229216\n",
      "LS+LI NMSE: 0.00277263717725873\n",
      " SNR: 30/30\n",
      " Training for LS+LI\n",
      "SNR: 30/30, LS+LI, Epoch 1/20, Loss: 0.06490523216509542 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.016663003628226845\n",
      "SNR: 30/30, LS+LI, Epoch 2/20, Loss: 0.00802728972603502 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.003682802293703637\n",
      "SNR: 30/30, LS+LI, Epoch 3/20, Loss: 0.0030375218741747357 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.002484555282122032\n",
      "SNR: 30/30, LS+LI, Epoch 4/20, Loss: 0.0020559352665576475 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0016294574703682554\n",
      "SNR: 30/30, LS+LI, Epoch 5/20, Loss: 0.0013495052211679691 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0011151437330144372\n",
      "SNR: 30/30, LS+LI, Epoch 6/20, Loss: 0.0010246237886116602 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.000935248451159251\n",
      "SNR: 30/30, LS+LI, Epoch 7/20, Loss: 0.0008846887920003081 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0008218518050853163\n",
      "SNR: 30/30, LS+LI, Epoch 8/20, Loss: 0.0007762761700622261 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0007212133747419681\n",
      "SNR: 30/30, LS+LI, Epoch 9/20, Loss: 0.0006851130405435512 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0006412819123149595\n",
      "SNR: 30/30, LS+LI, Epoch 10/20, Loss: 0.0006117337413110532 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0005728753418555822\n",
      "SNR: 30/30, LS+LI, Epoch 11/20, Loss: 0.0005472881520142627 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.000511426128055477\n",
      "SNR: 30/30, LS+LI, Epoch 12/20, Loss: 0.0004901413177681523 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.000460928218672052\n",
      "SNR: 30/30, LS+LI, Epoch 13/20, Loss: 0.00043646488856486325 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00041821017326914114\n",
      "SNR: 30/30, LS+LI, Epoch 14/20, Loss: 0.0003885211007463811 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0003569704488406635\n",
      "SNR: 30/30, LS+LI, Epoch 15/20, Loss: 0.0003413541765672393 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0003138178640669635\n",
      "SNR: 30/30, LS+LI, Epoch 16/20, Loss: 0.00030057173277126887 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0002783304344947365\n",
      "SNR: 30/30, LS+LI, Epoch 17/20, Loss: 0.0002695584544833468 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00025547616347797555\n",
      "SNR: 30/30, LS+LI, Epoch 18/20, Loss: 0.00024650685914228993 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00023405242179499262\n",
      "SNR: 30/30, LS+LI, Epoch 19/20, Loss: 0.00023044474945108537 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00022221232872900808\n",
      "SNR: 30/30, LS+LI, Epoch 20/20, Loss: 0.0002207149407822303 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00020623606674648315\n",
      "LI+NN NMSE: 0.0006394465453922749\n",
      "LS+LI NMSE: 8.345480455318466e-05\n",
      " Training for LS\n",
      "SNR: 30/30, LS, Epoch 1/20, Loss: 0.28408285360350166 \n",
      "SNR: 30/30, LS, Val Loss: 0.2528278041969646\n",
      "SNR: 30/30, LS, Epoch 2/20, Loss: 0.21685635982904322 \n",
      "SNR: 30/30, LS, Val Loss: 0.1648960987275297\n",
      "SNR: 30/30, LS, Epoch 3/20, Loss: 0.06851027419162524 \n",
      "SNR: 30/30, LS, Val Loss: 0.013973598527214066\n",
      "SNR: 30/30, LS, Epoch 4/20, Loss: 0.009583937598468173 \n",
      "SNR: 30/30, LS, Val Loss: 0.006861228098965843\n",
      "SNR: 30/30, LS, Epoch 5/20, Loss: 0.004841853827941989 \n",
      "SNR: 30/30, LS, Val Loss: 0.0035194278832271016\n",
      "SNR: 30/30, LS, Epoch 6/20, Loss: 0.002651176769367614 \n",
      "SNR: 30/30, LS, Val Loss: 0.0021038127253467046\n",
      "SNR: 30/30, LS, Epoch 7/20, Loss: 0.0017330575102214637 \n",
      "SNR: 30/30, LS, Val Loss: 0.0015064040198922157\n",
      "SNR: 30/30, LS, Epoch 8/20, Loss: 0.0013861475383110215 \n",
      "SNR: 30/30, LS, Val Loss: 0.0012712051896166734\n",
      "SNR: 30/30, LS, Epoch 9/20, Loss: 0.0012165441219749058 \n",
      "SNR: 30/30, LS, Val Loss: 0.001143189647171477\n",
      "SNR: 30/30, LS, Epoch 10/20, Loss: 0.0011353890606483749 \n",
      "SNR: 30/30, LS, Val Loss: 0.0010664946590126915\n",
      "SNR: 30/30, LS, Epoch 11/20, Loss: 0.0010457916282046976 \n",
      "SNR: 30/30, LS, Val Loss: 0.0010459717172621326\n",
      "SNR: 30/30, LS, Epoch 12/20, Loss: 0.0009898756820908856 \n",
      "SNR: 30/30, LS, Val Loss: 0.0009415282891661099\n",
      "SNR: 30/30, LS, Epoch 13/20, Loss: 0.0009420893471796325 \n",
      "SNR: 30/30, LS, Val Loss: 0.0008958257226781411\n",
      "SNR: 30/30, LS, Epoch 14/20, Loss: 0.0008900562557797923 \n",
      "SNR: 30/30, LS, Val Loss: 0.0008591874251247976\n",
      "SNR: 30/30, LS, Epoch 15/20, Loss: 0.0008516589851053648 \n",
      "SNR: 30/30, LS, Val Loss: 0.0008229963674569841\n",
      "SNR: 30/30, LS, Epoch 16/20, Loss: 0.0008289881597467979 \n",
      "SNR: 30/30, LS, Val Loss: 0.0007994165961545976\n",
      "SNR: 30/30, LS, Epoch 17/20, Loss: 0.0007946245635583521 \n",
      "SNR: 30/30, LS, Val Loss: 0.0007526917311638085\n",
      "SNR: 30/30, LS, Epoch 18/20, Loss: 0.0007632856875208618 \n",
      "SNR: 30/30, LS, Val Loss: 0.0007531828662401742\n",
      "SNR: 30/30, LS, Epoch 19/20, Loss: 0.0007489909235859117 \n",
      "SNR: 30/30, LS, Val Loss: 0.0007905834195712073\n",
      "SNR: 30/30, LS, Epoch 20/20, Loss: 0.0007277167998497959 \n",
      "SNR: 30/30, LS, Val Loss: 0.0006929432790705257\n",
      "LS+LI NMSE: 0.0021998423617333174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for snr in SNR:\n",
    "    print(f\" SNR: {snr}/{SNR[-1]}\")\n",
    "\n",
    "    [trainLabels, valLabels], [H_equal_train, H_linear_train, H_practical_train], [H_equal_val, H_linear_val, H_practical_val] = loader.load_data(outer_file_path, rows, fc, device, snr)\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 1. When input is H_linear (after LS+LI)\n",
    "    print(f\" Training for LS+LI\")\n",
    "    # [samples, 2, 612, 14]\n",
    "    train_loader, trainLabel_min, trainLabel_max = loader.genLoader(H_linear_train, trainLabels, BATCH_SIZE, device, 'train', True, norm_approach)\n",
    "    val_loader,     valLabel_min,   valLabel_max = loader.genLoader(H_linear_val,     valLabels, BATCH_SIZE, device, 'valid', False, norm_approach)\n",
    "        # no shuffle in validation so valLabel_min and valLable_max remain the min and max arrays\n",
    "                                                                                    # of valLabels\n",
    "        # train_loader, val_loader are already normalized by their own min, max\n",
    "        # scale to range [-1 1]\n",
    "        \n",
    "    # model\n",
    "    model = utils.CNN_Est(act = 'Tanh').to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 1.5 Training loop\n",
    "    train_loss =[]\n",
    "    val_loss = []\n",
    "    H_NN_val = torch.empty_like(valLabels) # [nVal, 2, 612, 14]\n",
    "    min_H_true = []\n",
    "    max_H_true = []\n",
    "    num_epochs = NUM_EPOCHS\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        if (epoch == num_epochs-1):\n",
    "            i = 0\n",
    "        for inputs, targets, targets_min, targets_max in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_loss.append(avg_train_loss)\n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS+LI, Epoch {epoch+1}/{num_epochs}, Loss: {avg_train_loss} \")\n",
    "        \n",
    "        # Validation \n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets, val_targetsMin, val_targetsMax in val_loader:\n",
    "                val_inputs_real = val_inputs[:,0,:,:].unsqueeze(1)\n",
    "                val_inputs_imag = val_inputs[:,1,:,:].unsqueeze(1)\n",
    "                val_targets_real = val_targets[:,0,:,:].unsqueeze(1)\n",
    "                val_targets_imag = val_targets[:,1,:,:].unsqueeze(1)\n",
    "                \n",
    "                val_outputs_real = model(val_inputs_real)\n",
    "                val_loss_real = criterion(val_outputs_real, val_targets_real)\n",
    "                running_val_loss += val_loss_real.item()\n",
    "                \n",
    "                val_outputs_imag = model(val_inputs_imag)\n",
    "                val_loss_imag = criterion(val_outputs_imag, val_targets_imag)\n",
    "                running_val_loss += val_loss_imag.item()\n",
    "                \n",
    "                if (epoch == num_epochs-1): # the results after the last training \n",
    "                    H_NN_val[i:i+val_outputs_real.size(0),0,:,:].unsqueeze(1).copy_(val_outputs_real)\n",
    "                    H_NN_val[i:i+val_outputs_imag.size(0),1,:,:].unsqueeze(1).copy_(val_outputs_imag)\n",
    "                    \n",
    "                    i = i+val_outputs_imag.size(0)       \n",
    "                    \n",
    "                \n",
    "        avg_val_loss = running_val_loss / (len(val_loader)*2)\n",
    "        val_loss.append(avg_val_loss)    \n",
    "                \n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS+LI, Val Loss: {avg_val_loss}\")\n",
    "\n",
    "\n",
    "    save_folder = os.path.join(save_folder_model, str(snr)+'dB')\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    index_save = loader.find_incremental_filename(save_folder, 'CNN_', '_variable')\n",
    "\n",
    "    model_save_path = os.path.join(save_folder,  'CNN_' +str(index_save)+'_LS_LI_CNN_model.pth')\n",
    "    variable_save_path = os.path.join(save_folder, 'CNN_' +str(index_save)+'_variable.pth')\n",
    "    params_save_path = os.path.join(save_folder, 'CNN_' +str(index_save)+'_params.mat')\n",
    "    \n",
    "    params = {   \n",
    "                'SNR': snr,\n",
    "                'epoc': NUM_EPOCHS,\n",
    "                'rows': rowss,\n",
    "                'learning_rate': learning_rate,\n",
    "                'train_track_LI': train_loss,\n",
    "                'val_track_LI': val_loss,\n",
    "    }\n",
    "    variables = {             \n",
    "                'train_track_LI': train_loss,\n",
    "                'val_track_LI': val_loss,\n",
    "                # 'train_min_LI': trainData_min.cpu(),\n",
    "                # 'train_max_LI': trainData_max.cpu(),\n",
    "                # 'train_label_min': trainLabels_min.cpu(),\n",
    "                # 'train_label_max': trainLabels_max.cpu(),\n",
    "    }\n",
    "    # variable to save \n",
    "    # Save the models' state dictionaries \n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, model_save_path)\n",
    "\n",
    "    figure_save_path = os.path.join(save_folder_fig, str(snr) + 'dB') \n",
    "    \n",
    "    os.makedirs(figure_save_path, exist_ok=True)\n",
    "\n",
    "    plotfig.figLoss(train_loss, val_loss, index_save, figure_save_path, '_LS_LI_Loss.png')\n",
    "\n",
    "\n",
    "    # True channel\n",
    "    H_val_true = valLabels.cpu()\n",
    "    # convert to complex matrices\n",
    "    H_val_true_complex = torch.complex(H_val_true[:,0,:,:], H_val_true[:,1,:,:])\n",
    "    # variables['H_val_true'] = H_val_true # (nVal, 2, 612, 14)\n",
    "\n",
    "    plotfig.figTrueChan(H_val_true[-1,0,:,:], 'True Channel', index_save, figure_save_path, '_trueChannel.png')\n",
    "\n",
    "    # Estimated Channel \n",
    "    H_val_NN = H_NN_val.cpu()    \n",
    "    plotfig.figTrueChan(H_val_NN[-1,0,:,:], 'LI+CNN Estimated Channel (before de-normlized)', \n",
    "                            index_save, figure_save_path, '_LS_LI_CNN_estimatedChan_before_denorm.png')\n",
    "\n",
    "    # De-normalized                                                               \n",
    "    H_val_NN_denormd = utils.deMinMax(H_NN_val, valLabel_min, valLabel_max)\n",
    "                        #     H_NN_val == [nVal, 2, 612, 14] \n",
    "                        # valLabel_min == [nVal,1]\n",
    "                        \n",
    "    H_val_NN_denormd = H_val_NN_denormd.cpu()\n",
    "    # variables['H_val_LI_NN'] = H_val_NN_denormd # (nVal, 2, 612, 14)\n",
    "\n",
    "    # convert to complex matrices\n",
    "    H_val_NN_denormd_complex = torch.complex(H_val_NN_denormd[:,0,:,:], H_val_NN_denormd[:,1,:,:])\n",
    "    \n",
    "    nmse_LI_NN = utils.calNMSE(H_val_NN_denormd_complex, H_val_true_complex)\n",
    "    \n",
    "    variables['NMSE_LI_NN'] = nmse_LI_NN.cpu().mean()\n",
    "    nmse_LI_NN_val.append(variables['NMSE_LI_NN'].item())\n",
    "    print(f\"LI+NN NMSE: {variables['NMSE_LI_NN'].item()}\")\n",
    "    \n",
    "    plotfig.figPredChan(H_val_NN_denormd[-1,0,:,:], 'LI+CNN Estimated Channel (after de-normlized)',\n",
    "                            nmse_LI_NN[-1], index_save, figure_save_path, '_LS_LI_CNN_estimatedChan.png')\n",
    "#####\n",
    "##### above is LS+LI+NN \n",
    "\n",
    "##### following is Linear interpolated channel (only LS+LI)\n",
    "    H_val_linInterp = H_linear_val.cpu()\n",
    "    # convert to complex matrices\n",
    "    H_val_linInterp_complex = torch.complex(H_val_linInterp[:,0,:,:], H_val_linInterp[:,1,:,:]) # [?, 612, 14]\n",
    "\n",
    "    # NMSE of Linear Interpolation\n",
    "    # Calculate the NMSE\n",
    "    nmse_LI = utils.calNMSE(H_val_linInterp_complex, H_val_true_complex)\n",
    "    \n",
    "    variables['NMSE_LI'] = nmse_LI.cpu().mean()\n",
    "    nmse_LS_LI_val.append(variables['NMSE_LI'].item())\n",
    "    print(f\"LS+LI NMSE: {variables['NMSE_LI'].item()}\")\n",
    "    \n",
    "    plotfig.figPredChan(H_val_linInterp[-1,0,:,:], 'LS + Interpolate Estimated Channel',\n",
    "                            nmse_LI[-1], index_save, figure_save_path, '_LS_LI_estimatedChan.png')\n",
    "\n",
    "\n",
    "##########################################\n",
    "    # ------------------------------------------------------\n",
    "    # When Input of the NN is just H_equalized\n",
    "    print(f\" Training for LS\")\n",
    "    # [samples, 2, 612, 14]\n",
    "    H_LS_train = H_equal_train.cpu()\n",
    "    plotfig.figTrueChan(H_LS_train[0,0,:,:], 'LS Channel', index_save, figure_save_path, '_LS_Chan.png')\n",
    "    \n",
    "    # Split into training and validation sets for H_NN training\n",
    "    train_loader, trainLabel_min, trainLabel_max = loader.genLoader(H_equal_train, trainLabels, BATCH_SIZE, device, 'train',  True, norm_approach)\n",
    "    val_loader,     valLabel_min,   vallabel_max = loader.genLoader(H_equal_val,     valLabels, BATCH_SIZE, device, 'valid', False, norm_approach)\n",
    "\n",
    "\n",
    "    model2 = utils.CNN_Est(act = 'Tanh').to(device)\n",
    "    optimizer2 = torch.optim.Adam(model2.parameters(), lr=learning_rate) \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Training loop\n",
    "    train_loss =[]\n",
    "    val_loss = []\n",
    "    H_NN_val = torch.empty_like(valLabels) # [nVal, 2, 612, 14]\n",
    "    num_epochs = NUM_EPOCHS\n",
    "    for epoch in range(num_epochs):\n",
    "        model2.train()\n",
    "        running_loss = 0.0\n",
    "        if (epoch == num_epochs-1):\n",
    "            i = 0\n",
    "        for inputs, targets, targets_min, targets_max in train_loader:\n",
    "            optimizer2.zero_grad()\n",
    "            outputs = model2(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_loss.append(avg_train_loss)\n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS, Epoch {epoch+1}/{num_epochs}, Loss: {avg_train_loss} \")\n",
    "        \n",
    "        # Validation \n",
    "        model2.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets, val_targetsMin, val_targetsMax in val_loader:\n",
    "                val_inputs_real = val_inputs[:,0,:,:].unsqueeze(1)\n",
    "                val_inputs_imag = val_inputs[:,1,:,:].unsqueeze(1)\n",
    "                val_targets_real = val_targets[:,0,:,:].unsqueeze(1)\n",
    "                val_targets_imag = val_targets[:,1,:,:].unsqueeze(1)\n",
    "                \n",
    "                val_outputs_real = model2(val_inputs_real)\n",
    "                val_loss_real = criterion(val_outputs_real, val_targets_real)\n",
    "                running_val_loss += val_loss_real.item()\n",
    "                \n",
    "                val_outputs_imag = model2(val_inputs_imag)\n",
    "                val_loss_imag = criterion(val_outputs_imag, val_targets_imag)\n",
    "                running_val_loss += val_loss_imag.item()\n",
    "                \n",
    "                if (epoch == num_epochs-1):\n",
    "                    H_NN_val[i:i+val_outputs_real.size(0),0,:,:].unsqueeze(1).copy_(val_outputs_real)\n",
    "                    H_NN_val[i:i+val_outputs_imag.size(0),1,:,:].unsqueeze(1).copy_(val_outputs_imag)\n",
    "                    i = i+val_outputs_imag.size(0)\n",
    "                \n",
    "        avg_val_loss = running_val_loss / (len(val_loader)*2)\n",
    "        val_loss.append(avg_val_loss)    \n",
    "                \n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS, Val Loss: {avg_val_loss}\")\n",
    "\n",
    "    plotfig.figLoss(train_loss, val_loss, index_save, figure_save_path, '_LS_Loss.png')\n",
    "\n",
    "    # De-normalized                                                                \n",
    "    H_val_NN_denormd = utils.deMinMax(H_NN_val, valLabel_min, valLabel_max)\n",
    "                        #     H_NN_val == [nVal, 2, 612, 14] \n",
    "                        # valLabel_min == [nVal,1]\n",
    "    H_val_NN_denormd = H_val_NN_denormd.cpu()\n",
    "\n",
    "    model_save_path = os.path.join(save_folder,  'CNN_' +str(index_save)+'_LS_CNN_model.pth')\n",
    "\n",
    "    # variables['H_val_LS_NN']= H_val_NN_denormd.cpu() # (nVal, 2, 612, 14)\n",
    "    variables['train_track_LS']= train_loss\n",
    "    variables['val_track_LS']= val_loss\n",
    "\n",
    "    # Save parameters\n",
    "    params['train_track_LS']= train_loss\n",
    "    params['val_track_LS']= val_loss\n",
    "    savemat(params_save_path, params)\n",
    "    # Save the models' state dictionaries \n",
    "    torch.save({'model_state_dict': model2.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "                }, model_save_path)\n",
    "\n",
    "\n",
    "    # NMSE of LS + NN\n",
    "    H_val_LS_NN_complex = torch.complex(H_val_NN_denormd[:,0,:,:], H_val_NN_denormd[:,1,:,:])\n",
    "    # Calculate the NMSE\n",
    "    nmse_LS_NN = utils.calNMSE(H_val_LS_NN_complex, H_val_true_complex)\n",
    "    \n",
    "    variables['NMSE_LS_NN'] = nmse_LS_NN.cpu().mean()\n",
    "    nmse_LS_NN_val.append(variables['NMSE_LS_NN'].item())\n",
    "    print(f\"LS+LI NMSE: {variables['NMSE_LS_NN'].item()}\")\n",
    "    \n",
    "    plotfig.figPredChan(H_val_NN_denormd[-1,0,:,:], 'LS+CNN Estimated Channel (after de-normlized)',\n",
    "                            nmse_LS_NN[-1], index_save, figure_save_path, '_LS_CNN_estimatedChan.png')\n",
    "    \n",
    "\n",
    "    torch.save( variables,variable_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(SNR, nmse_LS_LI_val, label='LS+LI')\n",
    "plt.plot(SNR, nmse_LS_NN_val, label='LS+NN')\n",
    "plt.plot(SNR, nmse_LI_NN_val, label='LS+LI+NN')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('NMSE')\n",
    "plt.title('Average NMSE over SNR')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
