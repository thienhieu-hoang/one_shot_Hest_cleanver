{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: DeepMIMO data: BS16, row3500_3516, 3.4 GHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "\n",
    "# Add the Torch_code directory to the Python path\n",
    "# sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "sys.path.append(os.path.abspath('../helper'))\n",
    "import config\n",
    "import utils\n",
    "import loader\n",
    "import plotfig\n",
    "# import skfuzzy as fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_PATH = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "# print(FILE_PATH)\n",
    "# print(config.temp_path)\n",
    "# print(config.FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 32 #64  # Batch size\n",
    "NUM_EPOCHS = 20 # 20\n",
    "\n",
    "# rows from DeepMIMO dataset settings\n",
    "# change rows according to the .mat dataset file \n",
    "rows = [['3500', '3516']] \n",
    "fc = '3p4' #Hz can change to '60'\n",
    "rowss = \"3500_3516\"\n",
    "learning_rate = 0.00001 # 1e-5\n",
    "SNR = np.arange(0, 31, 5) # 0:5:30 dB\n",
    "outer_file_path = os.path.abspath(os.path.join(config.FILE_PATH, \n",
    "                                                '..', 'DeepMIMOv2', 'DeepMIMO_Data', 'Static_BS16', 'freq_symb_1ant_612sub_ver4'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../model/static/CNN/BS16/3500_3516/ver13_/readme.txt' and ' ../figure/static/CNN/BS16/3500_3516/ver13_/readme.txt ' created and content written.\n"
     ]
    }
   ],
   "source": [
    "# create readme.txt file\n",
    "content = \"\"\"Generated by file 'train/static_CNN_lr1e-5_v6_1_(...).ipynb'.\n",
    "Correspond with BS16, 3.4 GHz fc, rows 3500_3516,\n",
    "DeepMIMOv2/DeepMIMO_Dta_Static_BS16/freq_sym_1ant_612sub_ver4,\n",
    "Using min-max [-1 1] scaler for each sample\n",
    "Using Tanh as activation function of CNN\"\"\"\n",
    "\n",
    "norm_approach = 'minmax' # can be set to 'std'\n",
    "\n",
    "# Paths to save\n",
    "idx_save_path = loader.find_incremental_filename('../model/static/CNN/BS16/'+ rowss,'ver', '_', '')\n",
    "model_path = '../model/static/CNN/BS16/' + rowss + '/ver' + str(idx_save_path) + '_/readme.txt'\n",
    "figure_path = '../figure/static/CNN/BS16/' + rowss + '/ver' + str(idx_save_path) + '_/readme.txt'\n",
    "\n",
    "if not os.path.exists(os.path.dirname(model_path)):\n",
    "    os.makedirs(os.path.dirname(model_path))\n",
    "if not os.path.exists(os.path.dirname(figure_path)):\n",
    "    os.makedirs(os.path.dirname(figure_path))\n",
    "\n",
    "# Open the file in write mode ('w'). If the file does not exist, it will be created.\n",
    "with open(model_path, 'w') as file:\n",
    "    # Write the content to the file\n",
    "    file.write(content)\n",
    "\n",
    "with open(figure_path, 'w') as file:\n",
    "    # Write the content to the file\n",
    "    file.write(content)\n",
    "\n",
    "print(f\"File '{model_path}' and ' {figure_path} ' created and content written.\")\n",
    "\n",
    "save_folder_model = os.path.join(config.FILE_PATH, 'model/static/CNN', 'BS16', rowss, 'ver' + str(idx_save_path) + '_')\n",
    "save_folder_fig = os.path.join(config.FILE_PATH, 'figure', 'static', 'CNN', 'BS16' ,  rowss, 'ver' + str(idx_save_path) +'_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmse_LS_LI_val   = []\n",
    "nmse_LS_NN_val   = []\n",
    "nmse_LI_NN_val   = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snr = 0\n",
    "# [trainLabels, valLabels], [H_equal_train, H_linear_train, H_practical_train], [H_equal_val, H_linear_val, H_practical_val] = loader.load_data(outer_file_path, rows, fc, device, snr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SNR: 0/30\n",
      " Training for LS+LI\n",
      "SNR: 0/30, LS+LI, Epoch 1/20, Loss: 0.07370615742970692 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.036327972754158756\n",
      "SNR: 0/30, LS+LI, Epoch 2/20, Loss: 0.02943219837927541 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.026854977943003178\n",
      "SNR: 0/30, LS+LI, Epoch 3/20, Loss: 0.025894180010614352 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.025820150378752838\n",
      "SNR: 0/30, LS+LI, Epoch 4/20, Loss: 0.025326291234557364 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.025550508363680405\n",
      "SNR: 0/30, LS+LI, Epoch 5/20, Loss: 0.025067855289942303 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.025275855071165344\n",
      "SNR: 0/30, LS+LI, Epoch 6/20, Loss: 0.024946149324911626 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.025097477122802626\n",
      "SNR: 0/30, LS+LI, Epoch 7/20, Loss: 0.02461991212763932 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.024830880828879097\n",
      "SNR: 0/30, LS+LI, Epoch 8/20, Loss: 0.024612486828118563 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.024611466225575317\n",
      "SNR: 0/30, LS+LI, Epoch 9/20, Loss: 0.024516594450011156 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02447800257835876\n",
      "SNR: 0/30, LS+LI, Epoch 10/20, Loss: 0.024300746089064105 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.024523266591131687\n",
      "SNR: 0/30, LS+LI, Epoch 11/20, Loss: 0.02418527785731956 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.024499903644689104\n",
      "SNR: 0/30, LS+LI, Epoch 12/20, Loss: 0.02407168446996704 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.024265855677764524\n",
      "SNR: 0/30, LS+LI, Epoch 13/20, Loss: 0.023962908121215744 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02406719703735276\n",
      "SNR: 0/30, LS+LI, Epoch 14/20, Loss: 0.024010919516386335 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02402432504194704\n",
      "SNR: 0/30, LS+LI, Epoch 15/20, Loss: 0.0238441958546985 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.023875303227793087\n",
      "SNR: 0/30, LS+LI, Epoch 16/20, Loss: 0.023739451428788694 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02385546686127782\n",
      "SNR: 0/30, LS+LI, Epoch 17/20, Loss: 0.023719045062830974 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02399301795627583\n",
      "SNR: 0/30, LS+LI, Epoch 18/20, Loss: 0.023679283702052956 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02377512555738742\n",
      "SNR: 0/30, LS+LI, Epoch 19/20, Loss: 0.023661147521505523 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02421517517756332\n",
      "SNR: 0/30, LS+LI, Epoch 20/20, Loss: 0.023548503460492507 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.023814708333123814\n",
      "LI+NN NMSE: 0.06768070161342621\n",
      "LS+LI NMSE: 0.08210165053606033\n",
      " Training for LS\n",
      "SNR: 0/30, LS, Epoch 1/20, Loss: 0.23437112426861775 \n",
      "SNR: 0/30, LS, Val Loss: 0.045102837749502876\n",
      "SNR: 0/30, LS, Epoch 2/20, Loss: 0.014646206868725807 \n",
      "SNR: 0/30, LS, Val Loss: 0.009541503196074204\n",
      "SNR: 0/30, LS, Epoch 3/20, Loss: 0.009504154053798248 \n",
      "SNR: 0/30, LS, Val Loss: 0.008987713160670617\n",
      "SNR: 0/30, LS, Epoch 4/20, Loss: 0.00898973394163646 \n",
      "SNR: 0/30, LS, Val Loss: 0.008732324233278632\n",
      "SNR: 0/30, LS, Epoch 5/20, Loss: 0.008452296416751694 \n",
      "SNR: 0/30, LS, Val Loss: 0.009501207069578495\n",
      "SNR: 0/30, LS, Epoch 6/20, Loss: 0.008358263047845212 \n",
      "SNR: 0/30, LS, Val Loss: 0.007666122358800335\n",
      "SNR: 0/30, LS, Epoch 7/20, Loss: 0.007960907224197547 \n",
      "SNR: 0/30, LS, Val Loss: 0.008009746586057272\n",
      "SNR: 0/30, LS, Epoch 8/20, Loss: 0.008045842787142583 \n",
      "SNR: 0/30, LS, Val Loss: 0.0075800062834539194\n",
      "SNR: 0/30, LS, Epoch 9/20, Loss: 0.00786404246410225 \n",
      "SNR: 0/30, LS, Val Loss: 0.007209545488215305\n",
      "SNR: 0/30, LS, Epoch 10/20, Loss: 0.00783084879824242 \n",
      "SNR: 0/30, LS, Val Loss: 0.0070657158820805225\n",
      "SNR: 0/30, LS, Epoch 11/20, Loss: 0.007394659319394376 \n",
      "SNR: 0/30, LS, Val Loss: 0.006903879958289591\n",
      "SNR: 0/30, LS, Epoch 12/20, Loss: 0.0074690922981041465 \n",
      "SNR: 0/30, LS, Val Loss: 0.00689302827231586\n",
      "SNR: 0/30, LS, Epoch 13/20, Loss: 0.007633154053067745 \n",
      "SNR: 0/30, LS, Val Loss: 0.006734931164167144\n",
      "SNR: 0/30, LS, Epoch 14/20, Loss: 0.007379250465567375 \n",
      "SNR: 0/30, LS, Val Loss: 0.007085375403138724\n",
      "SNR: 0/30, LS, Epoch 15/20, Loss: 0.007406605364278306 \n",
      "SNR: 0/30, LS, Val Loss: 0.006438008509576321\n",
      "SNR: 0/30, LS, Epoch 16/20, Loss: 0.00703826762714185 \n",
      "SNR: 0/30, LS, Val Loss: 0.00649110272272744\n",
      "SNR: 0/30, LS, Epoch 17/20, Loss: 0.00706267305974697 \n",
      "SNR: 0/30, LS, Val Loss: 0.006398687774146145\n",
      "SNR: 0/30, LS, Epoch 18/20, Loss: 0.007317472016438842 \n",
      "SNR: 0/30, LS, Val Loss: 0.006565754598175938\n",
      "SNR: 0/30, LS, Epoch 19/20, Loss: 0.0069674218322561925 \n",
      "SNR: 0/30, LS, Val Loss: 0.006428208900615573\n",
      "SNR: 0/30, LS, Epoch 20/20, Loss: 0.00690597676953604 \n",
      "SNR: 0/30, LS, Val Loss: 0.006188417234542695\n",
      "LS+LI NMSE: 0.017458762973546982\n",
      " SNR: 5/30\n",
      " Training for LS+LI\n",
      "SNR: 5/30, LS+LI, Epoch 1/20, Loss: 0.06525620629707742 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.0211380525407466\n",
      "SNR: 5/30, LS+LI, Epoch 2/20, Loss: 0.016182842945983242 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.013345903081988747\n",
      "SNR: 5/30, LS+LI, Epoch 3/20, Loss: 0.012855877000621931 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.011464416938410564\n",
      "SNR: 5/30, LS+LI, Epoch 4/20, Loss: 0.011493683900944022 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010477370180358941\n",
      "SNR: 5/30, LS+LI, Epoch 5/20, Loss: 0.010874868405787923 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010014815848659386\n",
      "SNR: 5/30, LS+LI, Epoch 6/20, Loss: 0.010583354566289588 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.009791071780703285\n",
      "SNR: 5/30, LS+LI, Epoch 7/20, Loss: 0.010500792922928583 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.009677748534489761\n",
      "SNR: 5/30, LS+LI, Epoch 8/20, Loss: 0.010211153199549678 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.009519081520424648\n",
      "SNR: 5/30, LS+LI, Epoch 9/20, Loss: 0.010117206251421986 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.009282131425359032\n",
      "SNR: 5/30, LS+LI, Epoch 10/20, Loss: 0.009971938743039446 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.00922973005270416\n",
      "SNR: 5/30, LS+LI, Epoch 11/20, Loss: 0.009827328661759926 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.009008445776998997\n",
      "SNR: 5/30, LS+LI, Epoch 12/20, Loss: 0.009781545033505144 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.009359253252941098\n",
      "SNR: 5/30, LS+LI, Epoch 13/20, Loss: 0.00962106472552689 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.008845629424534061\n",
      "SNR: 5/30, LS+LI, Epoch 14/20, Loss: 0.009593705730621032 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.008793768832798709\n",
      "SNR: 5/30, LS+LI, Epoch 15/20, Loss: 0.009590332347596453 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.00879244552925229\n",
      "SNR: 5/30, LS+LI, Epoch 16/20, Loss: 0.009434366785531301 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.009261202659796585\n",
      "SNR: 5/30, LS+LI, Epoch 17/20, Loss: 0.009454735113459444 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.00875155996023254\n",
      "SNR: 5/30, LS+LI, Epoch 18/20, Loss: 0.009408702524270602 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.00871227491139011\n",
      "SNR: 5/30, LS+LI, Epoch 19/20, Loss: 0.009393915052114184 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.008595493410459974\n",
      "SNR: 5/30, LS+LI, Epoch 20/20, Loss: 0.009358575326650468 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.00888282750648531\n",
      "LI+NN NMSE: 0.025589311495423317\n",
      "LS+LI NMSE: 0.025962645187973976\n",
      " Training for LS\n",
      "SNR: 5/30, LS, Epoch 1/20, Loss: 0.2335506132695564 \n",
      "SNR: 5/30, LS, Val Loss: 0.035198703916235405\n",
      "SNR: 5/30, LS, Epoch 2/20, Loss: 0.01204162932185153 \n",
      "SNR: 5/30, LS, Val Loss: 0.006227519024502148\n",
      "SNR: 5/30, LS, Epoch 3/20, Loss: 0.006662901006314123 \n",
      "SNR: 5/30, LS, Val Loss: 0.004980964078144593\n",
      "SNR: 5/30, LS, Epoch 4/20, Loss: 0.005872833501915772 \n",
      "SNR: 5/30, LS, Val Loss: 0.004778598445806314\n",
      "SNR: 5/30, LS, Epoch 5/20, Loss: 0.0056904945698523415 \n",
      "SNR: 5/30, LS, Val Loss: 0.004371365712193603\n",
      "SNR: 5/30, LS, Epoch 6/20, Loss: 0.005577636787906116 \n",
      "SNR: 5/30, LS, Val Loss: 0.004026652546599507\n",
      "SNR: 5/30, LS, Epoch 7/20, Loss: 0.005191307532503594 \n",
      "SNR: 5/30, LS, Val Loss: 0.003994657795622267\n",
      "SNR: 5/30, LS, Epoch 8/20, Loss: 0.005482818594278204 \n",
      "SNR: 5/30, LS, Val Loss: 0.0038607399699024177\n",
      "SNR: 5/30, LS, Epoch 9/20, Loss: 0.005000127090931719 \n",
      "SNR: 5/30, LS, Val Loss: 0.00376306269952858\n",
      "SNR: 5/30, LS, Epoch 10/20, Loss: 0.004724821632154026 \n",
      "SNR: 5/30, LS, Val Loss: 0.003606716255572709\n",
      "SNR: 5/30, LS, Epoch 11/20, Loss: 0.005008063964047577 \n",
      "SNR: 5/30, LS, Val Loss: 0.0036628517555072904\n",
      "SNR: 5/30, LS, Epoch 12/20, Loss: 0.004870228593909117 \n",
      "SNR: 5/30, LS, Val Loss: 0.0037075754953548312\n",
      "SNR: 5/30, LS, Epoch 13/20, Loss: 0.004768897250002293 \n",
      "SNR: 5/30, LS, Val Loss: 0.0034472112311050296\n",
      "SNR: 5/30, LS, Epoch 14/20, Loss: 0.004816972295926927 \n",
      "SNR: 5/30, LS, Val Loss: 0.0035001436481252313\n",
      "SNR: 5/30, LS, Epoch 15/20, Loss: 0.004635472258087248 \n",
      "SNR: 5/30, LS, Val Loss: 0.0034945295615629716\n",
      "SNR: 5/30, LS, Epoch 16/20, Loss: 0.0044171653761051945 \n",
      "SNR: 5/30, LS, Val Loss: 0.0032179992518980394\n",
      "SNR: 5/30, LS, Epoch 17/20, Loss: 0.004481251419218647 \n",
      "SNR: 5/30, LS, Val Loss: 0.004193477618338709\n",
      "SNR: 5/30, LS, Epoch 18/20, Loss: 0.004478146913623827 \n",
      "SNR: 5/30, LS, Val Loss: 0.003482628529044715\n",
      "SNR: 5/30, LS, Epoch 19/20, Loss: 0.00444011787645692 \n",
      "SNR: 5/30, LS, Val Loss: 0.0033295409838584337\n",
      "SNR: 5/30, LS, Epoch 20/20, Loss: 0.004441304872797932 \n",
      "SNR: 5/30, LS, Val Loss: 0.0031876552278514614\n",
      "LS+LI NMSE: 0.008919230662286282\n",
      " SNR: 10/30\n",
      " Training for LS+LI\n",
      "SNR: 10/30, LS+LI, Epoch 1/20, Loss: 0.057915162609153706 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.02019163305786523\n",
      "SNR: 10/30, LS+LI, Epoch 2/20, Loss: 0.011936252996282176 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.008329733994535425\n",
      "SNR: 10/30, LS+LI, Epoch 3/20, Loss: 0.007006190909440954 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.006119113288481127\n",
      "SNR: 10/30, LS+LI, Epoch 4/20, Loss: 0.005567824538407284 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.005380307070233605\n",
      "SNR: 10/30, LS+LI, Epoch 5/20, Loss: 0.005171420561626207 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.005245141431011937\n",
      "SNR: 10/30, LS+LI, Epoch 6/20, Loss: 0.005034071890106641 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.005098682732998647\n",
      "SNR: 10/30, LS+LI, Epoch 7/20, Loss: 0.0048972012889354904 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.005016271427104419\n",
      "SNR: 10/30, LS+LI, Epoch 8/20, Loss: 0.004712259961389612 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004830485664900731\n",
      "SNR: 10/30, LS+LI, Epoch 9/20, Loss: 0.004655569331547202 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004624594066461379\n",
      "SNR: 10/30, LS+LI, Epoch 10/20, Loss: 0.00453745418358161 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.005344709720123898\n",
      "SNR: 10/30, LS+LI, Epoch 11/20, Loss: 0.004402712646731024 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004704200800254263\n",
      "SNR: 10/30, LS+LI, Epoch 12/20, Loss: 0.00441524118921438 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.0043204841098155485\n",
      "SNR: 10/30, LS+LI, Epoch 13/20, Loss: 0.004223593112709391 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.0042366482719609685\n",
      "SNR: 10/30, LS+LI, Epoch 14/20, Loss: 0.004087720553628927 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004244170479730449\n",
      "SNR: 10/30, LS+LI, Epoch 15/20, Loss: 0.0041014749413291205 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.0041444727896966715\n",
      "SNR: 10/30, LS+LI, Epoch 16/20, Loss: 0.004058343346154881 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004368306423368102\n",
      "SNR: 10/30, LS+LI, Epoch 17/20, Loss: 0.00400768751061933 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004322609938257797\n",
      "SNR: 10/30, LS+LI, Epoch 18/20, Loss: 0.0039615179994143546 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004054687811400403\n",
      "SNR: 10/30, LS+LI, Epoch 19/20, Loss: 0.0039641539455218195 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004268236098472367\n",
      "SNR: 10/30, LS+LI, Epoch 20/20, Loss: 0.003916294924470834 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004104899411851709\n",
      "LI+NN NMSE: 0.011102642863988876\n",
      "LS+LI NMSE: 0.008175253868103027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thien/Hprediction/one_shot_Hest_cleanver/Torch_code/helper/plotfig.py:30: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(10, 5))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training for LS\n",
      "SNR: 10/30, LS, Epoch 1/20, Loss: 0.243845738687141 \n",
      "SNR: 10/30, LS, Val Loss: 0.05351934666660699\n",
      "SNR: 10/30, LS, Epoch 2/20, Loss: 0.01345342620774064 \n",
      "SNR: 10/30, LS, Val Loss: 0.007052382157946175\n",
      "SNR: 10/30, LS, Epoch 3/20, Loss: 0.006098536750150108 \n",
      "SNR: 10/30, LS, Val Loss: 0.005877260975963013\n",
      "SNR: 10/30, LS, Epoch 4/20, Loss: 0.0056589555081933045 \n",
      "SNR: 10/30, LS, Val Loss: 0.00507813922806897\n",
      "SNR: 10/30, LS, Epoch 5/20, Loss: 0.00527589306435656 \n",
      "SNR: 10/30, LS, Val Loss: 0.004525031185370277\n",
      "SNR: 10/30, LS, Epoch 6/20, Loss: 0.004926885342831875 \n",
      "SNR: 10/30, LS, Val Loss: 0.004614619030193849\n",
      "SNR: 10/30, LS, Epoch 7/20, Loss: 0.004621217170795209 \n",
      "SNR: 10/30, LS, Val Loss: 0.0038678195877847347\n",
      "SNR: 10/30, LS, Epoch 8/20, Loss: 0.004316514839829747 \n",
      "SNR: 10/30, LS, Val Loss: 0.0036580068673091855\n",
      "SNR: 10/30, LS, Epoch 9/20, Loss: 0.004493439002883036 \n",
      "SNR: 10/30, LS, Val Loss: 0.005071762970394709\n",
      "SNR: 10/30, LS, Epoch 10/20, Loss: 0.0041389383516426004 \n",
      "SNR: 10/30, LS, Val Loss: 0.005430491069670428\n",
      "SNR: 10/30, LS, Epoch 11/20, Loss: 0.003913109718691991 \n",
      "SNR: 10/30, LS, Val Loss: 0.0035008028975095262\n",
      "SNR: 10/30, LS, Epoch 12/20, Loss: 0.004057295495904115 \n",
      "SNR: 10/30, LS, Val Loss: 0.002928395999002863\n",
      "SNR: 10/30, LS, Epoch 13/20, Loss: 0.003833381415799607 \n",
      "SNR: 10/30, LS, Val Loss: 0.005208799128674648\n",
      "SNR: 10/30, LS, Epoch 14/20, Loss: 0.003953661925977009 \n",
      "SNR: 10/30, LS, Val Loss: 0.00313024000603367\n",
      "SNR: 10/30, LS, Epoch 15/20, Loss: 0.0037796367514813536 \n",
      "SNR: 10/30, LS, Val Loss: 0.002605341628871181\n",
      "SNR: 10/30, LS, Epoch 16/20, Loss: 0.0037016785706306785 \n",
      "SNR: 10/30, LS, Val Loss: 0.0027291164733469486\n",
      "SNR: 10/30, LS, Epoch 17/20, Loss: 0.0035746056785492964 \n",
      "SNR: 10/30, LS, Val Loss: 0.002565857425162738\n",
      "SNR: 10/30, LS, Epoch 18/20, Loss: 0.0035838172148350018 \n",
      "SNR: 10/30, LS, Val Loss: 0.0024096904513002796\n",
      "SNR: 10/30, LS, Epoch 19/20, Loss: 0.0034321895441809277 \n",
      "SNR: 10/30, LS, Val Loss: 0.0023050000944005496\n",
      "SNR: 10/30, LS, Epoch 20/20, Loss: 0.0035608628227342958 \n",
      "SNR: 10/30, LS, Val Loss: 0.0022350542818788777\n",
      "LS+LI NMSE: 0.0059885624796152115\n",
      " SNR: 15/30\n",
      " Training for LS+LI\n",
      "SNR: 15/30, LS+LI, Epoch 1/20, Loss: 0.06088516141049737 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.01687365169213577\n",
      "SNR: 15/30, LS+LI, Epoch 2/20, Loss: 0.009547473689497904 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.005646930245513266\n",
      "SNR: 15/30, LS+LI, Epoch 3/20, Loss: 0.004237820510752499 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0037019202506847\n",
      "SNR: 15/30, LS+LI, Epoch 4/20, Loss: 0.003430665684539045 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0032361182384192944\n",
      "SNR: 15/30, LS+LI, Epoch 5/20, Loss: 0.003166723982864168 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0029734865106134253\n",
      "SNR: 15/30, LS+LI, Epoch 6/20, Loss: 0.0029676507273147445 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0028729635735296392\n",
      "SNR: 15/30, LS+LI, Epoch 7/20, Loss: 0.0027816506592708446 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0025809214196421885\n",
      "SNR: 15/30, LS+LI, Epoch 8/20, Loss: 0.0026224949579891684 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0024461681340736422\n",
      "SNR: 15/30, LS+LI, Epoch 9/20, Loss: 0.0024843235631462532 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0023093900209377434\n",
      "SNR: 15/30, LS+LI, Epoch 10/20, Loss: 0.002400406406174392 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0021986504275859757\n",
      "SNR: 15/30, LS+LI, Epoch 11/20, Loss: 0.002216863047959673 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0021525344239886513\n",
      "SNR: 15/30, LS+LI, Epoch 12/20, Loss: 0.002188489669500742 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0021253763327629053\n",
      "SNR: 15/30, LS+LI, Epoch 13/20, Loss: 0.002086273069645083 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0019583492433990946\n",
      "SNR: 15/30, LS+LI, Epoch 14/20, Loss: 0.0021238458832877494 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0020669656329449604\n",
      "SNR: 15/30, LS+LI, Epoch 15/20, Loss: 0.001969150903764679 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.001876772280824794\n",
      "SNR: 15/30, LS+LI, Epoch 16/20, Loss: 0.0019729430516531995 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0019003078553148291\n",
      "SNR: 15/30, LS+LI, Epoch 17/20, Loss: 0.0019474338102533461 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0017362288252281194\n",
      "SNR: 15/30, LS+LI, Epoch 18/20, Loss: 0.0018522276253396168 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0016722097696567123\n",
      "SNR: 15/30, LS+LI, Epoch 19/20, Loss: 0.0018586897085485763 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0020156681262464685\n",
      "SNR: 15/30, LS+LI, Epoch 20/20, Loss: 0.001823107645641146 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.001788793084084649\n",
      "LI+NN NMSE: 0.005050125997513533\n",
      "LS+LI NMSE: 0.002613652031868696\n",
      " Training for LS\n",
      "SNR: 15/30, LS, Epoch 1/20, Loss: 0.2518719257570283 \n",
      "SNR: 15/30, LS, Val Loss: 0.08733843266963959\n",
      "SNR: 15/30, LS, Epoch 2/20, Loss: 0.019711529394763328 \n",
      "SNR: 15/30, LS, Val Loss: 0.005020335983400318\n",
      "SNR: 15/30, LS, Epoch 3/20, Loss: 0.005791014259367055 \n",
      "SNR: 15/30, LS, Val Loss: 0.004200527402149005\n",
      "SNR: 15/30, LS, Epoch 4/20, Loss: 0.005323913091858632 \n",
      "SNR: 15/30, LS, Val Loss: 0.0038134848529642277\n",
      "SNR: 15/30, LS, Epoch 5/20, Loss: 0.004696039294791516 \n",
      "SNR: 15/30, LS, Val Loss: 0.0030498009238561444\n",
      "SNR: 15/30, LS, Epoch 6/20, Loss: 0.00467498229442944 \n",
      "SNR: 15/30, LS, Val Loss: 0.0030251124937256627\n",
      "SNR: 15/30, LS, Epoch 7/20, Loss: 0.0042658640913730275 \n",
      "SNR: 15/30, LS, Val Loss: 0.002636515969325873\n",
      "SNR: 15/30, LS, Epoch 8/20, Loss: 0.0038562041672141572 \n",
      "SNR: 15/30, LS, Val Loss: 0.002424771368334239\n",
      "SNR: 15/30, LS, Epoch 9/20, Loss: 0.00373404277055416 \n",
      "SNR: 15/30, LS, Val Loss: 0.0021986552958630705\n",
      "SNR: 15/30, LS, Epoch 10/20, Loss: 0.003523913021930385 \n",
      "SNR: 15/30, LS, Val Loss: 0.0022004813178103755\n",
      "SNR: 15/30, LS, Epoch 11/20, Loss: 0.0035796565623672375 \n",
      "SNR: 15/30, LS, Val Loss: 0.0020983469915914943\n",
      "SNR: 15/30, LS, Epoch 12/20, Loss: 0.003465039890427359 \n",
      "SNR: 15/30, LS, Val Loss: 0.0023472203862514684\n",
      "SNR: 15/30, LS, Epoch 13/20, Loss: 0.00341712677483122 \n",
      "SNR: 15/30, LS, Val Loss: 0.0018737092561257834\n",
      "SNR: 15/30, LS, Epoch 14/20, Loss: 0.0032929442687566544 \n",
      "SNR: 15/30, LS, Val Loss: 0.0017297666636295617\n",
      "SNR: 15/30, LS, Epoch 15/20, Loss: 0.003447602954449479 \n",
      "SNR: 15/30, LS, Val Loss: 0.0016732264405370436\n",
      "SNR: 15/30, LS, Epoch 16/20, Loss: 0.0030584870817323842 \n",
      "SNR: 15/30, LS, Val Loss: 0.0017151572815650566\n",
      "SNR: 15/30, LS, Epoch 17/20, Loss: 0.003108354430878535 \n",
      "SNR: 15/30, LS, Val Loss: 0.0016928977671671998\n",
      "SNR: 15/30, LS, Epoch 18/20, Loss: 0.0030732156389371252 \n",
      "SNR: 15/30, LS, Val Loss: 0.002046285599300807\n",
      "SNR: 15/30, LS, Epoch 19/20, Loss: 0.0031645268297938326 \n",
      "SNR: 15/30, LS, Val Loss: 0.0015283575630746782\n",
      "SNR: 15/30, LS, Epoch 20/20, Loss: 0.002967172809253815 \n",
      "SNR: 15/30, LS, Val Loss: 0.0016151748184876685\n",
      "LS+LI NMSE: 0.0045186723582446575\n",
      " SNR: 20/30\n",
      " Training for LS+LI\n",
      "SNR: 20/30, LS+LI, Epoch 1/20, Loss: 0.053858516488744075 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.015523686751045963\n",
      "SNR: 20/30, LS+LI, Epoch 2/20, Loss: 0.00837439818166976 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.004533294937573373\n",
      "SNR: 20/30, LS+LI, Epoch 3/20, Loss: 0.0034409341345497863 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.002678003363226625\n",
      "SNR: 20/30, LS+LI, Epoch 4/20, Loss: 0.002750481102550619 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.00239096423716877\n",
      "SNR: 20/30, LS+LI, Epoch 5/20, Loss: 0.0024614560656076256 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.002105659895195541\n",
      "SNR: 20/30, LS+LI, Epoch 6/20, Loss: 0.0022134236926429495 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0018933718992312524\n",
      "SNR: 20/30, LS+LI, Epoch 7/20, Loss: 0.00204244434365158 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0019266781746409833\n",
      "SNR: 20/30, LS+LI, Epoch 8/20, Loss: 0.001957249061507714 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.001696740303569558\n",
      "SNR: 20/30, LS+LI, Epoch 9/20, Loss: 0.0017794158376027765 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0014032224161465738\n",
      "SNR: 20/30, LS+LI, Epoch 10/20, Loss: 0.0016110748171719702 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.001380489562341774\n",
      "SNR: 20/30, LS+LI, Epoch 11/20, Loss: 0.0014861167393635613 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0011640195628966796\n",
      "SNR: 20/30, LS+LI, Epoch 12/20, Loss: 0.0013969012679597146 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0014257515960542316\n",
      "SNR: 20/30, LS+LI, Epoch 13/20, Loss: 0.0013705713797609734 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0012941788352856581\n",
      "SNR: 20/30, LS+LI, Epoch 14/20, Loss: 0.0012241274316058775 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0009654726335694166\n",
      "SNR: 20/30, LS+LI, Epoch 15/20, Loss: 0.0012035636653647173 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0008908799447288567\n",
      "SNR: 20/30, LS+LI, Epoch 16/20, Loss: 0.0010894664165937494 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0008515915330711075\n",
      "SNR: 20/30, LS+LI, Epoch 17/20, Loss: 0.00112097050526266 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.001207086157096042\n",
      "SNR: 20/30, LS+LI, Epoch 18/20, Loss: 0.0010797465832627833 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0007665136746998707\n",
      "SNR: 20/30, LS+LI, Epoch 19/20, Loss: 0.0010347438540964842 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.000739728928734125\n",
      "SNR: 20/30, LS+LI, Epoch 20/20, Loss: 0.001079770590200327 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0007227952869884161\n",
      "LI+NN NMSE: 0.0020878734067082405\n",
      "LS+LI NMSE: 0.0008191458764486015\n",
      " Training for LS\n",
      "SNR: 20/30, LS, Epoch 1/20, Loss: 0.23334868832729583 \n",
      "SNR: 20/30, LS, Val Loss: 0.06154664622789079\n",
      "SNR: 20/30, LS, Epoch 2/20, Loss: 0.016712036373656848 \n",
      "SNR: 20/30, LS, Val Loss: 0.006758544551716609\n",
      "SNR: 20/30, LS, Epoch 3/20, Loss: 0.00672586880174877 \n",
      "SNR: 20/30, LS, Val Loss: 0.0060024752535603266\n",
      "SNR: 20/30, LS, Epoch 4/20, Loss: 0.005531570435566611 \n",
      "SNR: 20/30, LS, Val Loss: 0.004517942053181204\n",
      "SNR: 20/30, LS, Epoch 5/20, Loss: 0.005216644249780667 \n",
      "SNR: 20/30, LS, Val Loss: 0.00408604312476448\n",
      "SNR: 20/30, LS, Epoch 6/20, Loss: 0.004822438298508005 \n",
      "SNR: 20/30, LS, Val Loss: 0.0039135767146945\n",
      "SNR: 20/30, LS, Epoch 7/20, Loss: 0.0043361271179259515 \n",
      "SNR: 20/30, LS, Val Loss: 0.003169936540705914\n",
      "SNR: 20/30, LS, Epoch 8/20, Loss: 0.004004867288533078 \n",
      "SNR: 20/30, LS, Val Loss: 0.0029101699738847938\n",
      "SNR: 20/30, LS, Epoch 9/20, Loss: 0.003690869310535057 \n",
      "SNR: 20/30, LS, Val Loss: 0.002684889641717415\n",
      "SNR: 20/30, LS, Epoch 10/20, Loss: 0.0035782497075689565 \n",
      "SNR: 20/30, LS, Val Loss: 0.0026052296584980054\n",
      "SNR: 20/30, LS, Epoch 11/20, Loss: 0.0036908032732669177 \n",
      "SNR: 20/30, LS, Val Loss: 0.002166015561670065\n",
      "SNR: 20/30, LS, Epoch 12/20, Loss: 0.003380877267011593 \n",
      "SNR: 20/30, LS, Val Loss: 0.0024062740796414964\n",
      "SNR: 20/30, LS, Epoch 13/20, Loss: 0.0034710331034824944 \n",
      "SNR: 20/30, LS, Val Loss: 0.0022517547582868824\n",
      "SNR: 20/30, LS, Epoch 14/20, Loss: 0.00313256862153689 \n",
      "SNR: 20/30, LS, Val Loss: 0.0016993916889821942\n",
      "SNR: 20/30, LS, Epoch 15/20, Loss: 0.0033040730205505307 \n",
      "SNR: 20/30, LS, Val Loss: 0.0018257093986242332\n",
      "SNR: 20/30, LS, Epoch 16/20, Loss: 0.0029852112598154085 \n",
      "SNR: 20/30, LS, Val Loss: 0.002109368807974864\n",
      "SNR: 20/30, LS, Epoch 17/20, Loss: 0.0030739579064132604 \n",
      "SNR: 20/30, LS, Val Loss: 0.0019295432774180715\n",
      "SNR: 20/30, LS, Epoch 18/20, Loss: 0.0031008261504994576 \n",
      "SNR: 20/30, LS, Val Loss: 0.001529120411512188\n",
      "SNR: 20/30, LS, Epoch 19/20, Loss: 0.002912412490289615 \n",
      "SNR: 20/30, LS, Val Loss: 0.001606182306369936\n",
      "SNR: 20/30, LS, Epoch 20/20, Loss: 0.0028533058431876694 \n",
      "SNR: 20/30, LS, Val Loss: 0.0013416063659612767\n",
      "LS+LI NMSE: 0.0038116967771202326\n",
      " SNR: 25/30\n",
      " Training for LS+LI\n",
      "SNR: 25/30, LS+LI, Epoch 1/20, Loss: 0.06630738140192143 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.016926716335795143\n",
      "SNR: 25/30, LS+LI, Epoch 2/20, Loss: 0.009825511543123527 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.005992881551554257\n",
      "SNR: 25/30, LS+LI, Epoch 3/20, Loss: 0.004289998513169933 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.002709512270733037\n",
      "SNR: 25/30, LS+LI, Epoch 4/20, Loss: 0.002394706184764631 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0019270239281468093\n",
      "SNR: 25/30, LS+LI, Epoch 5/20, Loss: 0.0020531306556306867 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0017440383724698966\n",
      "SNR: 25/30, LS+LI, Epoch 6/20, Loss: 0.001785886560588382 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0015154609837653961\n",
      "SNR: 25/30, LS+LI, Epoch 7/20, Loss: 0.0016787071566119096 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0013584502422335472\n",
      "SNR: 25/30, LS+LI, Epoch 8/20, Loss: 0.0015416097337228442 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.00121267008415254\n",
      "SNR: 25/30, LS+LI, Epoch 9/20, Loss: 0.0013885601494071443 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0010439854628533465\n",
      "SNR: 25/30, LS+LI, Epoch 10/20, Loss: 0.0013023950613028 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0009596278118392961\n",
      "SNR: 25/30, LS+LI, Epoch 11/20, Loss: 0.001131368069340973 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0008414683292027224\n",
      "SNR: 25/30, LS+LI, Epoch 12/20, Loss: 0.001071884914106408 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0007744263857603073\n",
      "SNR: 25/30, LS+LI, Epoch 13/20, Loss: 0.0009744740324771724 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0011382706738500433\n",
      "SNR: 25/30, LS+LI, Epoch 14/20, Loss: 0.0008926813288992488 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0006414430866822262\n",
      "SNR: 25/30, LS+LI, Epoch 15/20, Loss: 0.000847768913825898 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0005927953197070482\n",
      "SNR: 25/30, LS+LI, Epoch 16/20, Loss: 0.0007928944866952681 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0005340147750261663\n",
      "SNR: 25/30, LS+LI, Epoch 17/20, Loss: 0.0007883496355768943 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.00046978760457767004\n",
      "SNR: 25/30, LS+LI, Epoch 18/20, Loss: 0.000723384674888938 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0004444442483046177\n",
      "SNR: 25/30, LS+LI, Epoch 19/20, Loss: 0.0008012672477106694 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0004433640516998077\n",
      "SNR: 25/30, LS+LI, Epoch 20/20, Loss: 0.0006988022466620625 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0004146882605379109\n",
      "LI+NN NMSE: 0.0011820573126897216\n",
      "LS+LI NMSE: 0.0002607359201647341\n",
      " Training for LS\n",
      "SNR: 25/30, LS, Epoch 1/20, Loss: 0.2574072059157283 \n",
      "SNR: 25/30, LS, Val Loss: 0.08687523413788188\n",
      "SNR: 25/30, LS, Epoch 2/20, Loss: 0.018114481872586578 \n",
      "SNR: 25/30, LS, Val Loss: 0.004979873478243297\n",
      "SNR: 25/30, LS, Epoch 3/20, Loss: 0.005482741742568134 \n",
      "SNR: 25/30, LS, Val Loss: 0.003717448594133285\n",
      "SNR: 25/30, LS, Epoch 4/20, Loss: 0.005061986351244932 \n",
      "SNR: 25/30, LS, Val Loss: 0.0045291175113313575\n",
      "SNR: 25/30, LS, Epoch 5/20, Loss: 0.00461721577990237 \n",
      "SNR: 25/30, LS, Val Loss: 0.003413287578785623\n",
      "SNR: 25/30, LS, Epoch 6/20, Loss: 0.003949236460463252 \n",
      "SNR: 25/30, LS, Val Loss: 0.002567816483364864\n",
      "SNR: 25/30, LS, Epoch 7/20, Loss: 0.0036702619278045413 \n",
      "SNR: 25/30, LS, Val Loss: 0.0025339296182871544\n",
      "SNR: 25/30, LS, Epoch 8/20, Loss: 0.00364887502131074 \n",
      "SNR: 25/30, LS, Val Loss: 0.002301024839239703\n",
      "SNR: 25/30, LS, Epoch 9/20, Loss: 0.0034259347824466438 \n",
      "SNR: 25/30, LS, Val Loss: 0.0022464926005341113\n",
      "SNR: 25/30, LS, Epoch 10/20, Loss: 0.0035564917800298265 \n",
      "SNR: 25/30, LS, Val Loss: 0.0019685008499602027\n",
      "SNR: 25/30, LS, Epoch 11/20, Loss: 0.003459974171777884 \n",
      "SNR: 25/30, LS, Val Loss: 0.0019170732544311745\n",
      "SNR: 25/30, LS, Epoch 12/20, Loss: 0.00317661912210264 \n",
      "SNR: 25/30, LS, Val Loss: 0.0020329781374047425\n",
      "SNR: 25/30, LS, Epoch 13/20, Loss: 0.003377545709575548 \n",
      "SNR: 25/30, LS, Val Loss: 0.0017526999052444642\n",
      "SNR: 25/30, LS, Epoch 14/20, Loss: 0.003053582609132972 \n",
      "SNR: 25/30, LS, Val Loss: 0.0016461000460284677\n",
      "SNR: 25/30, LS, Epoch 15/20, Loss: 0.0030319204135678796 \n",
      "SNR: 25/30, LS, Val Loss: 0.0017781397645277057\n",
      "SNR: 25/30, LS, Epoch 16/20, Loss: 0.0029669377721337127 \n",
      "SNR: 25/30, LS, Val Loss: 0.0014366068961945448\n",
      "SNR: 25/30, LS, Epoch 17/20, Loss: 0.003049230150239499 \n",
      "SNR: 25/30, LS, Val Loss: 0.0016721287604675374\n",
      "SNR: 25/30, LS, Epoch 18/20, Loss: 0.0029165786251068374 \n",
      "SNR: 25/30, LS, Val Loss: 0.0013597703179005873\n",
      "SNR: 25/30, LS, Epoch 19/20, Loss: 0.002891320078011039 \n",
      "SNR: 25/30, LS, Val Loss: 0.001274226720190861\n",
      "SNR: 25/30, LS, Epoch 20/20, Loss: 0.0027778493951150585 \n",
      "SNR: 25/30, LS, Val Loss: 0.0017540733573365617\n",
      "LS+LI NMSE: 0.005168002564460039\n",
      " SNR: 30/30\n",
      " Training for LS+LI\n",
      "SNR: 30/30, LS+LI, Epoch 1/20, Loss: 0.05956675361322109 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.01589215953241695\n",
      "SNR: 30/30, LS+LI, Epoch 2/20, Loss: 0.009235948053470184 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.005581493341279301\n",
      "SNR: 30/30, LS+LI, Epoch 3/20, Loss: 0.0037945253141007797 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0024737767367200418\n",
      "SNR: 30/30, LS+LI, Epoch 4/20, Loss: 0.0022849119119454434 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.001947469458999959\n",
      "SNR: 30/30, LS+LI, Epoch 5/20, Loss: 0.0019865112316924645 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.001787270630963824\n",
      "SNR: 30/30, LS+LI, Epoch 6/20, Loss: 0.0018252990205858855 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0014383075123822148\n",
      "SNR: 30/30, LS+LI, Epoch 7/20, Loss: 0.0015920945883154609 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0012885295754213903\n",
      "SNR: 30/30, LS+LI, Epoch 8/20, Loss: 0.0014316162269096822 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0011274890593168411\n",
      "SNR: 30/30, LS+LI, Epoch 9/20, Loss: 0.0012627334376596737 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0009911899264393883\n",
      "SNR: 30/30, LS+LI, Epoch 10/20, Loss: 0.0011846580015370883 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0011101912294345145\n",
      "SNR: 30/30, LS+LI, Epoch 11/20, Loss: 0.0010520949825065123 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0007847919292874973\n",
      "SNR: 30/30, LS+LI, Epoch 12/20, Loss: 0.0009962010849131878 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0006872273304245689\n",
      "SNR: 30/30, LS+LI, Epoch 13/20, Loss: 0.0009124376340688011 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0006038578708698465\n",
      "SNR: 30/30, LS+LI, Epoch 14/20, Loss: 0.0008428220821241306 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0005323917668482119\n",
      "SNR: 30/30, LS+LI, Epoch 15/20, Loss: 0.0008335507068401311 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0004977718940195205\n",
      "SNR: 30/30, LS+LI, Epoch 16/20, Loss: 0.0007798879872330703 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0004937458778096533\n",
      "SNR: 30/30, LS+LI, Epoch 17/20, Loss: 0.0006911979455549573 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0004673608160704713\n",
      "SNR: 30/30, LS+LI, Epoch 18/20, Loss: 0.0007630757390870713 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00037018222792539746\n",
      "SNR: 30/30, LS+LI, Epoch 19/20, Loss: 0.0006396473679139257 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0003820842764319175\n",
      "SNR: 30/30, LS+LI, Epoch 20/20, Loss: 0.000695691448717573 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0003420397710711272\n",
      "LI+NN NMSE: 0.0009396157693117857\n",
      "LS+LI NMSE: 8.351995347766206e-05\n",
      " Training for LS\n",
      "SNR: 30/30, LS, Epoch 1/20, Loss: 0.23838575277477503 \n",
      "SNR: 30/30, LS, Val Loss: 0.06134110739962621\n",
      "SNR: 30/30, LS, Epoch 2/20, Loss: 0.015423495801663849 \n",
      "SNR: 30/30, LS, Val Loss: 0.006115334404801781\n",
      "SNR: 30/30, LS, Epoch 3/20, Loss: 0.006027866420227774 \n",
      "SNR: 30/30, LS, Val Loss: 0.004560200455175204\n",
      "SNR: 30/30, LS, Epoch 4/20, Loss: 0.004910834409773003 \n",
      "SNR: 30/30, LS, Val Loss: 0.0038093358849767933\n",
      "SNR: 30/30, LS, Epoch 5/20, Loss: 0.004439882884007814 \n",
      "SNR: 30/30, LS, Val Loss: 0.003334561511027542\n",
      "SNR: 30/30, LS, Epoch 6/20, Loss: 0.00431517991210269 \n",
      "SNR: 30/30, LS, Val Loss: 0.0034364376809786668\n",
      "SNR: 30/30, LS, Epoch 7/20, Loss: 0.003989415114669693 \n",
      "SNR: 30/30, LS, Val Loss: 0.00285195707957345\n",
      "SNR: 30/30, LS, Epoch 8/20, Loss: 0.003919774191291613 \n",
      "SNR: 30/30, LS, Val Loss: 0.0026469565875066273\n",
      "SNR: 30/30, LS, Epoch 9/20, Loss: 0.0034846420207624 \n",
      "SNR: 30/30, LS, Val Loss: 0.0025133129622025244\n",
      "SNR: 30/30, LS, Epoch 10/20, Loss: 0.003527200491124288 \n",
      "SNR: 30/30, LS, Val Loss: 0.0021775353710505774\n",
      "SNR: 30/30, LS, Epoch 11/20, Loss: 0.003227299609936254 \n",
      "SNR: 30/30, LS, Val Loss: 0.0022201997238549998\n",
      "SNR: 30/30, LS, Epoch 12/20, Loss: 0.0031956713714333643 \n",
      "SNR: 30/30, LS, Val Loss: 0.003192169179039245\n",
      "SNR: 30/30, LS, Epoch 13/20, Loss: 0.00313142147172394 \n",
      "SNR: 30/30, LS, Val Loss: 0.001728025122164664\n",
      "SNR: 30/30, LS, Epoch 14/20, Loss: 0.002869314091756593 \n",
      "SNR: 30/30, LS, Val Loss: 0.0019414918338456614\n",
      "SNR: 30/30, LS, Epoch 15/20, Loss: 0.0032542181610086458 \n",
      "SNR: 30/30, LS, Val Loss: 0.0020947661250829697\n",
      "SNR: 30/30, LS, Epoch 16/20, Loss: 0.003060547896216862 \n",
      "SNR: 30/30, LS, Val Loss: 0.0023476281626658006\n",
      "SNR: 30/30, LS, Epoch 17/20, Loss: 0.0029116499773768146 \n",
      "SNR: 30/30, LS, Val Loss: 0.0017320969491265714\n",
      "SNR: 30/30, LS, Epoch 18/20, Loss: 0.0027482271198519008 \n",
      "SNR: 30/30, LS, Val Loss: 0.0015234620553780007\n",
      "SNR: 30/30, LS, Epoch 19/20, Loss: 0.002722146313740374 \n",
      "SNR: 30/30, LS, Val Loss: 0.0014901703424667094\n",
      "SNR: 30/30, LS, Epoch 20/20, Loss: 0.002730446054091201 \n",
      "SNR: 30/30, LS, Val Loss: 0.001961422338023443\n",
      "LS+LI NMSE: 0.00557470228523016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for snr in SNR:\n",
    "    print(f\" SNR: {snr}/{SNR[-1]}\")\n",
    "\n",
    "    [trainLabels, valLabels], [H_equal_train, H_linear_train, H_practical_train], [H_equal_val, H_linear_val, H_practical_val] = loader.load_data(outer_file_path, rows, fc, device, snr)\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 1. When input is H_linear (after LS+LI)\n",
    "    print(f\" Training for LS+LI\")\n",
    "    # [samples, 2, 612, 14]\n",
    "    train_loader, trainLabel_min, trainLabel_max = loader.genLoader(H_linear_train, trainLabels, BATCH_SIZE, device, 'train', True, norm_approach)\n",
    "    val_loader,     valLabel_min,   valLabel_max = loader.genLoader(H_linear_val,     valLabels, BATCH_SIZE, device, 'valid', False, norm_approach)\n",
    "        # no shuffle in validation so valLabel_min and valLable_max remain the min and max arrays\n",
    "                                                                                    # of valLabels\n",
    "        # train_loader, val_loader are already normalized by their own min, max\n",
    "        # scale to range [-1 1]\n",
    "        \n",
    "    # model\n",
    "    model = utils.CNN_Est(act = 'Tanh').to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 1.5 Training loop\n",
    "    train_loss =[]\n",
    "    val_loss = []\n",
    "    H_NN_val = torch.empty_like(valLabels) # [nVal, 2, 612, 14]\n",
    "    min_H_true = []\n",
    "    max_H_true = []\n",
    "    num_epochs = NUM_EPOCHS\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        if (epoch == num_epochs-1):\n",
    "            i = 0\n",
    "        for inputs, targets, targets_min, targets_max in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_loss.append(avg_train_loss)\n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS+LI, Epoch {epoch+1}/{num_epochs}, Loss: {avg_train_loss} \")\n",
    "        \n",
    "        # Validation \n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets, val_targetsMin, val_targetsMax in val_loader:\n",
    "                val_inputs_real = val_inputs[:,0,:,:].unsqueeze(1)\n",
    "                val_inputs_imag = val_inputs[:,1,:,:].unsqueeze(1)\n",
    "                val_targets_real = val_targets[:,0,:,:].unsqueeze(1)\n",
    "                val_targets_imag = val_targets[:,1,:,:].unsqueeze(1)\n",
    "                \n",
    "                val_outputs_real = model(val_inputs_real)\n",
    "                val_loss_real = criterion(val_outputs_real, val_targets_real)\n",
    "                running_val_loss += val_loss_real.item()\n",
    "                \n",
    "                val_outputs_imag = model(val_inputs_imag)\n",
    "                val_loss_imag = criterion(val_outputs_imag, val_targets_imag)\n",
    "                running_val_loss += val_loss_imag.item()\n",
    "                \n",
    "                if (epoch == num_epochs-1): # the results after the last training \n",
    "                    H_NN_val[i:i+val_outputs_real.size(0),0,:,:].unsqueeze(1).copy_(val_outputs_real)\n",
    "                    H_NN_val[i:i+val_outputs_imag.size(0),1,:,:].unsqueeze(1).copy_(val_outputs_imag)\n",
    "                    \n",
    "                    i = i+val_outputs_imag.size(0)       \n",
    "                    \n",
    "                \n",
    "        avg_val_loss = running_val_loss / (len(val_loader)*2)\n",
    "        val_loss.append(avg_val_loss)    \n",
    "                \n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS+LI, Val Loss: {avg_val_loss}\")\n",
    "\n",
    "\n",
    "    save_folder = os.path.join(save_folder_model, str(snr)+'dB')\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    index_save = loader.find_incremental_filename(save_folder, 'CNN_', '_variable')\n",
    "\n",
    "    model_save_path = os.path.join(save_folder,  'CNN_' +str(index_save)+'_LS_LI_CNN_model.pth')\n",
    "    variable_save_path = os.path.join(save_folder, 'CNN_' +str(index_save)+'_variable.pth')\n",
    "    params_save_path = os.path.join(save_folder, 'CNN_' +str(index_save)+'_params.mat')\n",
    "    \n",
    "    params = {   \n",
    "                'SNR': snr,\n",
    "                'epoc': NUM_EPOCHS,\n",
    "                'rows': rowss,\n",
    "                'learning_rate': learning_rate,\n",
    "                'train_track_LI': train_loss,\n",
    "                'val_track_LI': val_loss,\n",
    "    }\n",
    "    variables = {             \n",
    "                'train_track_LI': train_loss,\n",
    "                'val_track_LI': val_loss,\n",
    "                # 'train_min_LI': trainData_min.cpu(),\n",
    "                # 'train_max_LI': trainData_max.cpu(),\n",
    "                # 'train_label_min': trainLabels_min.cpu(),\n",
    "                # 'train_label_max': trainLabels_max.cpu(),\n",
    "    }\n",
    "    # variable to save \n",
    "    # Save the models' state dictionaries \n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, model_save_path)\n",
    "\n",
    "    figure_save_path = os.path.join(save_folder_fig, str(snr) + 'dB') \n",
    "    \n",
    "    os.makedirs(figure_save_path, exist_ok=True)\n",
    "\n",
    "    plotfig.figLoss(train_loss, val_loss, index_save, figure_save_path, '_LS_LI_Loss.png')\n",
    "\n",
    "\n",
    "    # True channel\n",
    "    H_val_true = valLabels.cpu()\n",
    "    # convert to complex matrices\n",
    "    H_val_true_complex = torch.complex(H_val_true[:,0,:,:], H_val_true[:,1,:,:])\n",
    "    # variables['H_val_true'] = H_val_true # (nVal, 2, 612, 14)\n",
    "\n",
    "    plotfig.figTrueChan(H_val_true[-1,0,:,:], 'True Channel', index_save, figure_save_path, '_trueChannel.png')\n",
    "\n",
    "    # Estimated Channel \n",
    "    H_val_NN = H_NN_val.cpu()    \n",
    "    plotfig.figTrueChan(H_val_NN[-1,0,:,:], 'LI+CNN Estimated Channel (before de-normlized)', \n",
    "                            index_save, figure_save_path, '_LS_LI_CNN_estimatedChan_before_denorm.png')\n",
    "\n",
    "    # De-normalized                                                               \n",
    "    H_val_NN_denormd = utils.deMinMax(H_NN_val, valLabel_min, valLabel_max)\n",
    "                        #     H_NN_val == [nVal, 2, 612, 14] \n",
    "                        # valLabel_min == [nVal,1]\n",
    "                        \n",
    "    H_val_NN_denormd = H_val_NN_denormd.cpu()\n",
    "    # variables['H_val_LI_NN'] = H_val_NN_denormd # (nVal, 2, 612, 14)\n",
    "\n",
    "    # convert to complex matrices\n",
    "    H_val_NN_denormd_complex = torch.complex(H_val_NN_denormd[:,0,:,:], H_val_NN_denormd[:,1,:,:])\n",
    "    \n",
    "    nmse_LI_NN = utils.calNMSE(H_val_NN_denormd_complex, H_val_true_complex)\n",
    "    \n",
    "    variables['NMSE_LI_NN'] = nmse_LI_NN.cpu().mean()\n",
    "    nmse_LI_NN_val.append(variables['NMSE_LI_NN'].item())\n",
    "    print(f\"LI+NN NMSE: {variables['NMSE_LI_NN'].item()}\")\n",
    "    \n",
    "    plotfig.figPredChan(H_val_NN_denormd[-1,0,:,:], 'LI+CNN Estimated Channel (after de-normlized)',\n",
    "                            nmse_LI_NN[-1], index_save, figure_save_path, '_LS_LI_CNN_estimatedChan.png')\n",
    "#####\n",
    "##### above is LS+LI+NN \n",
    "\n",
    "##### following is Linear interpolated channel (only LS+LI)\n",
    "    H_val_linInterp = H_linear_val.cpu()\n",
    "    # convert to complex matrices\n",
    "    H_val_linInterp_complex = torch.complex(H_val_linInterp[:,0,:,:], H_val_linInterp[:,1,:,:]) # [?, 612, 14]\n",
    "\n",
    "    # NMSE of Linear Interpolation\n",
    "    # Calculate the NMSE\n",
    "    nmse_LI = utils.calNMSE(H_val_linInterp_complex, H_val_true_complex)\n",
    "    \n",
    "    variables['NMSE_LI'] = nmse_LI.cpu().mean()\n",
    "    nmse_LS_LI_val.append(variables['NMSE_LI'].item())\n",
    "    print(f\"LS+LI NMSE: {variables['NMSE_LI'].item()}\")\n",
    "    \n",
    "    plotfig.figPredChan(H_val_linInterp[-1,0,:,:], 'LS + Interpolate Estimated Channel',\n",
    "                            nmse_LI[-1], index_save, figure_save_path, '_LS_LI_estimatedChan.png')\n",
    "\n",
    "\n",
    "##########################################\n",
    "    # ------------------------------------------------------\n",
    "    # When Input of the NN is just H_equalized\n",
    "    print(f\" Training for LS\")\n",
    "    # [samples, 2, 612, 14]\n",
    "    H_LS_train = H_equal_train.cpu()\n",
    "    plotfig.figTrueChan(H_LS_train[0,0,:,:], 'LS Channel', index_save, figure_save_path, '_LS_Chan.png')\n",
    "    \n",
    "    # Split into training and validation sets for H_NN training\n",
    "    train_loader, trainLabel_min, trainLabel_max = loader.genLoader(H_equal_train, trainLabels, BATCH_SIZE, device, 'train',  True, norm_approach)\n",
    "    val_loader,     valLabel_min,   vallabel_max = loader.genLoader(H_equal_val,     valLabels, BATCH_SIZE, device, 'valid', False, norm_approach)\n",
    "\n",
    "\n",
    "    model2 = utils.CNN_Est(act = 'Tanh').to(device)\n",
    "    optimizer2 = torch.optim.Adam(model2.parameters(), lr=learning_rate) \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Training loop\n",
    "    train_loss =[]\n",
    "    val_loss = []\n",
    "    H_NN_val = torch.empty_like(valLabels) # [nVal, 2, 612, 14]\n",
    "    num_epochs = NUM_EPOCHS\n",
    "    for epoch in range(num_epochs):\n",
    "        model2.train()\n",
    "        running_loss = 0.0\n",
    "        if (epoch == num_epochs-1):\n",
    "            i = 0\n",
    "        for inputs, targets, targets_min, targets_max in train_loader:\n",
    "            optimizer2.zero_grad()\n",
    "            outputs = model2(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_loss.append(avg_train_loss)\n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS, Epoch {epoch+1}/{num_epochs}, Loss: {avg_train_loss} \")\n",
    "        \n",
    "        # Validation \n",
    "        model2.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets, val_targetsMin, val_targetsMax in val_loader:\n",
    "                val_inputs_real = val_inputs[:,0,:,:].unsqueeze(1)\n",
    "                val_inputs_imag = val_inputs[:,1,:,:].unsqueeze(1)\n",
    "                val_targets_real = val_targets[:,0,:,:].unsqueeze(1)\n",
    "                val_targets_imag = val_targets[:,1,:,:].unsqueeze(1)\n",
    "                \n",
    "                val_outputs_real = model2(val_inputs_real)\n",
    "                val_loss_real = criterion(val_outputs_real, val_targets_real)\n",
    "                running_val_loss += val_loss_real.item()\n",
    "                \n",
    "                val_outputs_imag = model2(val_inputs_imag)\n",
    "                val_loss_imag = criterion(val_outputs_imag, val_targets_imag)\n",
    "                running_val_loss += val_loss_imag.item()\n",
    "                \n",
    "                if (epoch == num_epochs-1):\n",
    "                    H_NN_val[i:i+val_outputs_real.size(0),0,:,:].unsqueeze(1).copy_(val_outputs_real)\n",
    "                    H_NN_val[i:i+val_outputs_imag.size(0),1,:,:].unsqueeze(1).copy_(val_outputs_imag)\n",
    "                    i = i+val_outputs_imag.size(0)\n",
    "                \n",
    "        avg_val_loss = running_val_loss / (len(val_loader)*2)\n",
    "        val_loss.append(avg_val_loss)    \n",
    "                \n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS, Val Loss: {avg_val_loss}\")\n",
    "\n",
    "    plotfig.figLoss(train_loss, val_loss, index_save, figure_save_path, '_LS_Loss.png')\n",
    "\n",
    "    # De-normalized                                                                \n",
    "    H_val_NN_denormd = utils.deMinMax(H_NN_val, valLabel_min, valLabel_max)\n",
    "                        #     H_NN_val == [nVal, 2, 612, 14] \n",
    "                        # valLabel_min == [nVal,1]\n",
    "    H_val_NN_denormd = H_val_NN_denormd.cpu()\n",
    "\n",
    "    model_save_path = os.path.join(save_folder,  'CNN_' +str(index_save)+'_LS_CNN_model.pth')\n",
    "\n",
    "    # variables['H_val_LS_NN']= H_val_NN_denormd.cpu() # (nVal, 2, 612, 14)\n",
    "    variables['train_track_LS']= train_loss\n",
    "    variables['val_track_LS']= val_loss\n",
    "\n",
    "    # Save parameters\n",
    "    params['train_track_LS']= train_loss\n",
    "    params['val_track_LS']= val_loss\n",
    "    savemat(params_save_path, params)\n",
    "    # Save the models' state dictionaries \n",
    "    torch.save({'model_state_dict': model2.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "                }, model_save_path)\n",
    "\n",
    "\n",
    "    # NMSE of LS + NN\n",
    "    H_val_LS_NN_complex = torch.complex(H_val_NN_denormd[:,0,:,:], H_val_NN_denormd[:,1,:,:])\n",
    "    # Calculate the NMSE\n",
    "    nmse_LS_NN = utils.calNMSE(H_val_LS_NN_complex, H_val_true_complex)\n",
    "    \n",
    "    variables['NMSE_LS_NN'] = nmse_LS_NN.cpu().mean()\n",
    "    nmse_LS_NN_val.append(variables['NMSE_LS_NN'].item())\n",
    "    print(f\"LS+LI NMSE: {variables['NMSE_LS_NN'].item()}\")\n",
    "    \n",
    "    plotfig.figPredChan(H_val_NN_denormd[-1,0,:,:], 'LS+CNN Estimated Channel (after de-normlized)',\n",
    "                            nmse_LS_NN[-1], index_save, figure_save_path, '_LS_CNN_estimatedChan.png')\n",
    "    \n",
    "\n",
    "    torch.save( variables,variable_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure saved at \n",
      "/home/thien/Hprediction/one_shot_Hest_cleanver/Torch_code/figure/static/CNN/BS16/3500_3516/ver13_/NMSE1.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(SNR, nmse_LS_LI_val, label='LS+LI')\n",
    "plt.plot(SNR, nmse_LS_NN_val, label='LS+NN')\n",
    "plt.plot(SNR, nmse_LI_NN_val, label='LS+LI+NN')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('NMSE')\n",
    "plt.title('Average NMSE over SNR')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(save_folder_fig, \"NMSE1.png\"))\n",
    "plt.show()\n",
    "print('Figure saved at ')\n",
    "print(os.path.join(save_folder_fig, \"NMSE1.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
