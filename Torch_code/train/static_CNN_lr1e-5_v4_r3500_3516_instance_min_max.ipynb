{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: DeepMIMO data: BS16, row3500_3516, 3.4 GHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "\n",
    "# Add the Torch_code directory to the Python path\n",
    "# sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "sys.path.append(os.path.abspath('../helper'))\n",
    "import config\n",
    "import utils\n",
    "import loader\n",
    "import plotfig\n",
    "# import skfuzzy as fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_PATH = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "# print(FILE_PATH)\n",
    "# print(config.temp_path)\n",
    "# print(config.FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 32 #64  # Batch size\n",
    "NUM_EPOCHS = 20 # 20\n",
    "\n",
    "# rows from DeepMIMO dataset settings\n",
    "# change rows according to the .mat dataset file \n",
    "rows = [['3500', '3516']] \n",
    "fc = '3p4' #Hz can change to '60'\n",
    "rowss = \"3500_3516\"\n",
    "learning_rate = 0.00001 # 1e-5\n",
    "SNR = np.arange(0, 31, 5) # 0:5:30 dB\n",
    "outer_file_path = os.path.abspath(os.path.join(config.FILE_PATH, \n",
    "                                                '..', 'DeepMIMOv2', 'DeepMIMO_Data', 'Static_BS16', 'freq_symb_1ant_612sub_ver4'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../model/static/CNN/BS16/3500_3516/ver5_/readme.txt' and ' ../figure/static/CNN/BS16/3500_3516/ver5_/readme.txt ' created and content written.\n"
     ]
    }
   ],
   "source": [
    "# create readme.txt file\n",
    "content = \"\"\"Generated by file 'train/static_CNN_lr1e-5_v4_r3500_3516_3p4_instance_min_max.ipynb'.\n",
    "Correspond with BS16, 3.4 GHz fc, rows 3500_3516,\n",
    "DeepMIMOv2/DeepMIMO_Dta_Static_BS16/freq_sym_1ant_612sub_ver4,\n",
    "Using min-max scaler for each sample\"\"\"\n",
    "\n",
    "norm_approach = 'minmax' # can be set to 'std'\n",
    "\n",
    "# Paths to save\n",
    "idx_save_path = loader.find_incremental_filename('../model/static/CNN/BS16/'+ rowss,'ver', '_', '')\n",
    "model_path = '../model/static/CNN/BS16/' + rowss + '/ver' + str(idx_save_path) + '_/readme.txt'\n",
    "figure_path = '../figure/static/CNN/BS16/' + rowss + '/ver' + str(idx_save_path) + '_/readme.txt'\n",
    "\n",
    "if not os.path.exists(os.path.dirname(model_path)):\n",
    "    os.makedirs(os.path.dirname(model_path))\n",
    "if not os.path.exists(os.path.dirname(figure_path)):\n",
    "    os.makedirs(os.path.dirname(figure_path))\n",
    "\n",
    "# Open the file in write mode ('w'). If the file does not exist, it will be created.\n",
    "with open(model_path, 'w') as file:\n",
    "    # Write the content to the file\n",
    "    file.write(content)\n",
    "\n",
    "with open(figure_path, 'w') as file:\n",
    "    # Write the content to the file\n",
    "    file.write(content)\n",
    "\n",
    "print(f\"File '{model_path}' and ' {figure_path} ' created and content written.\")\n",
    "\n",
    "save_folder_model = os.path.join(config.FILE_PATH, 'model/static/CNN', 'BS16', rowss, 'ver' + str(idx_save_path) + '_')\n",
    "save_folder_fig = os.path.join(config.FILE_PATH, 'figure', 'static', 'CNN', 'BS16' ,  rowss, 'ver' + str(idx_save_path) +'_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmse_LS_LI_val   = []\n",
    "nmse_LS_NN_val   = []\n",
    "nmse_LI_NN_val   = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snr = 0\n",
    "# [trainLabels, valLabels], [H_equal_train, H_linear_train, H_practical_train], [H_equal_val, H_linear_val, H_practical_val] = loader.load_data(outer_file_path, rows, fc, device, snr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SNR: 0/30\n",
      " Training for LS+LI\n",
      "SNR: 0/30, LS+LI, Epoch 1/20, Loss: 0.1778416256961781 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.052925012836402115\n",
      "SNR: 0/30, LS+LI, Epoch 2/20, Loss: 0.04446716262244208 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.03901259135454893\n",
      "SNR: 0/30, LS+LI, Epoch 3/20, Loss: 0.030733077505300212 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.029501148372549902\n",
      "SNR: 0/30, LS+LI, Epoch 4/20, Loss: 0.026693628110034867 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02896802652288567\n",
      "SNR: 0/30, LS+LI, Epoch 5/20, Loss: 0.02618829302792979 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02879929779605432\n",
      "SNR: 0/30, LS+LI, Epoch 6/20, Loss: 0.026261510738973006 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02849329982630231\n",
      "SNR: 0/30, LS+LI, Epoch 7/20, Loss: 0.026159493994444262 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02863548251546242\n",
      "SNR: 0/30, LS+LI, Epoch 8/20, Loss: 0.025952820833989008 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02843992975117131\n",
      "SNR: 0/30, LS+LI, Epoch 9/20, Loss: 0.0258996361855764 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02839745424518531\n",
      "SNR: 0/30, LS+LI, Epoch 10/20, Loss: 0.025802166314882247 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.028342107564888218\n",
      "SNR: 0/30, LS+LI, Epoch 11/20, Loss: 0.0258222732173149 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.028279123446819456\n",
      "SNR: 0/30, LS+LI, Epoch 12/20, Loss: 0.025809568905293248 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02854380900548263\n",
      "SNR: 0/30, LS+LI, Epoch 13/20, Loss: 0.025775358876819875 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02830711764876138\n",
      "SNR: 0/30, LS+LI, Epoch 14/20, Loss: 0.02565090279744635 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.028163107747042723\n",
      "SNR: 0/30, LS+LI, Epoch 15/20, Loss: 0.02564741334834591 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.028533343903043053\n",
      "SNR: 0/30, LS+LI, Epoch 16/20, Loss: 0.025575667516826543 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02917486353015358\n",
      "SNR: 0/30, LS+LI, Epoch 17/20, Loss: 0.025645594015111062 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.028066523288461296\n",
      "SNR: 0/30, LS+LI, Epoch 18/20, Loss: 0.0254829969083847 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.028324748685752802\n",
      "SNR: 0/30, LS+LI, Epoch 19/20, Loss: 0.025547649511027823 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02815651118924672\n",
      "SNR: 0/30, LS+LI, Epoch 20/20, Loss: 0.025450260048125718 \n",
      "SNR: 0/30, LS+LI, Val Loss: 0.02770108492536978\n",
      "LI+NN NMSE: 0.08898461610078812\n",
      "LS+LI NMSE: 0.08349758386611938\n",
      " Training for LS\n",
      "SNR: 0/30, LS, Epoch 1/20, Loss: 0.3173788593605507 \n",
      "SNR: 0/30, LS, Val Loss: 0.28308292546055536\n",
      "SNR: 0/30, LS, Epoch 2/20, Loss: 0.2360502093337303 \n",
      "SNR: 0/30, LS, Val Loss: 0.14009089497002689\n",
      "SNR: 0/30, LS, Epoch 3/20, Loss: 0.04764309787559648 \n",
      "SNR: 0/30, LS, Val Loss: 0.023546663028272716\n",
      "SNR: 0/30, LS, Epoch 4/20, Loss: 0.020597383585693532 \n",
      "SNR: 0/30, LS, Val Loss: 0.018818299540064552\n",
      "SNR: 0/30, LS, Epoch 5/20, Loss: 0.016962082309345174 \n",
      "SNR: 0/30, LS, Val Loss: 0.015465299048545685\n",
      "SNR: 0/30, LS, Epoch 6/20, Loss: 0.014247146483813955 \n",
      "SNR: 0/30, LS, Val Loss: 0.013190535151145676\n",
      "SNR: 0/30, LS, Epoch 7/20, Loss: 0.01232528408751065 \n",
      "SNR: 0/30, LS, Val Loss: 0.011456277687102556\n",
      "SNR: 0/30, LS, Epoch 8/20, Loss: 0.01083417144755638 \n",
      "SNR: 0/30, LS, Val Loss: 0.010131134566935625\n",
      "SNR: 0/30, LS, Epoch 9/20, Loss: 0.009529678432574107 \n",
      "SNR: 0/30, LS, Val Loss: 0.009569260037758133\n",
      "SNR: 0/30, LS, Epoch 10/20, Loss: 0.00860808576933693 \n",
      "SNR: 0/30, LS, Val Loss: 0.00819018068300052\n",
      "SNR: 0/30, LS, Epoch 11/20, Loss: 0.00786828214497587 \n",
      "SNR: 0/30, LS, Val Loss: 0.00791474190455946\n",
      "SNR: 0/30, LS, Epoch 12/20, Loss: 0.007224020851386148 \n",
      "SNR: 0/30, LS, Val Loss: 0.0073019541927020655\n",
      "SNR: 0/30, LS, Epoch 13/20, Loss: 0.0069357430302473 \n",
      "SNR: 0/30, LS, Val Loss: 0.006766686935655095\n",
      "SNR: 0/30, LS, Epoch 14/20, Loss: 0.006701974977934083 \n",
      "SNR: 0/30, LS, Val Loss: 0.006590787300162695\n",
      "SNR: 0/30, LS, Epoch 15/20, Loss: 0.006463908214057081 \n",
      "SNR: 0/30, LS, Val Loss: 0.006431012562560764\n",
      "SNR: 0/30, LS, Epoch 16/20, Loss: 0.006240340179850369 \n",
      "SNR: 0/30, LS, Val Loss: 0.006294389615174045\n",
      "SNR: 0/30, LS, Epoch 17/20, Loss: 0.006204468244145256 \n",
      "SNR: 0/30, LS, Val Loss: 0.00629206410270523\n",
      "SNR: 0/30, LS, Epoch 18/20, Loss: 0.006024820876286127 \n",
      "SNR: 0/30, LS, Val Loss: 0.006129105892879042\n",
      "SNR: 0/30, LS, Epoch 19/20, Loss: 0.005961166028756388 \n",
      "SNR: 0/30, LS, Val Loss: 0.006150057814507322\n",
      "SNR: 0/30, LS, Epoch 20/20, Loss: 0.005887769168069543 \n",
      "SNR: 0/30, LS, Val Loss: 0.005877363051033833\n",
      "LS+LI NMSE: 0.0184397604316473\n",
      " SNR: 5/30\n",
      " Training for LS+LI\n",
      "SNR: 5/30, LS+LI, Epoch 1/20, Loss: 0.15165341032538995 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.03593956848437136\n",
      "SNR: 5/30, LS+LI, Epoch 2/20, Loss: 0.02891249490122116 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.01977963660928336\n",
      "SNR: 5/30, LS+LI, Epoch 3/20, Loss: 0.0131879030986754 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.01151903477412733\n",
      "SNR: 5/30, LS+LI, Epoch 4/20, Loss: 0.010967354658384656 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.011066228870979765\n",
      "SNR: 5/30, LS+LI, Epoch 5/20, Loss: 0.010810733897351595 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.011181190779263323\n",
      "SNR: 5/30, LS+LI, Epoch 6/20, Loss: 0.010722723881562436 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.01091510186564516\n",
      "SNR: 5/30, LS+LI, Epoch 7/20, Loss: 0.010670162074072936 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010918418284166943\n",
      "SNR: 5/30, LS+LI, Epoch 8/20, Loss: 0.0106027290160053 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010834260102869435\n",
      "SNR: 5/30, LS+LI, Epoch 9/20, Loss: 0.010533036745348296 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010797709586437453\n",
      "SNR: 5/30, LS+LI, Epoch 10/20, Loss: 0.01054547392752368 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010777848137711937\n",
      "SNR: 5/30, LS+LI, Epoch 11/20, Loss: 0.010512041843084748 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.01075938796963204\n",
      "SNR: 5/30, LS+LI, Epoch 12/20, Loss: 0.010414414643851478 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010687567611140284\n",
      "SNR: 5/30, LS+LI, Epoch 13/20, Loss: 0.010402532440588572 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010692914778536016\n",
      "SNR: 5/30, LS+LI, Epoch 14/20, Loss: 0.010320664118152372 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010533080182292244\n",
      "SNR: 5/30, LS+LI, Epoch 15/20, Loss: 0.010334136809200742 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010967708721926267\n",
      "SNR: 5/30, LS+LI, Epoch 16/20, Loss: 0.010204318234003907 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010794154791669413\n",
      "SNR: 5/30, LS+LI, Epoch 17/20, Loss: 0.010163861659875269 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010399451055987314\n",
      "SNR: 5/30, LS+LI, Epoch 18/20, Loss: 0.010209418283667155 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010356176051903854\n",
      "SNR: 5/30, LS+LI, Epoch 19/20, Loss: 0.01013157699733626 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.010547070361843163\n",
      "SNR: 5/30, LS+LI, Epoch 20/20, Loss: 0.010101748243255843 \n",
      "SNR: 5/30, LS+LI, Val Loss: 0.01032945259728215\n",
      "LI+NN NMSE: 0.028979063034057617\n",
      "LS+LI NMSE: 0.026449566707015038\n",
      " Training for LS\n",
      "SNR: 5/30, LS, Epoch 1/20, Loss: 0.3104212815158589 \n",
      "SNR: 5/30, LS, Val Loss: 0.2528111257336356\n",
      "SNR: 5/30, LS, Epoch 2/20, Loss: 0.22063786848339922 \n",
      "SNR: 5/30, LS, Val Loss: 0.18195173415270718\n",
      "SNR: 5/30, LS, Epoch 3/20, Loss: 0.07512228490871399 \n",
      "SNR: 5/30, LS, Val Loss: 0.026765571483834225\n",
      "SNR: 5/30, LS, Epoch 4/20, Loss: 0.020251016966392133 \n",
      "SNR: 5/30, LS, Val Loss: 0.018014590213583273\n",
      "SNR: 5/30, LS, Epoch 5/20, Loss: 0.015496099113248462 \n",
      "SNR: 5/30, LS, Val Loss: 0.014370216183703054\n",
      "SNR: 5/30, LS, Epoch 6/20, Loss: 0.012306714121815424 \n",
      "SNR: 5/30, LS, Val Loss: 0.011076192286881533\n",
      "SNR: 5/30, LS, Epoch 7/20, Loss: 0.009382329082528 \n",
      "SNR: 5/30, LS, Val Loss: 0.008487341735004024\n",
      "SNR: 5/30, LS, Epoch 8/20, Loss: 0.007279955267602968 \n",
      "SNR: 5/30, LS, Val Loss: 0.0066359057937833395\n",
      "SNR: 5/30, LS, Epoch 9/20, Loss: 0.005932293366640806 \n",
      "SNR: 5/30, LS, Val Loss: 0.005525532474910671\n",
      "SNR: 5/30, LS, Epoch 10/20, Loss: 0.005106699344333868 \n",
      "SNR: 5/30, LS, Val Loss: 0.005001707849177447\n",
      "SNR: 5/30, LS, Epoch 11/20, Loss: 0.00462170104431205 \n",
      "SNR: 5/30, LS, Val Loss: 0.004696746885945851\n",
      "SNR: 5/30, LS, Epoch 12/20, Loss: 0.004331911004386669 \n",
      "SNR: 5/30, LS, Val Loss: 0.004213598652065478\n",
      "SNR: 5/30, LS, Epoch 13/20, Loss: 0.004137058509513736 \n",
      "SNR: 5/30, LS, Val Loss: 0.004008936865085905\n",
      "SNR: 5/30, LS, Epoch 14/20, Loss: 0.003971565796365571 \n",
      "SNR: 5/30, LS, Val Loss: 0.003836853990585289\n",
      "SNR: 5/30, LS, Epoch 15/20, Loss: 0.0038319844630720135 \n",
      "SNR: 5/30, LS, Val Loss: 0.003704917098564858\n",
      "SNR: 5/30, LS, Epoch 16/20, Loss: 0.003725627132658955 \n",
      "SNR: 5/30, LS, Val Loss: 0.0036087082381444898\n",
      "SNR: 5/30, LS, Epoch 17/20, Loss: 0.003627239943070467 \n",
      "SNR: 5/30, LS, Val Loss: 0.003500882843085988\n",
      "SNR: 5/30, LS, Epoch 18/20, Loss: 0.0035224172535763924 \n",
      "SNR: 5/30, LS, Val Loss: 0.003410712985152548\n",
      "SNR: 5/30, LS, Epoch 19/20, Loss: 0.0034374156777292144 \n",
      "SNR: 5/30, LS, Val Loss: 0.003513387975875627\n",
      "SNR: 5/30, LS, Epoch 20/20, Loss: 0.0033807884712248695 \n",
      "SNR: 5/30, LS, Val Loss: 0.003270099204118279\n",
      "LS+LI NMSE: 0.01019578892737627\n",
      " SNR: 10/30\n",
      " Training for LS+LI\n",
      "SNR: 10/30, LS+LI, Epoch 1/20, Loss: 0.14904280672889464 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.032288194888017395\n",
      "SNR: 10/30, LS+LI, Epoch 2/20, Loss: 0.022570566664081672 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.012710576369003817\n",
      "SNR: 10/30, LS+LI, Epoch 3/20, Loss: 0.007193042318672375 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.005232092755084688\n",
      "SNR: 10/30, LS+LI, Epoch 4/20, Loss: 0.004619953274012132 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004775630744089457\n",
      "SNR: 10/30, LS+LI, Epoch 5/20, Loss: 0.0044048985569239705 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.0046252647457136345\n",
      "SNR: 10/30, LS+LI, Epoch 6/20, Loss: 0.004276648739103748 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004549889603037049\n",
      "SNR: 10/30, LS+LI, Epoch 7/20, Loss: 0.004207164617688503 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.00443454312202944\n",
      "SNR: 10/30, LS+LI, Epoch 8/20, Loss: 0.004160761689702266 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.0043633492155508566\n",
      "SNR: 10/30, LS+LI, Epoch 9/20, Loss: 0.004078438077771733 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.0043681690956211905\n",
      "SNR: 10/30, LS+LI, Epoch 10/20, Loss: 0.004014217660242562 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.00424816753630611\n",
      "SNR: 10/30, LS+LI, Epoch 11/20, Loss: 0.003966580635088301 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004177404652264985\n",
      "SNR: 10/30, LS+LI, Epoch 12/20, Loss: 0.0038984922372714377 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004188004669479348\n",
      "SNR: 10/30, LS+LI, Epoch 13/20, Loss: 0.0038793391621195126 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004090310169637881\n",
      "SNR: 10/30, LS+LI, Epoch 14/20, Loss: 0.003880149045915798 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.0040467846926979046\n",
      "SNR: 10/30, LS+LI, Epoch 15/20, Loss: 0.003795436862314683 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004053447586060925\n",
      "SNR: 10/30, LS+LI, Epoch 16/20, Loss: 0.0037749809399788637 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.003979458209042522\n",
      "SNR: 10/30, LS+LI, Epoch 17/20, Loss: 0.00376958227689313 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.0040476106911559\n",
      "SNR: 10/30, LS+LI, Epoch 18/20, Loss: 0.003711643871082383 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.003938842977566475\n",
      "SNR: 10/30, LS+LI, Epoch 19/20, Loss: 0.0036688774707727134 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.004025830622677776\n",
      "SNR: 10/30, LS+LI, Epoch 20/20, Loss: 0.0037038055489511165 \n",
      "SNR: 10/30, LS+LI, Val Loss: 0.003923502390865575\n",
      "LI+NN NMSE: 0.011942822486162186\n",
      "LS+LI NMSE: 0.008273791521787643\n",
      " Training for LS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thien/Hprediction/one_shot_Hest_cleanver/Torch_code/helper/plotfig.py:30: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(10, 5))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR: 10/30, LS, Epoch 1/20, Loss: 0.29602479787413466 \n",
      "SNR: 10/30, LS, Val Loss: 0.24016361074014145\n",
      "SNR: 10/30, LS, Epoch 2/20, Loss: 0.20509631957772168 \n",
      "SNR: 10/30, LS, Val Loss: 0.16733070666139777\n",
      "SNR: 10/30, LS, Epoch 3/20, Loss: 0.06737579854725059 \n",
      "SNR: 10/30, LS, Val Loss: 0.02019594796001911\n",
      "SNR: 10/30, LS, Epoch 4/20, Loss: 0.017188243784530217 \n",
      "SNR: 10/30, LS, Val Loss: 0.014854734805835918\n",
      "SNR: 10/30, LS, Epoch 5/20, Loss: 0.012929703033161025 \n",
      "SNR: 10/30, LS, Val Loss: 0.011276316956024279\n",
      "SNR: 10/30, LS, Epoch 6/20, Loss: 0.010042039861599373 \n",
      "SNR: 10/30, LS, Val Loss: 0.008925497320226648\n",
      "SNR: 10/30, LS, Epoch 7/20, Loss: 0.007911358654044222 \n",
      "SNR: 10/30, LS, Val Loss: 0.0070801237174733115\n",
      "SNR: 10/30, LS, Epoch 8/20, Loss: 0.006226021972441569 \n",
      "SNR: 10/30, LS, Val Loss: 0.005560685732317242\n",
      "SNR: 10/30, LS, Epoch 9/20, Loss: 0.004897644096270724 \n",
      "SNR: 10/30, LS, Val Loss: 0.004407858539542014\n",
      "SNR: 10/30, LS, Epoch 10/20, Loss: 0.0039905294488985525 \n",
      "SNR: 10/30, LS, Val Loss: 0.003691926852545955\n",
      "SNR: 10/30, LS, Epoch 11/20, Loss: 0.0034285866273675372 \n",
      "SNR: 10/30, LS, Val Loss: 0.003242152965288948\n",
      "SNR: 10/30, LS, Epoch 12/20, Loss: 0.003101962446614147 \n",
      "SNR: 10/30, LS, Val Loss: 0.0029547950252890587\n",
      "SNR: 10/30, LS, Epoch 13/20, Loss: 0.002858130430510311 \n",
      "SNR: 10/30, LS, Val Loss: 0.002795667980204929\n",
      "SNR: 10/30, LS, Epoch 14/20, Loss: 0.0026856136080527374 \n",
      "SNR: 10/30, LS, Val Loss: 0.0026818211825395174\n",
      "SNR: 10/30, LS, Epoch 15/20, Loss: 0.002558820567575695 \n",
      "SNR: 10/30, LS, Val Loss: 0.002604251357049427\n",
      "SNR: 10/30, LS, Epoch 16/20, Loss: 0.0024952652828962823 \n",
      "SNR: 10/30, LS, Val Loss: 0.0024381592679260807\n",
      "SNR: 10/30, LS, Epoch 17/20, Loss: 0.0023701885007159393 \n",
      "SNR: 10/30, LS, Val Loss: 0.002396974437446757\n",
      "SNR: 10/30, LS, Epoch 18/20, Loss: 0.00233232521527815 \n",
      "SNR: 10/30, LS, Val Loss: 0.002333862595895136\n",
      "SNR: 10/30, LS, Epoch 19/20, Loss: 0.0022294555403573743 \n",
      "SNR: 10/30, LS, Val Loss: 0.0023234081666239285\n",
      "SNR: 10/30, LS, Epoch 20/20, Loss: 0.0021741176667070925 \n",
      "SNR: 10/30, LS, Val Loss: 0.002212931026323614\n",
      "LS+LI NMSE: 0.006801434326916933\n",
      " SNR: 15/30\n",
      " Training for LS+LI\n",
      "SNR: 15/30, LS+LI, Epoch 1/20, Loss: 0.1339230262241218 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.026024533351036636\n",
      "SNR: 15/30, LS+LI, Epoch 2/20, Loss: 0.01590343459433508 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.008509363852102648\n",
      "SNR: 15/30, LS+LI, Epoch 3/20, Loss: 0.003938577851269741 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0024676328440281477\n",
      "SNR: 15/30, LS+LI, Epoch 4/20, Loss: 0.0023287613652808983 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.002330727959898385\n",
      "SNR: 15/30, LS+LI, Epoch 5/20, Loss: 0.0022005402303008404 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0022162660025060177\n",
      "SNR: 15/30, LS+LI, Epoch 6/20, Loss: 0.0021126461452208893 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0021456178563477642\n",
      "SNR: 15/30, LS+LI, Epoch 7/20, Loss: 0.0020279086602386087 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.002185109759342264\n",
      "SNR: 15/30, LS+LI, Epoch 8/20, Loss: 0.0019669168490678247 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.00201920899880034\n",
      "SNR: 15/30, LS+LI, Epoch 9/20, Loss: 0.0018783129567583633 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0019211332176134667\n",
      "SNR: 15/30, LS+LI, Epoch 10/20, Loss: 0.0018442464514559697 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0018530614695257761\n",
      "SNR: 15/30, LS+LI, Epoch 11/20, Loss: 0.0017727130440572754 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0017831235097467222\n",
      "SNR: 15/30, LS+LI, Epoch 12/20, Loss: 0.001729567777527886 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0017286675654097714\n",
      "SNR: 15/30, LS+LI, Epoch 13/20, Loss: 0.0016732070761527 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.001692434259563346\n",
      "SNR: 15/30, LS+LI, Epoch 14/20, Loss: 0.0016368382994821945 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0017179986779493365\n",
      "SNR: 15/30, LS+LI, Epoch 15/20, Loss: 0.0016064004496086476 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0016544567981988869\n",
      "SNR: 15/30, LS+LI, Epoch 16/20, Loss: 0.0015681432055845467 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0015704332512210715\n",
      "SNR: 15/30, LS+LI, Epoch 17/20, Loss: 0.0015428798715854714 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0015819413555701349\n",
      "SNR: 15/30, LS+LI, Epoch 18/20, Loss: 0.0015318573792302591 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0015403235765089366\n",
      "SNR: 15/30, LS+LI, Epoch 19/20, Loss: 0.0015399492979266268 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0015216931245628405\n",
      "SNR: 15/30, LS+LI, Epoch 20/20, Loss: 0.0014833511170784964 \n",
      "SNR: 15/30, LS+LI, Val Loss: 0.0016025869408622384\n",
      "LI+NN NMSE: 0.005067869555205107\n",
      "LS+LI NMSE: 0.002629202790558338\n",
      " Training for LS\n",
      "SNR: 15/30, LS, Epoch 1/20, Loss: 0.2958377636102743 \n",
      "SNR: 15/30, LS, Val Loss: 0.22648237442428415\n",
      "SNR: 15/30, LS, Epoch 2/20, Loss: 0.19661830313677012 \n",
      "SNR: 15/30, LS, Val Loss: 0.1560833945193074\n",
      "SNR: 15/30, LS, Epoch 3/20, Loss: 0.06964264862066091 \n",
      "SNR: 15/30, LS, Val Loss: 0.02307307610118931\n",
      "SNR: 15/30, LS, Epoch 4/20, Loss: 0.01730343860812312 \n",
      "SNR: 15/30, LS, Val Loss: 0.013401125515387817\n",
      "SNR: 15/30, LS, Epoch 5/20, Loss: 0.011542373867488877 \n",
      "SNR: 15/30, LS, Val Loss: 0.009672763461077755\n",
      "SNR: 15/30, LS, Epoch 6/20, Loss: 0.00846954717484931 \n",
      "SNR: 15/30, LS, Val Loss: 0.0070606655525890264\n",
      "SNR: 15/30, LS, Epoch 7/20, Loss: 0.00603765070568337 \n",
      "SNR: 15/30, LS, Val Loss: 0.0049370231360874395\n",
      "SNR: 15/30, LS, Epoch 8/20, Loss: 0.004238633614921466 \n",
      "SNR: 15/30, LS, Val Loss: 0.003748564525846053\n",
      "SNR: 15/30, LS, Epoch 9/20, Loss: 0.003144954300084866 \n",
      "SNR: 15/30, LS, Val Loss: 0.002765226844613525\n",
      "SNR: 15/30, LS, Epoch 10/20, Loss: 0.0026271209414496064 \n",
      "SNR: 15/30, LS, Val Loss: 0.002438234514556825\n",
      "SNR: 15/30, LS, Epoch 11/20, Loss: 0.0023141086919146567 \n",
      "SNR: 15/30, LS, Val Loss: 0.0022490334443070674\n",
      "SNR: 15/30, LS, Epoch 12/20, Loss: 0.002114313444106428 \n",
      "SNR: 15/30, LS, Val Loss: 0.002076895526525649\n",
      "SNR: 15/30, LS, Epoch 13/20, Loss: 0.0019802641472779214 \n",
      "SNR: 15/30, LS, Val Loss: 0.0019471720443107188\n",
      "SNR: 15/30, LS, Epoch 14/20, Loss: 0.0018675056797842126 \n",
      "SNR: 15/30, LS, Val Loss: 0.0019388143237764862\n",
      "SNR: 15/30, LS, Epoch 15/20, Loss: 0.001785068510121904 \n",
      "SNR: 15/30, LS, Val Loss: 0.0017544108294797215\n",
      "SNR: 15/30, LS, Epoch 16/20, Loss: 0.0016924865844356286 \n",
      "SNR: 15/30, LS, Val Loss: 0.00180095722052184\n",
      "SNR: 15/30, LS, Epoch 17/20, Loss: 0.001629679458389102 \n",
      "SNR: 15/30, LS, Val Loss: 0.001626914605201984\n",
      "SNR: 15/30, LS, Epoch 18/20, Loss: 0.0015746771363368214 \n",
      "SNR: 15/30, LS, Val Loss: 0.0016315995017066598\n",
      "SNR: 15/30, LS, Epoch 19/20, Loss: 0.0015033118868095065 \n",
      "SNR: 15/30, LS, Val Loss: 0.0015558127038688822\n",
      "SNR: 15/30, LS, Epoch 20/20, Loss: 0.0014847543342363869 \n",
      "SNR: 15/30, LS, Val Loss: 0.0014943224907090719\n",
      "LS+LI NMSE: 0.004505625460296869\n",
      " SNR: 20/30\n",
      " Training for LS+LI\n",
      "SNR: 20/30, LS+LI, Epoch 1/20, Loss: 0.15322951813293403 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.02610717908563939\n",
      "SNR: 20/30, LS+LI, Epoch 2/20, Loss: 0.016459892820675184 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.00786284501241012\n",
      "SNR: 20/30, LS+LI, Epoch 3/20, Loss: 0.003601971664465964 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0018295086073604498\n",
      "SNR: 20/30, LS+LI, Epoch 4/20, Loss: 0.0015574681963825728 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.001430442263054746\n",
      "SNR: 20/30, LS+LI, Epoch 5/20, Loss: 0.0013572423706009855 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.001320959111167626\n",
      "SNR: 20/30, LS+LI, Epoch 6/20, Loss: 0.0012580267293107977 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0012373825487553734\n",
      "SNR: 20/30, LS+LI, Epoch 7/20, Loss: 0.0011598114793995656 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0011245692492759024\n",
      "SNR: 20/30, LS+LI, Epoch 8/20, Loss: 0.001070082383257402 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0010217500146774744\n",
      "SNR: 20/30, LS+LI, Epoch 9/20, Loss: 0.0009839861568464183 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0009558083481599831\n",
      "SNR: 20/30, LS+LI, Epoch 10/20, Loss: 0.0008931675665946894 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0008785476820776239\n",
      "SNR: 20/30, LS+LI, Epoch 11/20, Loss: 0.0008472065765513541 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0008658884180476889\n",
      "SNR: 20/30, LS+LI, Epoch 12/20, Loss: 0.0007817432596375299 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0007722767226038162\n",
      "SNR: 20/30, LS+LI, Epoch 13/20, Loss: 0.0007372180894721126 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0007320560623404824\n",
      "SNR: 20/30, LS+LI, Epoch 14/20, Loss: 0.0007059732155920396 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0007411813223205337\n",
      "SNR: 20/30, LS+LI, Epoch 15/20, Loss: 0.0006823910723120884 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0006775818804850463\n",
      "SNR: 20/30, LS+LI, Epoch 16/20, Loss: 0.0006922780056240384 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0006778009106214581\n",
      "SNR: 20/30, LS+LI, Epoch 17/20, Loss: 0.0006463968705691852 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0006534387899922546\n",
      "SNR: 20/30, LS+LI, Epoch 18/20, Loss: 0.0006228549489521335 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0006634099342839115\n",
      "SNR: 20/30, LS+LI, Epoch 19/20, Loss: 0.0006355942085826617 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.00066627062253908\n",
      "SNR: 20/30, LS+LI, Epoch 20/20, Loss: 0.0006044157931302285 \n",
      "SNR: 20/30, LS+LI, Val Loss: 0.0006011986300687898\n",
      "LI+NN NMSE: 0.001976205036044121\n",
      "LS+LI NMSE: 0.0008332210127264261\n",
      " Training for LS\n",
      "SNR: 20/30, LS, Epoch 1/20, Loss: 0.2964118476523909 \n",
      "SNR: 20/30, LS, Val Loss: 0.21702755987644196\n",
      "SNR: 20/30, LS, Epoch 2/20, Loss: 0.19062256258587504 \n",
      "SNR: 20/30, LS, Val Loss: 0.1467098624191501\n",
      "SNR: 20/30, LS, Epoch 3/20, Loss: 0.06866358251982303 \n",
      "SNR: 20/30, LS, Val Loss: 0.020584368993612854\n",
      "SNR: 20/30, LS, Epoch 4/20, Loss: 0.017008403527312154 \n",
      "SNR: 20/30, LS, Val Loss: 0.014234276530756191\n",
      "SNR: 20/30, LS, Epoch 5/20, Loss: 0.012615603784662347 \n",
      "SNR: 20/30, LS, Val Loss: 0.010967821983451193\n",
      "SNR: 20/30, LS, Epoch 6/20, Loss: 0.009465770934015339 \n",
      "SNR: 20/30, LS, Val Loss: 0.007888838119635528\n",
      "SNR: 20/30, LS, Epoch 7/20, Loss: 0.006655023111095435 \n",
      "SNR: 20/30, LS, Val Loss: 0.005438985133712942\n",
      "SNR: 20/30, LS, Epoch 8/20, Loss: 0.004580465424264413 \n",
      "SNR: 20/30, LS, Val Loss: 0.0037696765608746896\n",
      "SNR: 20/30, LS, Epoch 9/20, Loss: 0.0032120947103384274 \n",
      "SNR: 20/30, LS, Val Loss: 0.0027288218155841937\n",
      "SNR: 20/30, LS, Epoch 10/20, Loss: 0.002507620407836945 \n",
      "SNR: 20/30, LS, Val Loss: 0.002304771438833665\n",
      "SNR: 20/30, LS, Epoch 11/20, Loss: 0.002199244902057703 \n",
      "SNR: 20/30, LS, Val Loss: 0.0021275498809038913\n",
      "SNR: 20/30, LS, Epoch 12/20, Loss: 0.0019960850220021985 \n",
      "SNR: 20/30, LS, Val Loss: 0.0018988091085868125\n",
      "SNR: 20/30, LS, Epoch 13/20, Loss: 0.0018370726029388607 \n",
      "SNR: 20/30, LS, Val Loss: 0.0017734003539027815\n",
      "SNR: 20/30, LS, Epoch 14/20, Loss: 0.0017295802745773176 \n",
      "SNR: 20/30, LS, Val Loss: 0.0017564276614311066\n",
      "SNR: 20/30, LS, Epoch 15/20, Loss: 0.001638466961340719 \n",
      "SNR: 20/30, LS, Val Loss: 0.0016525178639726205\n",
      "SNR: 20/30, LS, Epoch 16/20, Loss: 0.0015467864496438483 \n",
      "SNR: 20/30, LS, Val Loss: 0.001503304170910269\n",
      "SNR: 20/30, LS, Epoch 17/20, Loss: 0.0014737306878804554 \n",
      "SNR: 20/30, LS, Val Loss: 0.001412588415074755\n",
      "SNR: 20/30, LS, Epoch 18/20, Loss: 0.0014091200651438517 \n",
      "SNR: 20/30, LS, Val Loss: 0.001354182811072943\n",
      "SNR: 20/30, LS, Epoch 19/20, Loss: 0.0013394893132161003 \n",
      "SNR: 20/30, LS, Val Loss: 0.0013272744756911627\n",
      "SNR: 20/30, LS, Epoch 20/20, Loss: 0.0012970151860932928 \n",
      "SNR: 20/30, LS, Val Loss: 0.0013159212328239598\n",
      "LS+LI NMSE: 0.004238354507833719\n",
      " SNR: 25/30\n",
      " Training for LS+LI\n",
      "SNR: 25/30, LS+LI, Epoch 1/20, Loss: 0.14819219226537403 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.02512844279408455\n",
      "SNR: 25/30, LS+LI, Epoch 2/20, Loss: 0.0130992979257974 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.003753150400536304\n",
      "SNR: 25/30, LS+LI, Epoch 3/20, Loss: 0.0018524899821576858 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0013174605555832386\n",
      "SNR: 25/30, LS+LI, Epoch 4/20, Loss: 0.0011572796302484782 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.001093067914057015\n",
      "SNR: 25/30, LS+LI, Epoch 5/20, Loss: 0.001005288983727697 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0009643254716965286\n",
      "SNR: 25/30, LS+LI, Epoch 6/20, Loss: 0.0008775013990342876 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0008194936698683622\n",
      "SNR: 25/30, LS+LI, Epoch 7/20, Loss: 0.0007473626123270193 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0006895001067525961\n",
      "SNR: 25/30, LS+LI, Epoch 8/20, Loss: 0.0006189797700350281 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0005613912441979417\n",
      "SNR: 25/30, LS+LI, Epoch 9/20, Loss: 0.0005117183005326255 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0004781411870763722\n",
      "SNR: 25/30, LS+LI, Epoch 10/20, Loss: 0.0004414171995703391 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0004073273157700896\n",
      "SNR: 25/30, LS+LI, Epoch 11/20, Loss: 0.0003953411248593786 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0003614523514195091\n",
      "SNR: 25/30, LS+LI, Epoch 12/20, Loss: 0.0003499342964523537 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.00032322028048590505\n",
      "SNR: 25/30, LS+LI, Epoch 13/20, Loss: 0.0003291325683253384 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.00030396196763666177\n",
      "SNR: 25/30, LS+LI, Epoch 14/20, Loss: 0.0003048990553462889 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0003054066738431257\n",
      "SNR: 25/30, LS+LI, Epoch 15/20, Loss: 0.0002848690607162669 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0002669949449641122\n",
      "SNR: 25/30, LS+LI, Epoch 16/20, Loss: 0.00027717124130292577 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.00027892771322513\n",
      "SNR: 25/30, LS+LI, Epoch 17/20, Loss: 0.0002641687145135853 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.00025449826816012234\n",
      "SNR: 25/30, LS+LI, Epoch 18/20, Loss: 0.0002556623501647643 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0002376971692270176\n",
      "SNR: 25/30, LS+LI, Epoch 19/20, Loss: 0.00024693369118193554 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.00022710961109789258\n",
      "SNR: 25/30, LS+LI, Epoch 20/20, Loss: 0.00023450685093425713 \n",
      "SNR: 25/30, LS+LI, Val Loss: 0.0002281370775149712\n",
      "LI+NN NMSE: 0.0006819934933446348\n",
      "LS+LI NMSE: 0.0002648405497893691\n",
      " Training for LS\n",
      "SNR: 25/30, LS, Epoch 1/20, Loss: 0.2971295896311139 \n",
      "SNR: 25/30, LS, Val Loss: 0.22643222930756482\n",
      "SNR: 25/30, LS, Epoch 2/20, Loss: 0.17947438675477062 \n",
      "SNR: 25/30, LS, Val Loss: 0.12791994654319502\n",
      "SNR: 25/30, LS, Epoch 3/20, Loss: 0.04252100943284499 \n",
      "SNR: 25/30, LS, Val Loss: 0.018428795208985157\n",
      "SNR: 25/30, LS, Epoch 4/20, Loss: 0.014480165861070503 \n",
      "SNR: 25/30, LS, Val Loss: 0.012630162887613882\n",
      "SNR: 25/30, LS, Epoch 5/20, Loss: 0.010447644579852389 \n",
      "SNR: 25/30, LS, Val Loss: 0.009376825426112522\n",
      "SNR: 25/30, LS, Epoch 6/20, Loss: 0.007663409566766647 \n",
      "SNR: 25/30, LS, Val Loss: 0.00685171492990445\n",
      "SNR: 25/30, LS, Epoch 7/20, Loss: 0.005464971395815874 \n",
      "SNR: 25/30, LS, Val Loss: 0.004772658227011561\n",
      "SNR: 25/30, LS, Epoch 8/20, Loss: 0.0037884742947468576 \n",
      "SNR: 25/30, LS, Val Loss: 0.003582857496274466\n",
      "SNR: 25/30, LS, Epoch 9/20, Loss: 0.0028332999615042014 \n",
      "SNR: 25/30, LS, Val Loss: 0.0027896563353186302\n",
      "SNR: 25/30, LS, Epoch 10/20, Loss: 0.002365776401982361 \n",
      "SNR: 25/30, LS, Val Loss: 0.002358770939860154\n",
      "SNR: 25/30, LS, Epoch 11/20, Loss: 0.0020652515942306627 \n",
      "SNR: 25/30, LS, Val Loss: 0.002134908858517354\n",
      "SNR: 25/30, LS, Epoch 12/20, Loss: 0.0018688196852413375 \n",
      "SNR: 25/30, LS, Val Loss: 0.0019501017628830266\n",
      "SNR: 25/30, LS, Epoch 13/20, Loss: 0.0017111313801399584 \n",
      "SNR: 25/30, LS, Val Loss: 0.0017443258336491206\n",
      "SNR: 25/30, LS, Epoch 14/20, Loss: 0.0015964430545694953 \n",
      "SNR: 25/30, LS, Val Loss: 0.0016741990376348522\n",
      "SNR: 25/30, LS, Epoch 15/20, Loss: 0.0014819238567724824 \n",
      "SNR: 25/30, LS, Val Loss: 0.0015167385519651527\n",
      "SNR: 25/30, LS, Epoch 16/20, Loss: 0.0013972666570754332 \n",
      "SNR: 25/30, LS, Val Loss: 0.0014246787918223577\n",
      "SNR: 25/30, LS, Epoch 17/20, Loss: 0.001321687839240334 \n",
      "SNR: 25/30, LS, Val Loss: 0.0013567356297492304\n",
      "SNR: 25/30, LS, Epoch 18/20, Loss: 0.0012458515455702666 \n",
      "SNR: 25/30, LS, Val Loss: 0.0012963576062413101\n",
      "SNR: 25/30, LS, Epoch 19/20, Loss: 0.001197241643324581 \n",
      "SNR: 25/30, LS, Val Loss: 0.0012078879623335194\n",
      "SNR: 25/30, LS, Epoch 20/20, Loss: 0.0011242293731748062 \n",
      "SNR: 25/30, LS, Val Loss: 0.001142513430254026\n",
      "LS+LI NMSE: 0.0037021683529019356\n",
      " SNR: 30/30\n",
      " Training for LS+LI\n",
      "SNR: 30/30, LS+LI, Epoch 1/20, Loss: 0.1336093426812007 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.02611458868804303\n",
      "SNR: 30/30, LS+LI, Epoch 2/20, Loss: 0.013637432487421604 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0029988534409891477\n",
      "SNR: 30/30, LS+LI, Epoch 3/20, Loss: 0.0014613865154557103 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.001025738750203428\n",
      "SNR: 30/30, LS+LI, Epoch 4/20, Loss: 0.000914020947421641 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0008051537072540007\n",
      "SNR: 30/30, LS+LI, Epoch 5/20, Loss: 0.0007326854724534462 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0006460837744684382\n",
      "SNR: 30/30, LS+LI, Epoch 6/20, Loss: 0.0005826359806950529 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0005053941911848432\n",
      "SNR: 30/30, LS+LI, Epoch 7/20, Loss: 0.00045693175715589245 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00039903778815642\n",
      "SNR: 30/30, LS+LI, Epoch 8/20, Loss: 0.0003698622155731522 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00033109664615370673\n",
      "SNR: 30/30, LS+LI, Epoch 9/20, Loss: 0.00030612420088673814 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0002756126963173632\n",
      "SNR: 30/30, LS+LI, Epoch 10/20, Loss: 0.0002622235909701314 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00023839273026996207\n",
      "SNR: 30/30, LS+LI, Epoch 11/20, Loss: 0.0002290534607180784 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00021117403759324753\n",
      "SNR: 30/30, LS+LI, Epoch 12/20, Loss: 0.0002068386373054615 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00019298898802265865\n",
      "SNR: 30/30, LS+LI, Epoch 13/20, Loss: 0.00018949310766021331 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00018707320057067344\n",
      "SNR: 30/30, LS+LI, Epoch 14/20, Loss: 0.00017529034059492025 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00016792970762418753\n",
      "SNR: 30/30, LS+LI, Epoch 15/20, Loss: 0.00016263795496765956 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0001546033480818468\n",
      "SNR: 30/30, LS+LI, Epoch 16/20, Loss: 0.00015086067312865433 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00014477658359365623\n",
      "SNR: 30/30, LS+LI, Epoch 17/20, Loss: 0.00014255244583859337 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.0001448525869239926\n",
      "SNR: 30/30, LS+LI, Epoch 18/20, Loss: 0.00013803964078718754 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00013731239331801507\n",
      "SNR: 30/30, LS+LI, Epoch 19/20, Loss: 0.00012836610196330204 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00013787250811054201\n",
      "SNR: 30/30, LS+LI, Epoch 20/20, Loss: 0.00012709613115158538 \n",
      "SNR: 30/30, LS+LI, Val Loss: 0.00014500704855890945\n",
      "LI+NN NMSE: 0.00042966933688148856\n",
      "LS+LI NMSE: 8.486135629937053e-05\n",
      " Training for LS\n",
      "SNR: 30/30, LS, Epoch 1/20, Loss: 0.2928352277985839 \n",
      "SNR: 30/30, LS, Val Loss: 0.21307659758762879\n",
      "SNR: 30/30, LS, Epoch 2/20, Loss: 0.19357225186256474 \n",
      "SNR: 30/30, LS, Val Loss: 0.15695277296684004\n",
      "SNR: 30/30, LS, Epoch 3/20, Loss: 0.08549594679890676 \n",
      "SNR: 30/30, LS, Val Loss: 0.0210902996530587\n",
      "SNR: 30/30, LS, Epoch 4/20, Loss: 0.017346401520203367 \n",
      "SNR: 30/30, LS, Val Loss: 0.014504271686415781\n",
      "SNR: 30/30, LS, Epoch 5/20, Loss: 0.01309079710978928 \n",
      "SNR: 30/30, LS, Val Loss: 0.011503982019018043\n",
      "SNR: 30/30, LS, Epoch 6/20, Loss: 0.010407265308204778 \n",
      "SNR: 30/30, LS, Val Loss: 0.00908666971901601\n",
      "SNR: 30/30, LS, Epoch 7/20, Loss: 0.008156827753842917 \n",
      "SNR: 30/30, LS, Val Loss: 0.007076185598800128\n",
      "SNR: 30/30, LS, Epoch 8/20, Loss: 0.006262408848310453 \n",
      "SNR: 30/30, LS, Val Loss: 0.0052969674579799175\n",
      "SNR: 30/30, LS, Epoch 9/20, Loss: 0.004649379614166655 \n",
      "SNR: 30/30, LS, Val Loss: 0.003835089017891071\n",
      "SNR: 30/30, LS, Epoch 10/20, Loss: 0.003331216948301813 \n",
      "SNR: 30/30, LS, Val Loss: 0.002689250183969059\n",
      "SNR: 30/30, LS, Epoch 11/20, Loss: 0.00236145596542439 \n",
      "SNR: 30/30, LS, Val Loss: 0.002116263875822452\n",
      "SNR: 30/30, LS, Epoch 12/20, Loss: 0.0018759543153316562 \n",
      "SNR: 30/30, LS, Val Loss: 0.0017014896412464705\n",
      "SNR: 30/30, LS, Epoch 13/20, Loss: 0.0016566899883950692 \n",
      "SNR: 30/30, LS, Val Loss: 0.0015072455068796196\n",
      "SNR: 30/30, LS, Epoch 14/20, Loss: 0.0015182240533122664 \n",
      "SNR: 30/30, LS, Val Loss: 0.0013973849603313613\n",
      "SNR: 30/30, LS, Epoch 15/20, Loss: 0.0014070807522985824 \n",
      "SNR: 30/30, LS, Val Loss: 0.0013208419232713904\n",
      "SNR: 30/30, LS, Epoch 16/20, Loss: 0.0013307462139109279 \n",
      "SNR: 30/30, LS, Val Loss: 0.001222233917691152\n",
      "SNR: 30/30, LS, Epoch 17/20, Loss: 0.0012391777187964858 \n",
      "SNR: 30/30, LS, Val Loss: 0.0011740256450138986\n",
      "SNR: 30/30, LS, Epoch 18/20, Loss: 0.0011717283006535497 \n",
      "SNR: 30/30, LS, Val Loss: 0.0011890289156740023\n",
      "SNR: 30/30, LS, Epoch 19/20, Loss: 0.0011359676884536004 \n",
      "SNR: 30/30, LS, Val Loss: 0.0010327421372163701\n",
      "SNR: 30/30, LS, Epoch 20/20, Loss: 0.0010728932251114138 \n",
      "SNR: 30/30, LS, Val Loss: 0.0010020428109617735\n",
      "LS+LI NMSE: 0.003147732000797987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for snr in SNR:\n",
    "    print(f\" SNR: {snr}/{SNR[-1]}\")\n",
    "\n",
    "    [trainLabels, valLabels], [H_equal_train, H_linear_train, H_practical_train], [H_equal_val, H_linear_val, H_practical_val] = loader.load_data(outer_file_path, rows, fc, device, snr)\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 1. When input is H_linear (after LS+LI)\n",
    "    print(f\" Training for LS+LI\")\n",
    "    # [samples, 2, 612, 14]\n",
    "    train_loader, trainLabel_min, trainLabel_max = loader.genLoader(H_linear_train, trainLabels, BATCH_SIZE, device, 'train', True, norm_approach)\n",
    "    val_loader,     valLabel_min,   valLabel_max = loader.genLoader(H_linear_val,     valLabels, BATCH_SIZE, device, 'valid', False, norm_approach)\n",
    "        # no shuffle in validation so valLabel_min and valLable_max remain the min and max arrays\n",
    "        # of valLabels\n",
    "        \n",
    "    # model\n",
    "    model = utils.CNN_Est().to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 1.5 Training loop\n",
    "    train_loss =[]\n",
    "    val_loss = []\n",
    "    H_NN_val = torch.empty_like(valLabels) # [nVal, 2, 612, 14]\n",
    "    min_H_true = []\n",
    "    max_H_true = []\n",
    "    num_epochs = NUM_EPOCHS\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        if (epoch == num_epochs-1):\n",
    "            i = 0\n",
    "        for inputs, targets, targets_min, targets_max in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_loss.append(avg_train_loss)\n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS+LI, Epoch {epoch+1}/{num_epochs}, Loss: {avg_train_loss} \")\n",
    "        \n",
    "        # Validation \n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets, val_targetsMin, val_targetsMax in val_loader:\n",
    "                val_inputs_real = val_inputs[:,0,:,:].unsqueeze(1)\n",
    "                val_inputs_imag = val_inputs[:,1,:,:].unsqueeze(1)\n",
    "                val_targets_real = val_targets[:,0,:,:].unsqueeze(1)\n",
    "                val_targets_imag = val_targets[:,1,:,:].unsqueeze(1)\n",
    "                \n",
    "                val_outputs_real = model(val_inputs_real)\n",
    "                val_loss_real = criterion(val_outputs_real, val_targets_real)\n",
    "                running_val_loss += val_loss_real.item()\n",
    "                \n",
    "                val_outputs_imag = model(val_inputs_imag)\n",
    "                val_loss_imag = criterion(val_outputs_imag, val_targets_imag)\n",
    "                running_val_loss += val_loss_imag.item()\n",
    "                \n",
    "                if (epoch == num_epochs-1): # the results after the last training \n",
    "                    H_NN_val[i:i+val_outputs_real.size(0),0,:,:].unsqueeze(1).copy_(val_outputs_real)\n",
    "                    H_NN_val[i:i+val_outputs_imag.size(0),1,:,:].unsqueeze(1).copy_(val_outputs_imag)\n",
    "                    \n",
    "                    i = i+val_outputs_imag.size(0)       \n",
    "                    \n",
    "                \n",
    "        avg_val_loss = running_val_loss / (len(val_loader)*2)\n",
    "        val_loss.append(avg_val_loss)    \n",
    "                \n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS+LI, Val Loss: {avg_val_loss}\")\n",
    "\n",
    "\n",
    "    save_folder = os.path.join(save_folder_model, str(snr)+'dB')\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    index_save = loader.find_incremental_filename(save_folder, 'CNN_', '_variable')\n",
    "\n",
    "    model_save_path = os.path.join(save_folder,  'CNN_' +str(index_save)+'_LS_LI_CNN_model.pth')\n",
    "    variable_save_path = os.path.join(save_folder, 'CNN_' +str(index_save)+'_variable.pth')\n",
    "    params_save_path = os.path.join(save_folder, 'CNN_' +str(index_save)+'_params.mat')\n",
    "    \n",
    "    params = {   \n",
    "                'SNR': snr,\n",
    "                'epoc': NUM_EPOCHS,\n",
    "                'rows': rowss,\n",
    "                'learning_rate': learning_rate,\n",
    "                'train_track_LI': train_loss,\n",
    "                'val_track_LI': val_loss,\n",
    "    }\n",
    "    variables = {             \n",
    "                'train_track_LI': train_loss,\n",
    "                'val_track_LI': val_loss,\n",
    "                # 'train_min_LI': trainData_min.cpu(),\n",
    "                # 'train_max_LI': trainData_max.cpu(),\n",
    "                # 'train_label_min': trainLabels_min.cpu(),\n",
    "                # 'train_label_max': trainLabels_max.cpu(),\n",
    "    }\n",
    "    # variable to save \n",
    "    # Save the models' state dictionaries \n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, model_save_path)\n",
    "\n",
    "    figure_save_path = os.path.join(save_folder_fig, str(snr) + 'dB') \n",
    "    \n",
    "    os.makedirs(figure_save_path, exist_ok=True)\n",
    "\n",
    "    plotfig.figLoss(train_loss, val_loss, index_save, figure_save_path, '_LS_LI_Loss.png')\n",
    "\n",
    "\n",
    "    # True channel\n",
    "    H_val_true = valLabels.cpu()\n",
    "    # convert to complex matrices\n",
    "    H_val_true_complex = torch.complex(H_val_true[:,0,:,:], H_val_true[:,1,:,:])\n",
    "    # variables['H_val_true'] = H_val_true # (nVal, 2, 612, 14)\n",
    "\n",
    "    plotfig.figTrueChan(H_val_true[-1,0,:,:], 'True Channel', index_save, figure_save_path, '_trueChannel.png')\n",
    "\n",
    "    # Estimated Channel \n",
    "    H_val_NN = H_NN_val.cpu()    \n",
    "    plotfig.figTrueChan(H_val_NN[-1,0,:,:], 'LI+CNN Estimated Channel (before de-normlized)', \n",
    "                            index_save, figure_save_path, '_LS_LI_CNN_estimatedChan_before_denorm.png')\n",
    "\n",
    "    # De-normalized                                                               \n",
    "    H_val_NN_denormd = utils.deMinMax(H_NN_val, valLabel_min, valLabel_max)\n",
    "                        #     H_NN_val == [nVal, 2, 612, 14] \n",
    "                        # valLabel_min == [nVal,1]\n",
    "                        \n",
    "    H_val_NN_denormd = H_val_NN_denormd.cpu()\n",
    "    # variables['H_val_LI_NN'] = H_val_NN_denormd # (nVal, 2, 612, 14)\n",
    "\n",
    "    # convert to complex matrices\n",
    "    H_val_NN_denormd_complex = torch.complex(H_val_NN_denormd[:,0,:,:], H_val_NN_denormd[:,1,:,:])\n",
    "    \n",
    "    nmse_LI_NN = utils.calNMSE(H_val_NN_denormd_complex, H_val_true_complex)\n",
    "    \n",
    "    variables['NMSE_LI_NN'] = nmse_LI_NN.cpu().mean()\n",
    "    nmse_LI_NN_val.append(variables['NMSE_LI_NN'].item())\n",
    "    print(f\"LI+NN NMSE: {variables['NMSE_LI_NN'].item()}\")\n",
    "    \n",
    "    plotfig.figPredChan(H_val_NN_denormd[-1,0,:,:], 'LI+CNN Estimated Channel (after de-normlized)',\n",
    "                            nmse_LI_NN[-1], index_save, figure_save_path, '_LS_LI_CNN_estimatedChan.png')\n",
    "#####\n",
    "##### above is LS+LI+NN \n",
    "\n",
    "##### following is Linear interpolated channel (only LS+LI)\n",
    "    H_val_linInterp = H_linear_val.cpu()\n",
    "    # convert to complex matrices\n",
    "    H_val_linInterp_complex = torch.complex(H_val_linInterp[:,0,:,:], H_val_linInterp[:,1,:,:]) # [?, 612, 14]\n",
    "\n",
    "    # NMSE of Linear Interpolation\n",
    "    # Calculate the NMSE\n",
    "    nmse_LI = utils.calNMSE(H_val_linInterp_complex, H_val_true_complex)\n",
    "    \n",
    "    variables['NMSE_LI'] = nmse_LI.cpu().mean()\n",
    "    nmse_LS_LI_val.append(variables['NMSE_LI'].item())\n",
    "    print(f\"LS+LI NMSE: {variables['NMSE_LI'].item()}\")\n",
    "    \n",
    "    plotfig.figPredChan(H_val_linInterp[-1,0,:,:], 'LS + Interpolate Estimated Channel',\n",
    "                            nmse_LI[-1], index_save, figure_save_path, '_LS_LI_estimatedChan.png')\n",
    "\n",
    "\n",
    "##########################################\n",
    "    # ------------------------------------------------------\n",
    "    # When Input of the NN is just H_equalized\n",
    "    print(f\" Training for LS\")\n",
    "    # [samples, 2, 612, 14]\n",
    "    H_LS_train = H_equal_train.cpu()\n",
    "    plotfig.figTrueChan(H_LS_train[0,0,:,:], 'LS Channel', index_save, figure_save_path, '_LS_Chan.png')\n",
    "    \n",
    "    # Split into training and validation sets for H_NN training\n",
    "    train_loader, trainLabel_min, trainLabel_max = loader.genLoader(H_equal_train, trainLabels, BATCH_SIZE, device, 'train',  True, norm_approach)\n",
    "    val_loader,     valLabel_min,   vallabel_max = loader.genLoader(H_equal_val,     valLabels, BATCH_SIZE, device, 'valid', False, norm_approach)\n",
    "\n",
    "\n",
    "    model2 = utils.CNN_Est().to(device)\n",
    "    optimizer2 = torch.optim.Adam(model2.parameters(), lr=learning_rate) \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Training loop\n",
    "    train_loss =[]\n",
    "    val_loss = []\n",
    "    H_NN_val = torch.empty_like(valLabels) # [nVal, 2, 612, 14]\n",
    "    num_epochs = NUM_EPOCHS\n",
    "    for epoch in range(num_epochs):\n",
    "        model2.train()\n",
    "        running_loss = 0.0\n",
    "        if (epoch == num_epochs-1):\n",
    "            i = 0\n",
    "        for inputs, targets, targets_min, targets_max in train_loader:\n",
    "            optimizer2.zero_grad()\n",
    "            outputs = model2(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_loss.append(avg_train_loss)\n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS, Epoch {epoch+1}/{num_epochs}, Loss: {avg_train_loss} \")\n",
    "        \n",
    "        # Validation \n",
    "        model2.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets, val_targetsMin, val_targetsMax in val_loader:\n",
    "                val_inputs_real = val_inputs[:,0,:,:].unsqueeze(1)\n",
    "                val_inputs_imag = val_inputs[:,1,:,:].unsqueeze(1)\n",
    "                val_targets_real = val_targets[:,0,:,:].unsqueeze(1)\n",
    "                val_targets_imag = val_targets[:,1,:,:].unsqueeze(1)\n",
    "                \n",
    "                val_outputs_real = model2(val_inputs_real)\n",
    "                val_loss_real = criterion(val_outputs_real, val_targets_real)\n",
    "                running_val_loss += val_loss_real.item()\n",
    "                \n",
    "                val_outputs_imag = model2(val_inputs_imag)\n",
    "                val_loss_imag = criterion(val_outputs_imag, val_targets_imag)\n",
    "                running_val_loss += val_loss_imag.item()\n",
    "                \n",
    "                if (epoch == num_epochs-1):\n",
    "                    H_NN_val[i:i+val_outputs_real.size(0),0,:,:].unsqueeze(1).copy_(val_outputs_real)\n",
    "                    H_NN_val[i:i+val_outputs_imag.size(0),1,:,:].unsqueeze(1).copy_(val_outputs_imag)\n",
    "                    i = i+val_outputs_imag.size(0)\n",
    "                \n",
    "        avg_val_loss = running_val_loss / (len(val_loader)*2)\n",
    "        val_loss.append(avg_val_loss)    \n",
    "                \n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS, Val Loss: {avg_val_loss}\")\n",
    "\n",
    "    plotfig.figLoss(train_loss, val_loss, index_save, figure_save_path, '_LS_Loss.png')\n",
    "\n",
    "    # De-normalized                                                                \n",
    "    H_val_NN_denormd = utils.deMinMax(H_NN_val, valLabel_min, valLabel_max)\n",
    "                        #     H_NN_val == [nVal, 2, 612, 14] \n",
    "                        # valLabel_min == [nVal,1]\n",
    "    H_val_NN_denormd = H_val_NN_denormd.cpu()\n",
    "\n",
    "    model_save_path = os.path.join(save_folder,  'CNN_' +str(index_save)+'_LS_CNN_model.pth')\n",
    "\n",
    "    # variables['H_val_LS_NN']= H_val_NN_denormd.cpu() # (nVal, 2, 612, 14)\n",
    "    variables['train_track_LS']= train_loss\n",
    "    variables['val_track_LS']= val_loss\n",
    "\n",
    "    # Save parameters\n",
    "    params['train_track_LS']= train_loss\n",
    "    params['val_track_LS']= val_loss\n",
    "    savemat(params_save_path, params)\n",
    "    # Save the models' state dictionaries \n",
    "    torch.save({'model_state_dict': model2.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "                }, model_save_path)\n",
    "\n",
    "\n",
    "    # NMSE of LS + NN\n",
    "    H_val_LS_NN_complex = torch.complex(H_val_NN_denormd[:,0,:,:], H_val_NN_denormd[:,1,:,:])\n",
    "    # Calculate the NMSE\n",
    "    nmse_LS_NN = utils.calNMSE(H_val_LS_NN_complex, H_val_true_complex)\n",
    "    \n",
    "    variables['NMSE_LS_NN'] = nmse_LS_NN.cpu().mean()\n",
    "    nmse_LS_NN_val.append(variables['NMSE_LS_NN'].item())\n",
    "    print(f\"LS+LI NMSE: {variables['NMSE_LS_NN'].item()}\")\n",
    "    \n",
    "    plotfig.figPredChan(H_val_NN_denormd[-1,0,:,:], 'LS+CNN Estimated Channel (after de-normlized)',\n",
    "                            nmse_LS_NN[-1], index_save, figure_save_path, '_LS_CNN_estimatedChan.png')\n",
    "    \n",
    "\n",
    "    torch.save( variables,variable_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(SNR, nmse_LS_LI_val, label='LS+LI')\n",
    "plt.plot(SNR, nmse_LS_NN_val, label='LS+NN')\n",
    "plt.plot(SNR, nmse_LI_NN_val, label='LS+LI+NN')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('NMSE')\n",
    "plt.title('Average NMSE over SNR')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
